{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"assisted-service Prerequisites Docker skipper https://github.com/stratoscale/skipper minikube (for tests) kubectl First Setup To push your build target to a Docker registry you first need to change the default target. Create a quay.io or Docker Hub account if you don't already have one. These instructions refer to quay.io, Docker Hub is similar. Create a repository called assisted-service. Make sure you have your ~/.docker/config.json file set up to point to your account. For quay.io, you can go to quay.io -> User Settings, and click \"Generate Encrypted Password\" under \"Docker CLI Password\". Login to quay.io using docker login quay.io . Export the SERVICE environment variable to your Docker registry, and pass a tag of your choice, e.g., \"test\": export SERVICE=quay.io/<username>/assisted-service:<tag> For the first build of the build container run: skipper build assisted-service-build Build skipper make all Generate code after swagger changes After every change in the API ( swagger.yaml ) the code should be generated and the build must pass. skipper make generate-from-swagger Test Pre-configuration Run minikube on your system. Deploy services skipper make deploy-test Run system tests skipper make test Run system tests with regex skipper make test FOCUS=versions Run only unit tests skipper make unit-test Run unit tests for specific package skipper make unit-test TEST=./internal/host Run unit tests with regex skipper make unit-test FOCUS=cluster Update service for the subsystem tests if you are making changes and don't want to deploy everything once again you can simply run this command: make update-service && kubectl get pod --namespace assisted-installer -o name | grep assisted-service | xargs kubectl delete --namespace assisted-installer It will build and push a new image of the service to your Docker registry, then delete the service pod from minikube, the deployment will handle the update and pull the new image to start the service again. Update Discovery Image base OS If you want to update the underlying operating system image used by the discovery iso, follow these steps: Choose the base os image you want to use RHCOS: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/ Fedora CoreOS: https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable Build the new iso generator image sh # Example with RHCOS BASE_OS_IMAGE=https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/pre-release/latest/rhcos-4.6.0-0.nightly-2020-08-26-093617-x86_64-live.x86_64.iso make build-assisted-iso-generator-image Deployment Deploy to minikube The deployment is a system deployment, it contains all the components the service need for all the operations to work (if implemented). S3 service (scality), DB and will use the image generator to create the images in the deployed S3 and create relevant bucket in S3. skipper make deploy-all Deploy to OpenShift Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. skipper make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example: skipper make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note: All deployment configurations are under the deploy directory in case more detailed configuration is required. Deploy UI This service support optional UI deployment. skipper make deploy-ui * In case you are using podman run the above command without skipper . For OpenShift users, look at the service deployment options on OpenShift platform. Deploy Monitoring This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: On Minikube # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring On Openshift # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus. Deploy by tag This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest Deploy without a Kubernetes cluster There are two ways the assisted service can be deployed without using a Kubernetes cluster: Using a pod on your local host In this scenario the service and associated components are deployed onto your local host as a pod using Podman. export SERVICE=quay.io/<your-org>/assisted-service:latest To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090 . Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem To run the subsystem tests: make test-onprem Using assisted-service Live-ISO The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. The live ISO boots up and deploys the assisted-service using containers on host. Using assisted-service Live-ISO Storage assisted-service maintains a cache of openshift-baremetal-install binaries at $WORK_DIR/installercache/ . Persistent storage can optionally be mounted there to persist the cache accross container restarts. However, that storage should not be shared accross multiple assisted-service processes. Cache Expiration Currently there is no mechanism to expire openshift-baremetal-install binaries out of the cache. The recommendation for now is to allow the cache to use the container's own local storage that will vanish when the Pod gets replaced, for example during upgrade. That will prevent the cache from growing forever while allowing it to be effective most of the time. Troubleshooting A document that can assist troubleshooting: link Documentation To rebuild the site after adding some documentation to the Markdown files, you just need to execute this Make target before the push make docs To validate the documentation generated, go to the root of the repo and execute make docs_serve After that, you just need to access to 127.0.0.1:8000 on your browser and check the new content. NOTE: To use these features, you need to have mkdocs installed in your system, to do that you just need to execute this command pip3 install --user mkdocs Linked repositories coreos_installation_iso https://github.com/oshercc/coreos_installation_iso Image in charge of generating the Fedora-coreOs image used to install the host with the relevant ignition file.\\ Image is uploaded to deployed S3 under the name template \"installer-image-\\<cluster-id>\".","title":"OAS Home"},{"location":"#assisted-service","text":"","title":"assisted-service"},{"location":"#prerequisites","text":"Docker skipper https://github.com/stratoscale/skipper minikube (for tests) kubectl","title":"Prerequisites"},{"location":"#first-setup","text":"To push your build target to a Docker registry you first need to change the default target. Create a quay.io or Docker Hub account if you don't already have one. These instructions refer to quay.io, Docker Hub is similar. Create a repository called assisted-service. Make sure you have your ~/.docker/config.json file set up to point to your account. For quay.io, you can go to quay.io -> User Settings, and click \"Generate Encrypted Password\" under \"Docker CLI Password\". Login to quay.io using docker login quay.io . Export the SERVICE environment variable to your Docker registry, and pass a tag of your choice, e.g., \"test\": export SERVICE=quay.io/<username>/assisted-service:<tag> For the first build of the build container run: skipper build assisted-service-build","title":"First Setup"},{"location":"#build","text":"skipper make all","title":"Build"},{"location":"#generate-code-after-swagger-changes","text":"After every change in the API ( swagger.yaml ) the code should be generated and the build must pass. skipper make generate-from-swagger","title":"Generate code after swagger changes"},{"location":"#test","text":"","title":"Test"},{"location":"#pre-configuration","text":"Run minikube on your system. Deploy services skipper make deploy-test","title":"Pre-configuration"},{"location":"#run-system-tests","text":"skipper make test","title":"Run system tests"},{"location":"#run-system-tests-with-regex","text":"skipper make test FOCUS=versions","title":"Run system tests with regex"},{"location":"#run-only-unit-tests","text":"skipper make unit-test","title":"Run only unit tests"},{"location":"#run-unit-tests-for-specific-package","text":"skipper make unit-test TEST=./internal/host","title":"Run unit tests for specific package"},{"location":"#run-unit-tests-with-regex","text":"skipper make unit-test FOCUS=cluster","title":"Run unit tests with regex"},{"location":"#update-service-for-the-subsystem-tests","text":"if you are making changes and don't want to deploy everything once again you can simply run this command: make update-service && kubectl get pod --namespace assisted-installer -o name | grep assisted-service | xargs kubectl delete --namespace assisted-installer It will build and push a new image of the service to your Docker registry, then delete the service pod from minikube, the deployment will handle the update and pull the new image to start the service again.","title":"Update service for the subsystem tests"},{"location":"#update-discovery-image-base-os","text":"If you want to update the underlying operating system image used by the discovery iso, follow these steps: Choose the base os image you want to use RHCOS: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/ Fedora CoreOS: https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable Build the new iso generator image sh # Example with RHCOS BASE_OS_IMAGE=https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/pre-release/latest/rhcos-4.6.0-0.nightly-2020-08-26-093617-x86_64-live.x86_64.iso make build-assisted-iso-generator-image","title":"Update Discovery Image base OS"},{"location":"#deployment","text":"","title":"Deployment"},{"location":"#deploy-to-minikube","text":"The deployment is a system deployment, it contains all the components the service need for all the operations to work (if implemented). S3 service (scality), DB and will use the image generator to create the images in the deployed S3 and create relevant bucket in S3. skipper make deploy-all","title":"Deploy to minikube"},{"location":"#deploy-to-openshift","text":"Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. skipper make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example: skipper make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note: All deployment configurations are under the deploy directory in case more detailed configuration is required.","title":"Deploy to OpenShift"},{"location":"#deploy-ui","text":"This service support optional UI deployment. skipper make deploy-ui * In case you are using podman run the above command without skipper . For OpenShift users, look at the service deployment options on OpenShift platform.","title":"Deploy UI"},{"location":"#deploy-monitoring","text":"This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: On Minikube # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring On Openshift # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus.","title":"Deploy Monitoring"},{"location":"#deploy-by-tag","text":"This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"Deploy by tag"},{"location":"#deploy-without-a-kubernetes-cluster","text":"There are two ways the assisted service can be deployed without using a Kubernetes cluster:","title":"Deploy without a Kubernetes cluster"},{"location":"#using-a-pod-on-your-local-host","text":"In this scenario the service and associated components are deployed onto your local host as a pod using Podman. export SERVICE=quay.io/<your-org>/assisted-service:latest To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090 . Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem To run the subsystem tests: make test-onprem","title":"Using a pod on your local host"},{"location":"#using-assisted-service-live-iso","text":"The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. The live ISO boots up and deploys the assisted-service using containers on host. Using assisted-service Live-ISO","title":"Using assisted-service Live-ISO"},{"location":"#storage","text":"assisted-service maintains a cache of openshift-baremetal-install binaries at $WORK_DIR/installercache/ . Persistent storage can optionally be mounted there to persist the cache accross container restarts. However, that storage should not be shared accross multiple assisted-service processes.","title":"Storage"},{"location":"#cache-expiration","text":"Currently there is no mechanism to expire openshift-baremetal-install binaries out of the cache. The recommendation for now is to allow the cache to use the container's own local storage that will vanish when the Pod gets replaced, for example during upgrade. That will prevent the cache from growing forever while allowing it to be effective most of the time.","title":"Cache Expiration"},{"location":"#troubleshooting","text":"A document that can assist troubleshooting: link","title":"Troubleshooting"},{"location":"#documentation","text":"To rebuild the site after adding some documentation to the Markdown files, you just need to execute this Make target before the push make docs To validate the documentation generated, go to the root of the repo and execute make docs_serve After that, you just need to access to 127.0.0.1:8000 on your browser and check the new content. NOTE: To use these features, you need to have mkdocs installed in your system, to do that you just need to execute this command pip3 install --user mkdocs","title":"Documentation"},{"location":"#linked-repositories","text":"","title":"Linked repositories"},{"location":"#coreos_installation_iso","text":"https://github.com/oshercc/coreos_installation_iso Image in charge of generating the Fedora-coreOs image used to install the host with the relevant ignition file.\\ Image is uploaded to deployed S3 under the name template \"installer-image-\\<cluster-id>\".","title":"coreos_installation_iso"},{"location":"installer-live-iso/","text":"assisted-service Live ISO The assisted-service can be deployed using a live ISO. The live ISO deploys the assisted-service using containers on RHCOS. The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. How to create an assisted-service live ISO Create the ignition config A ignition config that deploys the assisted-service is available at https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign. Download this ignition config and modify it to include your ssh public key and your registry.redhat.io pull secret. The example below assumes your pull secret is saved into a file called auth.json. wget https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign export SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub) export PULL_SECRET_ENCODED=$(export PULL_SECRET=$(cat auth.json); urlencode $PULL_SECRET) sed -i 's#replace-with-your-ssh-public-key#'\"${SSH_PUBLIC_KEY}\"'#' onprem-iso-config.ign sed -i 's#replace-with-your-urlencoded-pull-secret#'\"${PULL_SECRET_ENCODED}\"'#' onprem-iso-config.ign Currently, the upstream assisted-service container image cannot be used with the live ISO. You will need to build a custom container image and push it to quay.io. export SERVICE=quay.io/<your-org>/assisted-service:latest make build docker push ${SERVICE} Then update the ignition config file to use your assisted-service container image. sed -i 's#quay.io/ocpmetal/assisted-service:latest#'\"${SERVICE}\"'#' onprem-iso-config.ign Download the base RHCOS live ISO The base live ISO is extracted from a container image. Run the container image containing the ISO, copy the ISO, and then stop and remove the container. podman run --privileged --pull=always --rm \\ -v .:/download -w /download \\ --entrypoint /bin/bash \\ quay.io/ocpmetal/assisted-iso-create:latest cp /data/livecd.iso /download/livecd.iso Create the assisted-service live ISO Finally, use the ignition config (onprem-iso-config.ign) and the base live ISO (livecd.iso) to create the assisted-service live ISO. podman run --rm --privileged -v /dev:/dev -v /run/udev:/run/udev -v .:/data \\ quay.io/coreos/coreos-installer:release iso ignition embed -i /data/onprem-iso-config.ign -o /data/assisted-service.iso /data/livecd.iso The live ISO, assisted-service.iso (not livecd.iso), can then be used to deploy the installer. The live ISO storage system is emphemeral and its size depends on the amount of memory installed on the host. A minimum of 10GB of memory is required to deploy the installer, generate a single discovery ISO, and install an OCP cluster. After the live ISO boots, the UI should be accessible from the browser at https://<hostname-or-ip>:8443. It may take a couple of minutes for the assisted-service and UI to become ready after you see the login prompt. How to debug Login to the host using your ssh private key. The assisted-service components are deployed as systemd services. * assisted-service-installer.service * assisted-service-db.service * assisted-service-ui.service Verify that the containers deploy by those services are running. sudo podman ps -a Examine the assisted-service-installer.service logs: sudo journalctl -f -u assisted-service-installer.service The environment file used to deploy the assisted-service is located at /etc/assisted-service/environment. Pull secrets are saved to a file located at /etc/assisted-service/auth.json. How to use the FCC file to generate the base ignition config file The ignition file is created using a predefined Fedore CoreOS Config (FCC) file provided in /config/onprem-iso-fcc.yaml. FCC files are easier to read and edit than the machine readable ignition files. The FCC file transpiles to an ignition config using: podman run --rm -v ./config/onprem-iso-fcc.yaml:/config.fcc:z quay.io/coreos/fcct:release --pretty --strict /config.fcc > onprem-iso-config.ign There is also a make target that you can use, which wraps the above command to generate the ignition file: make generate-onprem-iso-ignition","title":"assisted-service Live ISO"},{"location":"installer-live-iso/#assisted-service-live-iso","text":"The assisted-service can be deployed using a live ISO. The live ISO deploys the assisted-service using containers on RHCOS. The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file.","title":"assisted-service Live ISO"},{"location":"installer-live-iso/#how-to-create-an-assisted-service-live-iso","text":"","title":"How to create an assisted-service live ISO"},{"location":"installer-live-iso/#create-the-ignition-config","text":"A ignition config that deploys the assisted-service is available at https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign. Download this ignition config and modify it to include your ssh public key and your registry.redhat.io pull secret. The example below assumes your pull secret is saved into a file called auth.json. wget https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign export SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub) export PULL_SECRET_ENCODED=$(export PULL_SECRET=$(cat auth.json); urlencode $PULL_SECRET) sed -i 's#replace-with-your-ssh-public-key#'\"${SSH_PUBLIC_KEY}\"'#' onprem-iso-config.ign sed -i 's#replace-with-your-urlencoded-pull-secret#'\"${PULL_SECRET_ENCODED}\"'#' onprem-iso-config.ign Currently, the upstream assisted-service container image cannot be used with the live ISO. You will need to build a custom container image and push it to quay.io. export SERVICE=quay.io/<your-org>/assisted-service:latest make build docker push ${SERVICE} Then update the ignition config file to use your assisted-service container image. sed -i 's#quay.io/ocpmetal/assisted-service:latest#'\"${SERVICE}\"'#' onprem-iso-config.ign","title":"Create the ignition config"},{"location":"installer-live-iso/#download-the-base-rhcos-live-iso","text":"The base live ISO is extracted from a container image. Run the container image containing the ISO, copy the ISO, and then stop and remove the container. podman run --privileged --pull=always --rm \\ -v .:/download -w /download \\ --entrypoint /bin/bash \\ quay.io/ocpmetal/assisted-iso-create:latest cp /data/livecd.iso /download/livecd.iso","title":"Download the base RHCOS live ISO"},{"location":"installer-live-iso/#create-the-assisted-service-live-iso","text":"Finally, use the ignition config (onprem-iso-config.ign) and the base live ISO (livecd.iso) to create the assisted-service live ISO. podman run --rm --privileged -v /dev:/dev -v /run/udev:/run/udev -v .:/data \\ quay.io/coreos/coreos-installer:release iso ignition embed -i /data/onprem-iso-config.ign -o /data/assisted-service.iso /data/livecd.iso The live ISO, assisted-service.iso (not livecd.iso), can then be used to deploy the installer. The live ISO storage system is emphemeral and its size depends on the amount of memory installed on the host. A minimum of 10GB of memory is required to deploy the installer, generate a single discovery ISO, and install an OCP cluster. After the live ISO boots, the UI should be accessible from the browser at https://<hostname-or-ip>:8443. It may take a couple of minutes for the assisted-service and UI to become ready after you see the login prompt.","title":"Create the assisted-service live ISO"},{"location":"installer-live-iso/#how-to-debug","text":"Login to the host using your ssh private key. The assisted-service components are deployed as systemd services. * assisted-service-installer.service * assisted-service-db.service * assisted-service-ui.service Verify that the containers deploy by those services are running. sudo podman ps -a Examine the assisted-service-installer.service logs: sudo journalctl -f -u assisted-service-installer.service The environment file used to deploy the assisted-service is located at /etc/assisted-service/environment. Pull secrets are saved to a file located at /etc/assisted-service/auth.json.","title":"How to debug"},{"location":"installer-live-iso/#how-to-use-the-fcc-file-to-generate-the-base-ignition-config-file","text":"The ignition file is created using a predefined Fedore CoreOS Config (FCC) file provided in /config/onprem-iso-fcc.yaml. FCC files are easier to read and edit than the machine readable ignition files. The FCC file transpiles to an ignition config using: podman run --rm -v ./config/onprem-iso-fcc.yaml:/config.fcc:z quay.io/coreos/fcct:release --pretty --strict /config.fcc > onprem-iso-config.ign There is also a make target that you can use, which wraps the above command to generate the ignition file: make generate-onprem-iso-ignition","title":"How to use the FCC file to generate the base ignition config file"},{"location":"dev/migrations/","text":"Migrations Why? There are some database changes that GORM auto migration will not handle for us . These need to be dealt with separately. How? To do this we are using a migration module called gormigrate . Migrations are run at startup right after the automigration is run. Adding a new migration Each migration should have a separate file in /internal/migrations prefixed with a timestamp which will also be the migration ID Each migration should have a single function named to describe the change which returns a *gormigrate.Migration Both migrate (up) and rollback (down) should be implemented if possible Every migration should have a corresponding test (especially for migrations which are changing data) A new migration scaffold can be created using MIGRATION_NAME=someMigrationNameHere make generate-migration This will give the migration files a proper timestamp as well as (empty) tests To be run, every migration function should be added to the list in migrations.all()","title":"Migrations"},{"location":"dev/migrations/#migrations","text":"","title":"Migrations"},{"location":"dev/migrations/#why","text":"There are some database changes that GORM auto migration will not handle for us . These need to be dealt with separately.","title":"Why?"},{"location":"dev/migrations/#how","text":"To do this we are using a migration module called gormigrate . Migrations are run at startup right after the automigration is run.","title":"How?"},{"location":"dev/migrations/#adding-a-new-migration","text":"Each migration should have a separate file in /internal/migrations prefixed with a timestamp which will also be the migration ID Each migration should have a single function named to describe the change which returns a *gormigrate.Migration Both migrate (up) and rollback (down) should be implemented if possible Every migration should have a corresponding test (especially for migrations which are changing data) A new migration scaffold can be created using MIGRATION_NAME=someMigrationNameHere make generate-migration This will give the migration files a proper timestamp as well as (empty) tests To be run, every migration function should be added to the list in migrations.all()","title":"Adding a new migration"},{"location":"user-guide/assisted-service-on-local/","text":"Running OpenShift Assisted Service (OAS) on local machine For that we have some ways we could follow, let's take a look on some of them. Running OAS on Minikube This is the easiest way to deploy OAS, there are some Make targets to do that, but firstly we need Minikube: minikube start --driver=kvm2 skipper make deploy-all skipper make deploy-ui You could use kubectl proxy command to expose the Assisted Service UI for external access purpose Running OAS on Podman The assisted service can also be deployed without using a Kubernetes cluster. In this scenario the service and associated components are deployed onto your local host as a pod using Podman. This type of deployment requires a different container image that combines components that are used to generate the installer ISO and configuration files. First build the image: export SERVICE_ONPREM=quay.io/<your-org>/assisted-service:latest make build-onprem To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090. Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem Running OAS on vanilla K8s work in progress... Running OAS on K3s work in progress... Running OAS on KinD work in progress... Deploying Monitoring service for OAS This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus. Deploy by tag This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"OAS Running on Local"},{"location":"user-guide/assisted-service-on-local/#running-openshift-assisted-service-oas-on-local-machine","text":"For that we have some ways we could follow, let's take a look on some of them.","title":"Running OpenShift Assisted Service (OAS) on local machine"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-minikube","text":"This is the easiest way to deploy OAS, there are some Make targets to do that, but firstly we need Minikube: minikube start --driver=kvm2 skipper make deploy-all skipper make deploy-ui You could use kubectl proxy command to expose the Assisted Service UI for external access purpose","title":"Running OAS on Minikube"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-podman","text":"The assisted service can also be deployed without using a Kubernetes cluster. In this scenario the service and associated components are deployed onto your local host as a pod using Podman. This type of deployment requires a different container image that combines components that are used to generate the installer ISO and configuration files. First build the image: export SERVICE_ONPREM=quay.io/<your-org>/assisted-service:latest make build-onprem To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090. Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem","title":"Running OAS on Podman"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-vanilla-k8s","text":"work in progress...","title":"Running OAS on vanilla K8s"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-k3s","text":"work in progress...","title":"Running OAS on K3s"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-kind","text":"work in progress...","title":"Running OAS on KinD"},{"location":"user-guide/assisted-service-on-local/#deploying-monitoring-service-for-oas","text":"This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus.","title":"Deploying Monitoring service for OAS"},{"location":"user-guide/assisted-service-on-local/#deploy-by-tag","text":"This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"Deploy by tag"},{"location":"user-guide/assisted-service-on-openshift/","text":"How to deploy OAS on OpenShift Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: 1. APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. 1. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note : All deployment configurations are under the deploy directory in case more detailed configuration is required. Deploying the UI This service support optional UI deployment. make deploy-ui TARGET=oc-ingress Deploy Monitoring This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false","title":"OAS Running on Openshift"},{"location":"user-guide/assisted-service-on-openshift/#how-to-deploy-oas-on-openshift","text":"Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: 1. APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. 1. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note : All deployment configurations are under the deploy directory in case more detailed configuration is required.","title":"How to deploy OAS on OpenShift"},{"location":"user-guide/assisted-service-on-openshift/#deploying-the-ui","text":"This service support optional UI deployment. make deploy-ui TARGET=oc-ingress","title":"Deploying the UI"},{"location":"user-guide/assisted-service-on-openshift/#deploy-monitoring","text":"This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false","title":"Deploy Monitoring"},{"location":"user-guide/deploy-on-OSP/","text":"Openshift deployment with OAS - On Openstack work in progress...","title":"OCP Deployment on Openstack"},{"location":"user-guide/deploy-on-OSP/#openshift-deployment-with-oas-on-openstack","text":"work in progress...","title":"Openshift deployment with OAS - On Openstack"},{"location":"user-guide/deploy-on-RHEV/","text":"Openshift deployment with OAS - On RHEV/oVirt work in progress...","title":"OCP Deployment on RHEV"},{"location":"user-guide/deploy-on-RHEV/#openshift-deployment-with-oas-on-rhevovirt","text":"work in progress...","title":"Openshift deployment with OAS - On RHEV/oVirt"},{"location":"user-guide/deploy-on-bare-metal/","text":"Openshift deployment with OAS - On Bare Metal This guide contains all the sections regarding Bare Metal deployment method, like iPXE/PXE, VirtualMedia, etc... let's get started General This section is generic for the most of the cases: DHCP/DNS running on the network you wanna deploy the OCP cluster. Assisted Installer up & running (It's ok if you're working with cloud version). Typical DNS entries for API VIP and Ingress VIP. Pull Secret to reach the OCP Container Images. SSH Key pair. Note : This method could be used also in Virtual environment With that we could start, first step is create the cluster Fill the Cluster name and Pull Secret fields, also select the version you wanna deploy: Now fill the Base Domain field and the SSH Host Public Key Click on Download Discovery ISO Fill again the SSH public key and click on Generate Discovery ISO Wait for ISO generation to finish and you will reach this checkpoint iPXE iPXE deployment method NOTE1 : We use a sample URL, please change to fit your use case accordingly NOTE2 : We've set the live_url as the node hostname on 8080 port , please change to fit your use case accordingly Automatic The automatic way is done using podman, just follow this steps: export IPXE_DIR=/tmp/ipxe/ai mkdir -p ${IPXE_DIR} # This command will download the ISO, extract the Images and create the ignition config files podman run -e BASE_URL=http://devscripts2ipv6.e2e.bos.redhat.com:8080 -e ISO_URL=http://devscripts2ipv6.e2e.bos.redhat.com:6008/api/assisted-install/v1/clusters/33ffb056-ee65-4fee-91c9-f60e5ebea4a3/downloads/image -v /tmp/ipxe/ai:/data:Z --net=host -it --rm quay.io/ohadlevy/ai-ipxe # This command will host the iPXE files on an podman container podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe Manual The manual way is explained here. You need at least to have the Discovery ISO already generated Now let's download that ISO in the provisioning machine, where the iPXE files will be hosted (use the Command to download the ISO button from the Assisted Service website export IPXE_DIR=/tmp/ipxe/ai export IMAGE_PATH=/tmp/discovery_image_ocp.iso wget -O ${IMAGE_PATH} 'http://cloud.redhat.com/api/assisted-install/v1/clusters/<cluster_id>/downloads/image' Now we need to create the folder and the ipxe file definition mkdir -p ${IPXE_DIR} cat > $IPXE_DIR/ipxe << EOF #!ipxe set live_url $(hostname):8080 kernel \\${live_url}/vmlinuz ignition.config.url=\\${live_url}/config.ign coreos.live.rootfs_url=\\${live_url}/rootfs.img ${KERNEL_OPTS} initrd \\${live_url}/initrd.img boot EOF We also need to extract the images from the ISO export PXE_IMAGES=`isoinfo -i $IMAGE_PATH -f | grep -i images/pxeboot` for img in $PXE_IMAGES; do export name=`basename ${img,,} | sed 's/\\;1//' | sed 's/\\.$//'` echo extracting $name isoinfo -i $IMAGE_PATH -x $img > $IPXE_DIR/$name done And as a last step, write the Ignition files for the deployment echo writing custom user ignition echo '{' > $IPXE_DIR/config.ign isoinfo -i $IMAGE_PATH -x '/IMAGES/IGNITION.IMG;1' | xz -dc - | sed '1d; $d' >> $IPXE_DIR/config.ign echo '}' >> $IPXE_DIR/config.ign After the Ignition files creation we need to host the files, for that we will use a podman contianer based on nginx podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe Booting the nodes from iPXE First step, we need to set up the boot mode on the iDrac's as boot once for iPXE, this will depend on the steps on every Bare Metal Manufacturer/Version/Hardware. When you are booting the nodes, stay tuned to press crtl-b when the prompt say that: Now we need to get a correct IP and point to the right iPXE file And we just need to wait until the boot was finished, and the nodes start appearing on the Assisted Service interface Then we will modify the nodename to use a right name for Openshift Create another 2 more nodes and repeat this step Now fill the API Virtual IP and Ingress Virtual IP fields Now you just need to click on Install Cluster button and wait for the installation to finish.","title":"OCP Deployment on Bare Metal"},{"location":"user-guide/deploy-on-bare-metal/#openshift-deployment-with-oas-on-bare-metal","text":"This guide contains all the sections regarding Bare Metal deployment method, like iPXE/PXE, VirtualMedia, etc... let's get started","title":"Openshift deployment with OAS - On Bare Metal"},{"location":"user-guide/deploy-on-bare-metal/#general","text":"This section is generic for the most of the cases: DHCP/DNS running on the network you wanna deploy the OCP cluster. Assisted Installer up & running (It's ok if you're working with cloud version). Typical DNS entries for API VIP and Ingress VIP. Pull Secret to reach the OCP Container Images. SSH Key pair. Note : This method could be used also in Virtual environment With that we could start, first step is create the cluster Fill the Cluster name and Pull Secret fields, also select the version you wanna deploy: Now fill the Base Domain field and the SSH Host Public Key Click on Download Discovery ISO Fill again the SSH public key and click on Generate Discovery ISO Wait for ISO generation to finish and you will reach this checkpoint","title":"General"},{"location":"user-guide/deploy-on-bare-metal/#ipxe","text":"iPXE deployment method NOTE1 : We use a sample URL, please change to fit your use case accordingly NOTE2 : We've set the live_url as the node hostname on 8080 port , please change to fit your use case accordingly","title":"iPXE"},{"location":"user-guide/deploy-on-bare-metal/#automatic","text":"The automatic way is done using podman, just follow this steps: export IPXE_DIR=/tmp/ipxe/ai mkdir -p ${IPXE_DIR} # This command will download the ISO, extract the Images and create the ignition config files podman run -e BASE_URL=http://devscripts2ipv6.e2e.bos.redhat.com:8080 -e ISO_URL=http://devscripts2ipv6.e2e.bos.redhat.com:6008/api/assisted-install/v1/clusters/33ffb056-ee65-4fee-91c9-f60e5ebea4a3/downloads/image -v /tmp/ipxe/ai:/data:Z --net=host -it --rm quay.io/ohadlevy/ai-ipxe # This command will host the iPXE files on an podman container podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe","title":"Automatic"},{"location":"user-guide/deploy-on-bare-metal/#manual","text":"The manual way is explained here. You need at least to have the Discovery ISO already generated Now let's download that ISO in the provisioning machine, where the iPXE files will be hosted (use the Command to download the ISO button from the Assisted Service website export IPXE_DIR=/tmp/ipxe/ai export IMAGE_PATH=/tmp/discovery_image_ocp.iso wget -O ${IMAGE_PATH} 'http://cloud.redhat.com/api/assisted-install/v1/clusters/<cluster_id>/downloads/image' Now we need to create the folder and the ipxe file definition mkdir -p ${IPXE_DIR} cat > $IPXE_DIR/ipxe << EOF #!ipxe set live_url $(hostname):8080 kernel \\${live_url}/vmlinuz ignition.config.url=\\${live_url}/config.ign coreos.live.rootfs_url=\\${live_url}/rootfs.img ${KERNEL_OPTS} initrd \\${live_url}/initrd.img boot EOF We also need to extract the images from the ISO export PXE_IMAGES=`isoinfo -i $IMAGE_PATH -f | grep -i images/pxeboot` for img in $PXE_IMAGES; do export name=`basename ${img,,} | sed 's/\\;1//' | sed 's/\\.$//'` echo extracting $name isoinfo -i $IMAGE_PATH -x $img > $IPXE_DIR/$name done And as a last step, write the Ignition files for the deployment echo writing custom user ignition echo '{' > $IPXE_DIR/config.ign isoinfo -i $IMAGE_PATH -x '/IMAGES/IGNITION.IMG;1' | xz -dc - | sed '1d; $d' >> $IPXE_DIR/config.ign echo '}' >> $IPXE_DIR/config.ign After the Ignition files creation we need to host the files, for that we will use a podman contianer based on nginx podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe","title":"Manual"},{"location":"user-guide/deploy-on-bare-metal/#booting-the-nodes-from-ipxe","text":"First step, we need to set up the boot mode on the iDrac's as boot once for iPXE, this will depend on the steps on every Bare Metal Manufacturer/Version/Hardware. When you are booting the nodes, stay tuned to press crtl-b when the prompt say that: Now we need to get a correct IP and point to the right iPXE file And we just need to wait until the boot was finished, and the nodes start appearing on the Assisted Service interface Then we will modify the nodename to use a right name for Openshift Create another 2 more nodes and repeat this step Now fill the API Virtual IP and Ingress Virtual IP fields Now you just need to click on Install Cluster button and wait for the installation to finish.","title":"Booting the nodes from iPXE"},{"location":"user-guide/deploy-on-local/","text":"Openshift deployment with OAS - On Local work in progress...","title":"OCP Deployment on Local"},{"location":"user-guide/deploy-on-local/#openshift-deployment-with-oas-on-local","text":"work in progress...","title":"Openshift deployment with OAS - On Local"},{"location":"user-guide/deploy-on-vsphere/","text":"Openshift deployment with OAS - On vSphere work in progress...","title":"OCP Deployment on vSphere"},{"location":"user-guide/deploy-on-vsphere/#openshift-deployment-with-oas-on-vsphere","text":"work in progress...","title":"Openshift deployment with OAS - On vSphere"},{"location":"user-guide/user-guide/","text":"User Guide Welcome to the Openshift Assisted Service User Guide. Here we will look for the best way to help you deploying Openshift 4 in the provider you desire. OCP Deployment on Local OCP Deployment on Bare Metal OCP Deployment on vSphere OCP Deployment on RHEV OCP Deployment on Openstack","title":"User Guide Index"},{"location":"user-guide/user-guide/#user-guide","text":"Welcome to the Openshift Assisted Service User Guide. Here we will look for the best way to help you deploying Openshift 4 in the provider you desire. OCP Deployment on Local OCP Deployment on Bare Metal OCP Deployment on vSphere OCP Deployment on RHEV OCP Deployment on Openstack","title":"User Guide"}]}