{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"assisted-service About This repository provides a service that installs OpenShift. Its main benefits include a minimum amount of prerequisites from the user's infrastructure, as well as comprehensive pre-flight validations to ensure a successful installation. The service exposes either a REST API, or it can be deployed as an Operator where it exposes a Kubernetes-native API via Custom Resources. A UI is available that uses the REST API. The Assisted Service can currently install clusters with highly-available control planes (3 hosts and above) and can also install Single-Node OpenShift (SNO). Highly available clusters are configured to use OpenShift's baremetal platform (typically used in bare metal IPI deployments ), while SNO uses none (typically used in UPI deployments ). The basic flow for creating a new OpenShift cluster using the Assisted Service via the UI or REST API is: 1. Create a new Cluster resource with the minimal required properties. 1. Generate and download a bootable image which is customized for that cluster. This image is based on RHCOS and is customized to automatically run an agent upon boot. 1. Boot the hosts that will form the cluster with the image from the previous step. The boot method is left to the user (e.g., USB drive, virtual media, PXE, etc.). 1. The agent running on each host contacts the Assisted Service via REST API and performs discovery (sends hardware inventory and connectivity information). 1. The UI guides the user through the installation, with the service performing validations along the way. Alternatively, this can be done via API. 1. Once all validations pass, the user may initiate the installation. Progress may be viewed via the UI or API, and logs are made available for download directly from the service. Demos and blog posts Below are some recent demos and blog posts: * Blog, Jan 2021: Using the OpenShift Assisted Installer Service to Deploy an OpenShift Cluster on Bare Metal and vSphere * Blog and demo, Dec 2020: Making OpenShift on Bare Metal easy * Blog and demo, Oct 2020: It's Inside your House! Assisted Installer on Bare Metal Demonstration User documentation By continuing to read this document you will learn how to build and deploy Assisted Service. If you are interested in using Assisted Service to deploy an OCP cluster, please refer to the User Documentation . Prerequisites Docker skipper https://github.com/stratoscale/skipper minikube (for tests) kubectl Python modules pip install waiting First Setup To push your build target to a Docker registry you first need to change the default target. Create a quay.io or Docker Hub account if you don't already have one. These instructions refer to quay.io, Docker Hub is similar. Create a repository called assisted-service. Make sure you have your ~/.docker/config.json file set up to point to your account. For quay.io, you can go to quay.io -> User Settings, and click \"Generate Encrypted Password\" under \"Docker CLI Password\". Login to quay.io using docker login quay.io . Export the SERVICE environment variable to your Docker registry, and pass a tag of your choice, e.g., \"test\": export SERVICE=quay.io/<username>/assisted-service:<tag> For the first build of the build container run: skipper build assisted-service-build Build skipper make all Generate code after swagger changes After every change in the API ( swagger.yaml ) the code should be generated and the build must pass. skipper make generate-from-swagger Testing More information is available here: Assisted Installer Testing Update Discovery Image base OS If you want to update the underlying operating system image used by the discovery iso, follow these steps: Choose the base os image you want to use RHCOS: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/ Fedora CoreOS: https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable Build the new iso generator image sh # Example with RHCOS BASE_OS_IMAGE=https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/pre-release/latest/rhcos-4.6.0-0.nightly-2020-08-26-093617-x86_64-live.x86_64.iso make build-assisted-iso-generator-image Deployment Deploy to minikube The deployment is a system deployment, it contains all the components the service need for all the operations to work (if implemented). S3 service (minio), DB and will use the image generator to create the images in the deployed S3 and create relevant bucket in S3. skipper make deploy-all Deploy to OpenShift Besides default minikube deployment, the service supports deployment to OpenShift cluster using ingress as the access point to the service. skipper make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example: skipper make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note: All deployment configurations are under the deploy directory in case more detailed configuration is required. Deploy UI This service supports optional UI deployment. skipper make deploy-ui * In case you are using podman run the above command without skipper . For OpenShift users, look at the service deployment options on OpenShift platform. Deploy Monitoring Note: This target is only for development purpose. This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: On Minikube # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring On Openshift # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus. Deploy by tag This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest Deploy without a Kubernetes cluster There are two ways the assisted service can be deployed without using a Kubernetes cluster: Using a pod on your local host In this scenario the service and associated components are deployed onto your local host as a pod using Podman. export SERVICE=quay.io/<your-org>/assisted-service:latest To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090 . Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem To run the subsystem tests: make test-onprem Using assisted-service Live-ISO The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. The live ISO boots up and deploys the assisted-service using containers on host. Using assisted-service Live-ISO Storage assisted-service maintains a cache of openshift-baremetal-install binaries at $WORK_DIR/installercache/ . Persistent storage can optionally be mounted there to persist the cache across container restarts. However, that storage should not be shared across multiple assisted-service processes. Cache Expiration Currently there is no mechanism to expire openshift-baremetal-install binaries out of the cache. The recommendation for now is to allow the cache to use the container's own local storage that will vanish when the Pod gets replaced, for example during upgrade. That will prevent the cache from growing forever while allowing it to be effective most of the time. Troubleshooting A document that can assist troubleshooting: link Documentation To rebuild the site after adding some documentation to the Markdown files, you just need to execute this Make target before the push make docs To validate the documentation generated, go to the root of the repo and execute make docs_serve After that, you just need to access to 127.0.0.1:8000 on your browser and check the new content. NOTE: To use these features, you need to have mkdocs installed in your system, to do that you just need to execute this command pip3 install --user mkdocs Linked repositories coreos_installation_iso https://github.com/oshercc/coreos_installation_iso Image in charge of generating the Fedora-coreOs image used to install the host with the relevant ignition file.\\ Image is uploaded to deployed S3 under the name template \"installer-image-\\<cluster-id>\". Assisted Service on console.redhat.com The Assisted Installer is also available for users as a SAAS hosted in console.redhat.com. More information is available here: Assisted Installer on console.redhat.com Setting a custom discovery ISO password It's possible to modify the discovery ISO (via the API) to enable password login for troubleshooting purposes. More information is available here: Set discovery ISO user password example Contributing Please, read our CONTRIBUTING guidelines for more info about how to create, document, and review PRs.","title":"OAS Home"},{"location":"#assisted-service","text":"","title":"assisted-service"},{"location":"#about","text":"This repository provides a service that installs OpenShift. Its main benefits include a minimum amount of prerequisites from the user's infrastructure, as well as comprehensive pre-flight validations to ensure a successful installation. The service exposes either a REST API, or it can be deployed as an Operator where it exposes a Kubernetes-native API via Custom Resources. A UI is available that uses the REST API. The Assisted Service can currently install clusters with highly-available control planes (3 hosts and above) and can also install Single-Node OpenShift (SNO). Highly available clusters are configured to use OpenShift's baremetal platform (typically used in bare metal IPI deployments ), while SNO uses none (typically used in UPI deployments ). The basic flow for creating a new OpenShift cluster using the Assisted Service via the UI or REST API is: 1. Create a new Cluster resource with the minimal required properties. 1. Generate and download a bootable image which is customized for that cluster. This image is based on RHCOS and is customized to automatically run an agent upon boot. 1. Boot the hosts that will form the cluster with the image from the previous step. The boot method is left to the user (e.g., USB drive, virtual media, PXE, etc.). 1. The agent running on each host contacts the Assisted Service via REST API and performs discovery (sends hardware inventory and connectivity information). 1. The UI guides the user through the installation, with the service performing validations along the way. Alternatively, this can be done via API. 1. Once all validations pass, the user may initiate the installation. Progress may be viewed via the UI or API, and logs are made available for download directly from the service.","title":"About"},{"location":"#demos-and-blog-posts","text":"Below are some recent demos and blog posts: * Blog, Jan 2021: Using the OpenShift Assisted Installer Service to Deploy an OpenShift Cluster on Bare Metal and vSphere * Blog and demo, Dec 2020: Making OpenShift on Bare Metal easy * Blog and demo, Oct 2020: It's Inside your House! Assisted Installer on Bare Metal Demonstration","title":"Demos and blog posts"},{"location":"#user-documentation","text":"By continuing to read this document you will learn how to build and deploy Assisted Service. If you are interested in using Assisted Service to deploy an OCP cluster, please refer to the User Documentation .","title":"User documentation"},{"location":"#prerequisites","text":"Docker skipper https://github.com/stratoscale/skipper minikube (for tests) kubectl Python modules pip install waiting","title":"Prerequisites"},{"location":"#first-setup","text":"To push your build target to a Docker registry you first need to change the default target. Create a quay.io or Docker Hub account if you don't already have one. These instructions refer to quay.io, Docker Hub is similar. Create a repository called assisted-service. Make sure you have your ~/.docker/config.json file set up to point to your account. For quay.io, you can go to quay.io -> User Settings, and click \"Generate Encrypted Password\" under \"Docker CLI Password\". Login to quay.io using docker login quay.io . Export the SERVICE environment variable to your Docker registry, and pass a tag of your choice, e.g., \"test\": export SERVICE=quay.io/<username>/assisted-service:<tag> For the first build of the build container run: skipper build assisted-service-build","title":"First Setup"},{"location":"#build","text":"skipper make all","title":"Build"},{"location":"#generate-code-after-swagger-changes","text":"After every change in the API ( swagger.yaml ) the code should be generated and the build must pass. skipper make generate-from-swagger","title":"Generate code after swagger changes"},{"location":"#testing","text":"More information is available here: Assisted Installer Testing","title":"Testing"},{"location":"#update-discovery-image-base-os","text":"If you want to update the underlying operating system image used by the discovery iso, follow these steps: Choose the base os image you want to use RHCOS: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/ Fedora CoreOS: https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable Build the new iso generator image sh # Example with RHCOS BASE_OS_IMAGE=https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/pre-release/latest/rhcos-4.6.0-0.nightly-2020-08-26-093617-x86_64-live.x86_64.iso make build-assisted-iso-generator-image","title":"Update Discovery Image base OS"},{"location":"#deployment","text":"","title":"Deployment"},{"location":"#deploy-to-minikube","text":"The deployment is a system deployment, it contains all the components the service need for all the operations to work (if implemented). S3 service (minio), DB and will use the image generator to create the images in the deployed S3 and create relevant bucket in S3. skipper make deploy-all","title":"Deploy to minikube"},{"location":"#deploy-to-openshift","text":"Besides default minikube deployment, the service supports deployment to OpenShift cluster using ingress as the access point to the service. skipper make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example: skipper make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note: All deployment configurations are under the deploy directory in case more detailed configuration is required.","title":"Deploy to OpenShift"},{"location":"#deploy-ui","text":"This service supports optional UI deployment. skipper make deploy-ui * In case you are using podman run the above command without skipper . For OpenShift users, look at the service deployment options on OpenShift platform.","title":"Deploy UI"},{"location":"#deploy-monitoring","text":"Note: This target is only for development purpose. This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: On Minikube # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring On Openshift # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus.","title":"Deploy Monitoring"},{"location":"#deploy-by-tag","text":"This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"Deploy by tag"},{"location":"#deploy-without-a-kubernetes-cluster","text":"There are two ways the assisted service can be deployed without using a Kubernetes cluster:","title":"Deploy without a Kubernetes cluster"},{"location":"#using-a-pod-on-your-local-host","text":"In this scenario the service and associated components are deployed onto your local host as a pod using Podman. export SERVICE=quay.io/<your-org>/assisted-service:latest To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090 . Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem To run the subsystem tests: make test-onprem","title":"Using a pod on your local host"},{"location":"#using-assisted-service-live-iso","text":"The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. The live ISO boots up and deploys the assisted-service using containers on host. Using assisted-service Live-ISO","title":"Using assisted-service Live-ISO"},{"location":"#storage","text":"assisted-service maintains a cache of openshift-baremetal-install binaries at $WORK_DIR/installercache/ . Persistent storage can optionally be mounted there to persist the cache across container restarts. However, that storage should not be shared across multiple assisted-service processes.","title":"Storage"},{"location":"#cache-expiration","text":"Currently there is no mechanism to expire openshift-baremetal-install binaries out of the cache. The recommendation for now is to allow the cache to use the container's own local storage that will vanish when the Pod gets replaced, for example during upgrade. That will prevent the cache from growing forever while allowing it to be effective most of the time.","title":"Cache Expiration"},{"location":"#troubleshooting","text":"A document that can assist troubleshooting: link","title":"Troubleshooting"},{"location":"#documentation","text":"To rebuild the site after adding some documentation to the Markdown files, you just need to execute this Make target before the push make docs To validate the documentation generated, go to the root of the repo and execute make docs_serve After that, you just need to access to 127.0.0.1:8000 on your browser and check the new content. NOTE: To use these features, you need to have mkdocs installed in your system, to do that you just need to execute this command pip3 install --user mkdocs","title":"Documentation"},{"location":"#linked-repositories","text":"","title":"Linked repositories"},{"location":"#coreos_installation_iso","text":"https://github.com/oshercc/coreos_installation_iso Image in charge of generating the Fedora-coreOs image used to install the host with the relevant ignition file.\\ Image is uploaded to deployed S3 under the name template \"installer-image-\\<cluster-id>\".","title":"coreos_installation_iso"},{"location":"#assisted-service-on-consoleredhatcom","text":"The Assisted Installer is also available for users as a SAAS hosted in console.redhat.com. More information is available here: Assisted Installer on console.redhat.com","title":"Assisted Service on console.redhat.com"},{"location":"#setting-a-custom-discovery-iso-password","text":"It's possible to modify the discovery ISO (via the API) to enable password login for troubleshooting purposes. More information is available here: Set discovery ISO user password example","title":"Setting a custom discovery ISO password"},{"location":"#contributing","text":"Please, read our CONTRIBUTING guidelines for more info about how to create, document, and review PRs.","title":"Contributing"},{"location":"architecture/","text":"Architectural Overview Introduction File Storage State Machines Host State Machine Cluster State Machine Discovery Image Generation Agent Installation flow Introduction The Assisted Service contains logic for handling API requests as well as several periodic tasks that run in the background. It exposes both a REST API as well as a Kubernetes API implemented via Custom Resources . The REST API for the service is described in OpenAPI/Swagger 2.0 in this repository ( raw , HTML ). The main resources in the REST API are: * Cluster: A definition of an OpenShift cluster, along with its current installation state and progress * Host: A host that is associated with a cluster resource, which like the cluster resource includes its current installation state and progress. It also includes a description of its hardware inventory and current connectivity information. * Image: The definition of a bootable image that the service generates and is used for host discovery. Once the image is generated, the resource contains a URL from where the image may be downloaded. ------------ ----------- | REST API | | k8s API | --------------------------- | Service logic | --------------------------- | | V V -------------- ---------- | file store | | SQL DB | -------------- ---------- File Storage As can be seen in the elegant diagram above, the service requires storage for files which include: a cache of RHCOS images that the service uses for boot image generation, the boot images that it generates, various Ignition configuration files, as well as log files. The service can be configured to use two S3 buckets for these files (a public one for the RHCOS image cache and a private one for all the rest), or two local directories. S3 is generally used when deploying the Assisted Service in the cloud, while using directories on a file system is used when deploying the service as an operator (a Persistent Volume should be used). Additionally, the service requires an SQL database to store metadata about the OpenShift clusters being installed and the hosts that comprise them. State Machines Each cluster and each host being installed moves through their respective state machines that are defined in the service. A cluster or host can transition its state either via user action, or via periodic monitor tasks that run in the service and determine the appropriate state. Host State Machine Discovering: Initial state where the host agent sends hardware and connectivity information. Pending-for-input: The user should input some configuration information so that the service can validate and move the host to \u201cknown\u201d or \u201cinsufficient\u201d state. Known: Hardware and link information is known and sufficient. Insufficient: One or more host validations is failing, for example the hardware or connectivity is not sufficient. Hosts in this state must either be fixed or disabled to continue with the installation. Disconnected: The host has not sent a ping to the service for some time (3 minutes). Hosts in this state must either be fixed or disabled to continue with the installation. Disabled: The user has selected to disable this host. Hosts in this state will not participate in the installation. Installation states: Triggered once the user initiates installation. Preparing-for-installation: The service runs openshift-install create ignition-configs and uploads all files to S3. If the user chose to use route53 for DNS, the service creates those record sets. Installing: The service is ready to begin the cluster installation. Next time the agent asks for instructions, the service will instruct it to begin the installation, and then moves the state to installing-in-progress. Installing-in-progress: The host is currently installing. Installing-pending-user-action: If the service expected the host to reboot and boot from disk, but the agent came up again and contacted the service, the host enters this state to notify the user to fix the server\u2019s boot order. Resetting: If the user requested to reset the installation, the host enters this transient state while the service resets. Resetting-pending-user-action: To reset the installation, the host needs to be booted from the live image. If the host already booted from disk in a previous installation, the host enters this state to notify the user to boot from the live image. Installed: The installation has successfully completed on the host. Error: The installation has failed. Cluster State Machine Pending-for-input: The user should input some configuration information so that the service can validate and move the cluster to \u201cready\u201d or \u201cinsufficient\u201d state. Insufficient: One or more cluster validations is failing. Ready: The cluster is ready for the user to request the installation to start. Preparing-for-installation: Same as hosts\u2019s preparing-for-installation state. Installing: Cluster is currently installing. Finalizing: Cluster is formed, waiting for components to come up. Installed : Cluster installed successfully. Error: Error during installation. The installation will be marked successful if all control plane nodes were deployed successfully, and if at least 2 worker nodes were deployed successfully (in case the cluster definition specified worker nodes). Discovery Image Generation The Assisted Service can currently be configured to generate two types of ISOs, full and minimal, both based on Red Hat Enterprise Linux CoreOS (RHCOS). A live ISO is used, such that everything is run from memory, until an RHCOS image is written to disk and the host is rebooted during installation. The full ISO is simply an RHCOS live ISO with an Ignition config embedded in it, which includes information such as the cluster ID, the user's pull secret (used for authentication), as well as the service file to start the agent process. The minimal ISO is significantly smaller in size due to the fact that the rootfs is downloaded upon boot rather than being embedded in the ISO. This ISO format is especially useful for booting via Virtual Media over a slow network, where the rootfs can later be download over a faster network. Other than the Igntion config that is embedded similarly to the full ISO, network configuration (e.g., static IPs, VLANs, bonds, etc.) is also embedded so that the rootfs can be downloaded at an early stage. Agent When a host is booted with a discovery image, an agent automatically runs and registers with the Assisted Service. Communication is always initiated by the agent, as the service may not be able to contact the hosts being installed. The agent contacts the service once a minute to receive instructions, and then posts the results as well. The instructions to be performed are based on the host's state, and possibly other properties. See below for a description of the various host states. Installation flow When the installation is started, all hosts are still booted from the live ISOs and have agents running which are periodically contacting the Assisted Service for instructions. The first thing that the Assisted Service does when installation is initiated is compile an install-config.yaml, and then run the OpenShift installer to generate the ignition configs and place them in the file storage. At this point the service will also validate the installation disk speed on all hosts (this test writes to the disk so it is not performed before the user initiates the installation). OpenShift installation generally requires a temporary host to be allocated during installation to run the bootstrap logic. The Assisted Service does not require an additional host, but instead one of the control plane nodes is randomly selected to run bootstrap logic during the installation. The installation flow for a host that isn't running the bootstrap logic is as follows: 1. Fetch the relevant ignition file from the service's REST API. 1. Run coreos-installer to write the relevant RHCOS image and ignition to disk (1st ignition that will point to API VIP). 1. Trigger host reboot. 1. The host will start with the new RHCOS image and ignition, and will contact the machine-config-server running on the bootstrap host in order to complete the installation. 1. The nodes will get approved by the csr-approver service running on the bootstrap host. The flow for the host running the bootstrap logic is as follows: 1. Fetch the bootstrap ignition file from the REST API. 1. Run the MCO container for writing the configuration to disk (using once-from option). 1. Copy assisted-controller deployment files to manifests folder (/opt/openshift/manifests). The assisted-controller is a Kubernetes Job that completes the installation monitoring once all hosts have booted from disk, and agents are therefore no longer running. 1. Start the bootstrap services ( bootkube.service , approve-csr.service , progress.service ), at this point the bootstrap will start a temporary control plane. 1. Use the kubeconfig-loopback (part of the bootstrap ignition) and wait for 2 control plane nodes to appear. 1. Wait for the bootkube service to complete. 1. Execute the non-bootstrap installation flow. 1. Get approved by the assisted-controller . The assisted-controller: * Approves any node that tries to join the cluster (by approving the certificate sign requests) * Lists the nodes in the cluster and reports installation progress. * Monitors progress of operator installation, specifically console, CVO, and additional operators selected by the user (e.g., OCS, CNV). * Collects logs and posts them to the service's REST API. * Once all nodes have joined, notifies the installation has completed, and exits.","title":"Architectural Overview"},{"location":"architecture/#architectural-overview","text":"Introduction File Storage State Machines Host State Machine Cluster State Machine Discovery Image Generation Agent Installation flow","title":"Architectural Overview"},{"location":"architecture/#introduction","text":"The Assisted Service contains logic for handling API requests as well as several periodic tasks that run in the background. It exposes both a REST API as well as a Kubernetes API implemented via Custom Resources . The REST API for the service is described in OpenAPI/Swagger 2.0 in this repository ( raw , HTML ). The main resources in the REST API are: * Cluster: A definition of an OpenShift cluster, along with its current installation state and progress * Host: A host that is associated with a cluster resource, which like the cluster resource includes its current installation state and progress. It also includes a description of its hardware inventory and current connectivity information. * Image: The definition of a bootable image that the service generates and is used for host discovery. Once the image is generated, the resource contains a URL from where the image may be downloaded. ------------ ----------- | REST API | | k8s API | --------------------------- | Service logic | --------------------------- | | V V -------------- ---------- | file store | | SQL DB | -------------- ----------","title":"Introduction"},{"location":"architecture/#file-storage","text":"As can be seen in the elegant diagram above, the service requires storage for files which include: a cache of RHCOS images that the service uses for boot image generation, the boot images that it generates, various Ignition configuration files, as well as log files. The service can be configured to use two S3 buckets for these files (a public one for the RHCOS image cache and a private one for all the rest), or two local directories. S3 is generally used when deploying the Assisted Service in the cloud, while using directories on a file system is used when deploying the service as an operator (a Persistent Volume should be used). Additionally, the service requires an SQL database to store metadata about the OpenShift clusters being installed and the hosts that comprise them.","title":"File Storage"},{"location":"architecture/#state-machines","text":"Each cluster and each host being installed moves through their respective state machines that are defined in the service. A cluster or host can transition its state either via user action, or via periodic monitor tasks that run in the service and determine the appropriate state.","title":"State Machines"},{"location":"architecture/#host-state-machine","text":"Discovering: Initial state where the host agent sends hardware and connectivity information. Pending-for-input: The user should input some configuration information so that the service can validate and move the host to \u201cknown\u201d or \u201cinsufficient\u201d state. Known: Hardware and link information is known and sufficient. Insufficient: One or more host validations is failing, for example the hardware or connectivity is not sufficient. Hosts in this state must either be fixed or disabled to continue with the installation. Disconnected: The host has not sent a ping to the service for some time (3 minutes). Hosts in this state must either be fixed or disabled to continue with the installation. Disabled: The user has selected to disable this host. Hosts in this state will not participate in the installation. Installation states: Triggered once the user initiates installation. Preparing-for-installation: The service runs openshift-install create ignition-configs and uploads all files to S3. If the user chose to use route53 for DNS, the service creates those record sets. Installing: The service is ready to begin the cluster installation. Next time the agent asks for instructions, the service will instruct it to begin the installation, and then moves the state to installing-in-progress. Installing-in-progress: The host is currently installing. Installing-pending-user-action: If the service expected the host to reboot and boot from disk, but the agent came up again and contacted the service, the host enters this state to notify the user to fix the server\u2019s boot order. Resetting: If the user requested to reset the installation, the host enters this transient state while the service resets. Resetting-pending-user-action: To reset the installation, the host needs to be booted from the live image. If the host already booted from disk in a previous installation, the host enters this state to notify the user to boot from the live image. Installed: The installation has successfully completed on the host. Error: The installation has failed.","title":"Host State Machine"},{"location":"architecture/#cluster-state-machine","text":"Pending-for-input: The user should input some configuration information so that the service can validate and move the cluster to \u201cready\u201d or \u201cinsufficient\u201d state. Insufficient: One or more cluster validations is failing. Ready: The cluster is ready for the user to request the installation to start. Preparing-for-installation: Same as hosts\u2019s preparing-for-installation state. Installing: Cluster is currently installing. Finalizing: Cluster is formed, waiting for components to come up. Installed : Cluster installed successfully. Error: Error during installation. The installation will be marked successful if all control plane nodes were deployed successfully, and if at least 2 worker nodes were deployed successfully (in case the cluster definition specified worker nodes).","title":"Cluster State Machine"},{"location":"architecture/#discovery-image-generation","text":"The Assisted Service can currently be configured to generate two types of ISOs, full and minimal, both based on Red Hat Enterprise Linux CoreOS (RHCOS). A live ISO is used, such that everything is run from memory, until an RHCOS image is written to disk and the host is rebooted during installation. The full ISO is simply an RHCOS live ISO with an Ignition config embedded in it, which includes information such as the cluster ID, the user's pull secret (used for authentication), as well as the service file to start the agent process. The minimal ISO is significantly smaller in size due to the fact that the rootfs is downloaded upon boot rather than being embedded in the ISO. This ISO format is especially useful for booting via Virtual Media over a slow network, where the rootfs can later be download over a faster network. Other than the Igntion config that is embedded similarly to the full ISO, network configuration (e.g., static IPs, VLANs, bonds, etc.) is also embedded so that the rootfs can be downloaded at an early stage.","title":"Discovery Image Generation"},{"location":"architecture/#agent","text":"When a host is booted with a discovery image, an agent automatically runs and registers with the Assisted Service. Communication is always initiated by the agent, as the service may not be able to contact the hosts being installed. The agent contacts the service once a minute to receive instructions, and then posts the results as well. The instructions to be performed are based on the host's state, and possibly other properties. See below for a description of the various host states.","title":"Agent"},{"location":"architecture/#installation-flow","text":"When the installation is started, all hosts are still booted from the live ISOs and have agents running which are periodically contacting the Assisted Service for instructions. The first thing that the Assisted Service does when installation is initiated is compile an install-config.yaml, and then run the OpenShift installer to generate the ignition configs and place them in the file storage. At this point the service will also validate the installation disk speed on all hosts (this test writes to the disk so it is not performed before the user initiates the installation). OpenShift installation generally requires a temporary host to be allocated during installation to run the bootstrap logic. The Assisted Service does not require an additional host, but instead one of the control plane nodes is randomly selected to run bootstrap logic during the installation. The installation flow for a host that isn't running the bootstrap logic is as follows: 1. Fetch the relevant ignition file from the service's REST API. 1. Run coreos-installer to write the relevant RHCOS image and ignition to disk (1st ignition that will point to API VIP). 1. Trigger host reboot. 1. The host will start with the new RHCOS image and ignition, and will contact the machine-config-server running on the bootstrap host in order to complete the installation. 1. The nodes will get approved by the csr-approver service running on the bootstrap host. The flow for the host running the bootstrap logic is as follows: 1. Fetch the bootstrap ignition file from the REST API. 1. Run the MCO container for writing the configuration to disk (using once-from option). 1. Copy assisted-controller deployment files to manifests folder (/opt/openshift/manifests). The assisted-controller is a Kubernetes Job that completes the installation monitoring once all hosts have booted from disk, and agents are therefore no longer running. 1. Start the bootstrap services ( bootkube.service , approve-csr.service , progress.service ), at this point the bootstrap will start a temporary control plane. 1. Use the kubeconfig-loopback (part of the bootstrap ignition) and wait for 2 control plane nodes to appear. 1. Wait for the bootkube service to complete. 1. Execute the non-bootstrap installation flow. 1. Get approved by the assisted-controller . The assisted-controller: * Approves any node that tries to join the cluster (by approving the certificate sign requests) * Lists the nodes in the cluster and reports installation progress. * Monitors progress of operator installation, specifically console, CVO, and additional operators selected by the user (e.g., OCS, CNV). * Collects logs and posts them to the service's REST API. * Once all nodes have joined, notifies the installation has completed, and exits.","title":"Installation flow"},{"location":"cloud/","text":"Assisted Installer hosted in console.redhat.com Users with a Red Hat account in console.redhat.com are able to use the Assisted Installer to install OCP clusters on their Bare Metals nodes. Using Assisted Installer via UI The UI is available here: https://console.redhat.com/openshift/assisted-installer/clusters/ Using Assisted Installer via API The API is available here: https://api.openshift.com/api/assisted-install/v1/ Authentication On console.redhat.com, Assisted Service APIs calls are authenticated. There are two kind of authentications: User and Agent. Some APIs accept both types. See configuration in API definition . Agent authentication is used by the processes running on the nodes (Agent, Installer & Controller). The Agent APIs are not meant to be used by end users. User Authentication User Authentication is using JWT tokens. There are two ways to get the token: ocm client sudo dnf copr enable ocm/tools sudo dnf install ocm-cli Obtain offline token from https://console.redhat.com/openshift/token (This token does not expire) Login to ocm: ocm login --token (with token from https://console.redhat.com/openshift/token) Generate JWT token : JWT_TOKEN=$(ocm token) HTTP request JWT_TOKEN=$(curl https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token -d client_id=cloud-services -d grant_type=refresh_token -d refresh_token=${OFFLINE_TOKEN} | jq -r '.access_token') The JWT token is valid for 15 minutes and need to be provided in the header of the HTTP request. Here an example to get all the user's clusters: curl https://api.openshift.com/api/assisted-install/v1/clusters -H \"Authorization: Bearer ${JWT_TOKEN}\" Agent Authentication Agent authentication uses a token from the Pull Secret and needs to be provided in the header of the HTTP request. Here an example: bash curl -X POST https://api.openshift.com/api/assisted-install/v1/clusters/f74fe2e3-1d99-4383-b2f3-8213af03ddeb/hosts -H \"X-Secret-Key: <PULL_SECRET_TOKEN>\" ### UUIDs API objects instances are defined with an unique identifier (UUID). In the UI, the cluster UUID is available as part of the URL, for example: https://console.redhat.com/openshift/assisted-installer/clusters/f74fe2e3-1d99-4383-b2f3-8213af03ddeb The host UUID is available in the UI in the 'Host Details' section.","title":"Assisted Installer hosted in console.redhat.com"},{"location":"cloud/#assisted-installer-hosted-in-consoleredhatcom","text":"Users with a Red Hat account in console.redhat.com are able to use the Assisted Installer to install OCP clusters on their Bare Metals nodes.","title":"Assisted Installer hosted in console.redhat.com"},{"location":"cloud/#using-assisted-installer-via-ui","text":"The UI is available here: https://console.redhat.com/openshift/assisted-installer/clusters/","title":"Using Assisted Installer via UI"},{"location":"cloud/#using-assisted-installer-via-api","text":"The API is available here: https://api.openshift.com/api/assisted-install/v1/","title":"Using Assisted Installer via API"},{"location":"cloud/#authentication","text":"On console.redhat.com, Assisted Service APIs calls are authenticated. There are two kind of authentications: User and Agent. Some APIs accept both types. See configuration in API definition . Agent authentication is used by the processes running on the nodes (Agent, Installer & Controller). The Agent APIs are not meant to be used by end users.","title":"Authentication"},{"location":"cloud/#user-authentication","text":"User Authentication is using JWT tokens. There are two ways to get the token:","title":"User Authentication"},{"location":"cloud/#ocm-client","text":"sudo dnf copr enable ocm/tools sudo dnf install ocm-cli Obtain offline token from https://console.redhat.com/openshift/token (This token does not expire) Login to ocm: ocm login --token (with token from https://console.redhat.com/openshift/token) Generate JWT token : JWT_TOKEN=$(ocm token)","title":"ocm client"},{"location":"cloud/#http-request","text":"JWT_TOKEN=$(curl https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token -d client_id=cloud-services -d grant_type=refresh_token -d refresh_token=${OFFLINE_TOKEN} | jq -r '.access_token') The JWT token is valid for 15 minutes and need to be provided in the header of the HTTP request. Here an example to get all the user's clusters: curl https://api.openshift.com/api/assisted-install/v1/clusters -H \"Authorization: Bearer ${JWT_TOKEN}\"","title":"HTTP request"},{"location":"cloud/#agent-authentication","text":"Agent authentication uses a token from the Pull Secret and needs to be provided in the header of the HTTP request. Here an example: bash curl -X POST https://api.openshift.com/api/assisted-install/v1/clusters/f74fe2e3-1d99-4383-b2f3-8213af03ddeb/hosts -H \"X-Secret-Key: <PULL_SECRET_TOKEN>\" ### UUIDs API objects instances are defined with an unique identifier (UUID). In the UI, the cluster UUID is available as part of the URL, for example: https://console.redhat.com/openshift/assisted-installer/clusters/f74fe2e3-1d99-4383-b2f3-8213af03ddeb The host UUID is available in the UI in the 'Host Details' section.","title":"Agent Authentication"},{"location":"events/","text":"Events Events that may be of interest to users of the Assisted Service are made available via the REST API. The events are stored in the SQL database. Each event is associated with a cluster and has a severity; some events are also associated with a host. Using events, a user should be able to understand how the cluster reached its current state, and understand what, if anything, is wrong with it. Note : Be sure not to disclose secrets or sensitive information that is not otherwise available. Event emission guidelines ERROR Something wrong has happened that must be investigated. These should be relatively rare and are important to understand, so make them as verbose as possible - describe what happened, what the user might do to mitigate it, and anything needed to debug it. Examples: 1. REST API call failure. 1. When an async process related to the resource fails. WARNING Something unexpected happened, but we can continue. As with ERROR logs, make the messages as verbose as necessary for understanding what happened. Examples: 1. When a previously-passing (or previously-uncomputed) validation fails. 1. When non-critical components failed to install. 1. When the cluster specs pass minimum validations but are not supported. INFO This is good for marking major milestones in a flow for debuggability. Verbosity here should be as low as possible without impeding debuggability in the field. GET requests should have no INFO logs. They should be added for major milestones in flows where things may go wrong and are interesting to note. Examples: 1. When a cluster or host resource changes status. 1. When a previously-failing validation passes. 1. When a cluster or host resource progresses to a new installation stage.","title":"Events"},{"location":"events/#events","text":"Events that may be of interest to users of the Assisted Service are made available via the REST API. The events are stored in the SQL database. Each event is associated with a cluster and has a severity; some events are also associated with a host. Using events, a user should be able to understand how the cluster reached its current state, and understand what, if anything, is wrong with it. Note : Be sure not to disclose secrets or sensitive information that is not otherwise available.","title":"Events"},{"location":"events/#event-emission-guidelines","text":"","title":"Event emission guidelines"},{"location":"events/#error","text":"Something wrong has happened that must be investigated. These should be relatively rare and are important to understand, so make them as verbose as possible - describe what happened, what the user might do to mitigate it, and anything needed to debug it. Examples: 1. REST API call failure. 1. When an async process related to the resource fails.","title":"ERROR"},{"location":"events/#warning","text":"Something unexpected happened, but we can continue. As with ERROR logs, make the messages as verbose as necessary for understanding what happened. Examples: 1. When a previously-passing (or previously-uncomputed) validation fails. 1. When non-critical components failed to install. 1. When the cluster specs pass minimum validations but are not supported.","title":"WARNING"},{"location":"events/#info","text":"This is good for marking major milestones in a flow for debuggability. Verbosity here should be as low as possible without impeding debuggability in the field. GET requests should have no INFO logs. They should be added for major milestones in flows where things may go wrong and are interesting to note. Examples: 1. When a cluster or host resource changes status. 1. When a previously-failing validation passes. 1. When a cluster or host resource progresses to a new installation stage.","title":"INFO"},{"location":"installer-live-iso/","text":"assisted-service Live ISO The assisted-service can be deployed using a live ISO. The live ISO deploys the assisted-service using containers on RHCOS. The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file. How to create an assisted-service live ISO Create the ignition config A ignition config that deploys the assisted-service is available at https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign. Download this ignition config and modify it to include your ssh public key and your registry.redhat.io pull secret. The example below assumes your pull secret is saved into a file called auth.json. wget https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign export SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub) export PULL_SECRET_ENCODED=$(export PULL_SECRET=$(cat auth.json); urlencode $PULL_SECRET) sed -i 's#replace-with-your-ssh-public-key#'\"${SSH_PUBLIC_KEY}\"'#' onprem-iso-config.ign sed -i 's#replace-with-your-urlencoded-pull-secret#'\"${PULL_SECRET_ENCODED}\"'#' onprem-iso-config.ign Currently, the upstream assisted-service container image cannot be used with the live ISO. You will need to build a custom container image and push it to quay.io. export SERVICE=quay.io/<your-org>/assisted-service:latest make build docker push ${SERVICE} Then update the ignition config file to use your assisted-service container image. sed -i 's#quay.io/ocpmetal/assisted-service:latest#'\"${SERVICE}\"'#' onprem-iso-config.ign Download the base RHCOS live ISO wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/latest/rhcos-live.x86_64.iso Create the assisted-service live ISO Finally, use the ignition config (onprem-iso-config.ign) and the base live ISO (rhcos-live.x86_64.iso) to create the assisted-service live ISO. podman run --rm --privileged -v /dev:/dev -v /run/udev:/run/udev -v .:/data \\ quay.io/coreos/coreos-installer:release iso ignition embed -i /data/onprem-iso-config.ign -o /data/assisted-service.iso /data/rhcos-live.x86_64.iso The live ISO, assisted-service.iso (not rhcos-live.x86_64.iso), can then be used to deploy the installer. The live ISO storage system is emphemeral and its size depends on the amount of memory installed on the host. A minimum of 10GB of memory is required to deploy the installer, generate a single discovery ISO, and install an OCP cluster. After the live ISO boots, the UI should be accessible from the browser at https://<hostname-or-ip>:8443. It may take a couple of minutes for the assisted-service and UI to become ready after you see the login prompt. How to debug Login to the host using your ssh private key. The assisted-service components are deployed as systemd services. * assisted-service-installer.service * assisted-service-db.service * assisted-service-ui.service Verify that the containers deploy by those services are running. sudo podman ps -a Examine the assisted-service-installer.service logs: sudo journalctl -f -u assisted-service-installer.service The environment file used to deploy the assisted-service is located at /etc/assisted-service/environment. Pull secrets are saved to a file located at /etc/assisted-service/auth.json. How to use the FCC file to generate the base ignition config file The ignition file is created using a predefined Fedore CoreOS Config (FCC) file provided in /config/onprem-iso-fcc.yaml. FCC files are easier to read and edit than the machine readable ignition files. The FCC file transpiles to an ignition config using: podman run --rm -v ./config/onprem-iso-fcc.yaml:/config.fcc:z quay.io/coreos/fcct:release --pretty --strict /config.fcc > onprem-iso-config.ign There is also a make target that you can use, which wraps the above command to generate the ignition file: make generate-onprem-iso-ignition","title":"assisted-service Live ISO"},{"location":"installer-live-iso/#assisted-service-live-iso","text":"The assisted-service can be deployed using a live ISO. The live ISO deploys the assisted-service using containers on RHCOS. The assisted-service live ISO is a RHCOS live ISO that is customized with an ignition config file.","title":"assisted-service Live ISO"},{"location":"installer-live-iso/#how-to-create-an-assisted-service-live-iso","text":"","title":"How to create an assisted-service live ISO"},{"location":"installer-live-iso/#create-the-ignition-config","text":"A ignition config that deploys the assisted-service is available at https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign. Download this ignition config and modify it to include your ssh public key and your registry.redhat.io pull secret. The example below assumes your pull secret is saved into a file called auth.json. wget https://raw.githubusercontent.com/openshift/assisted-service/master/config/onprem-iso-config.ign export SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub) export PULL_SECRET_ENCODED=$(export PULL_SECRET=$(cat auth.json); urlencode $PULL_SECRET) sed -i 's#replace-with-your-ssh-public-key#'\"${SSH_PUBLIC_KEY}\"'#' onprem-iso-config.ign sed -i 's#replace-with-your-urlencoded-pull-secret#'\"${PULL_SECRET_ENCODED}\"'#' onprem-iso-config.ign Currently, the upstream assisted-service container image cannot be used with the live ISO. You will need to build a custom container image and push it to quay.io. export SERVICE=quay.io/<your-org>/assisted-service:latest make build docker push ${SERVICE} Then update the ignition config file to use your assisted-service container image. sed -i 's#quay.io/ocpmetal/assisted-service:latest#'\"${SERVICE}\"'#' onprem-iso-config.ign","title":"Create the ignition config"},{"location":"installer-live-iso/#download-the-base-rhcos-live-iso","text":"wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/latest/rhcos-live.x86_64.iso","title":"Download the base RHCOS live ISO"},{"location":"installer-live-iso/#create-the-assisted-service-live-iso","text":"Finally, use the ignition config (onprem-iso-config.ign) and the base live ISO (rhcos-live.x86_64.iso) to create the assisted-service live ISO. podman run --rm --privileged -v /dev:/dev -v /run/udev:/run/udev -v .:/data \\ quay.io/coreos/coreos-installer:release iso ignition embed -i /data/onprem-iso-config.ign -o /data/assisted-service.iso /data/rhcos-live.x86_64.iso The live ISO, assisted-service.iso (not rhcos-live.x86_64.iso), can then be used to deploy the installer. The live ISO storage system is emphemeral and its size depends on the amount of memory installed on the host. A minimum of 10GB of memory is required to deploy the installer, generate a single discovery ISO, and install an OCP cluster. After the live ISO boots, the UI should be accessible from the browser at https://<hostname-or-ip>:8443. It may take a couple of minutes for the assisted-service and UI to become ready after you see the login prompt.","title":"Create the assisted-service live ISO"},{"location":"installer-live-iso/#how-to-debug","text":"Login to the host using your ssh private key. The assisted-service components are deployed as systemd services. * assisted-service-installer.service * assisted-service-db.service * assisted-service-ui.service Verify that the containers deploy by those services are running. sudo podman ps -a Examine the assisted-service-installer.service logs: sudo journalctl -f -u assisted-service-installer.service The environment file used to deploy the assisted-service is located at /etc/assisted-service/environment. Pull secrets are saved to a file located at /etc/assisted-service/auth.json.","title":"How to debug"},{"location":"installer-live-iso/#how-to-use-the-fcc-file-to-generate-the-base-ignition-config-file","text":"The ignition file is created using a predefined Fedore CoreOS Config (FCC) file provided in /config/onprem-iso-fcc.yaml. FCC files are easier to read and edit than the machine readable ignition files. The FCC file transpiles to an ignition config using: podman run --rm -v ./config/onprem-iso-fcc.yaml:/config.fcc:z quay.io/coreos/fcct:release --pretty --strict /config.fcc > onprem-iso-config.ign There is also a make target that you can use, which wraps the above command to generate the ignition file: make generate-onprem-iso-ignition","title":"How to use the FCC file to generate the base ignition config file"},{"location":"installer-stand-alone/","text":"Assisted Installer Stand-Alone This document describes the process of running the Assisted Installer in stand-alone mode via podman . Pre-Requisites OpenShift User Pull Secret You will need a valid OpenShift user pull secret. Copy or download the pull secret from https://console.redhat.com/openshift/install/pull-secret Running the Assisted Installer using Podman Environment The first thing you will need to do is grab the onprem-environment file. Once you have this file, source it. source onprem-environment NOTE * The remainder of this document relies on the values stored in onprem-environment being set in the shell. * The SERVICE_BASE_URL is the ip:port where the assisted-service API is being served. The Assisted Installer's agent uses the SERVICE_BASE_URL to talk back to the API. NGINX Configuration Once you have sourced onprem-environment , you will need to grab the nginx.conf used to configure the Assisted Installer's UI. There are two fields of note: listen 8080; refers to the port used to access the Assisted Installer's UI. As an example, if you wanted the UI to listen on port 9090 to avoid conflict with a port already used on the host you would sed -i \"s|listen.*;|listen 9090;|\" nginx.conf . proxy_pass http://localhost:8090; is the default value of SERVICE_BASE_URL . You could update this with, sed -i \"s|proxy_pass.*;|proxy_pass $SERVICE_BASE_URL;|\" nginx.conf . Create the Assisted Installer Pod Once you have made any adjustments to ports as necessary, you can create the assisted-installer pod. podman pod create --name assisted-installer -p 5432:5432,8080:8080,8090:8090 NOTE The ports allocated to the assisted-installer should be updated to reflect any changes required for your configuration. 5432 is the port for Database communication 8080 is the port for accessing the Assisted Installer's UI 8090 is the port referenced in SERVICE_BASE_URL ; the URL used by the Assisted Installer's agent to talk back to the assisted-service. Start PostgreSQL Use podman to run postgreSQL. podman run -dt --pod assisted-installer \\ --name db \\ --env-file onprem-environment \\ --pull always \\ quay.io/ocpmetal/postgresql-12-centos7 NOTE * onprem-environment is the file downloaded and modified previously Start Assisted Service Use podman to start the Assisted Service. podman run -dt --pod assisted-installer \\ --name installer \\ --env-file onprem-environment \\ --pull always \\ --restart always \\ quay.io/ocpmetal/assisted-service:latest /assisted-service NOTE * onprem-environment is the file downloaded and modified previously * If you modified the port for SERVICE_BASE_URL you would add --port ${SERVICE_API_PORT} Start Assisted Installer UI podman run -dt --pod assisted-installer \\ --name ui \\ --env-file onprem-environment \\ --pull always \\ -v ${PWD}/nginx.conf:/opt/bitnami/nginx/conf/server_blocks/nginx.conf:z \\ quay.io/edge-infrastructure/assisted-installer-ui:latest NOTE * onprem-environment is the file downloaded and modified previously * $(PWD)/nginx.conf references the previously downloaded -- and potentially modified -- nginx.conf Accessing the Assisted Installer At this stage, you should be able to access the Assisted Installer UI at http://localhost:8080","title":"Installer stand alone"},{"location":"installer-stand-alone/#assisted-installer-stand-alone","text":"This document describes the process of running the Assisted Installer in stand-alone mode via podman .","title":"Assisted Installer Stand-Alone"},{"location":"installer-stand-alone/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"installer-stand-alone/#openshift-user-pull-secret","text":"You will need a valid OpenShift user pull secret. Copy or download the pull secret from https://console.redhat.com/openshift/install/pull-secret","title":"OpenShift User Pull Secret"},{"location":"installer-stand-alone/#running-the-assisted-installer-using-podman","text":"","title":"Running the Assisted Installer using Podman"},{"location":"installer-stand-alone/#environment","text":"The first thing you will need to do is grab the onprem-environment file. Once you have this file, source it. source onprem-environment NOTE * The remainder of this document relies on the values stored in onprem-environment being set in the shell. * The SERVICE_BASE_URL is the ip:port where the assisted-service API is being served. The Assisted Installer's agent uses the SERVICE_BASE_URL to talk back to the API.","title":"Environment"},{"location":"installer-stand-alone/#nginx-configuration","text":"Once you have sourced onprem-environment , you will need to grab the nginx.conf used to configure the Assisted Installer's UI. There are two fields of note: listen 8080; refers to the port used to access the Assisted Installer's UI. As an example, if you wanted the UI to listen on port 9090 to avoid conflict with a port already used on the host you would sed -i \"s|listen.*;|listen 9090;|\" nginx.conf . proxy_pass http://localhost:8090; is the default value of SERVICE_BASE_URL . You could update this with, sed -i \"s|proxy_pass.*;|proxy_pass $SERVICE_BASE_URL;|\" nginx.conf .","title":"NGINX Configuration"},{"location":"installer-stand-alone/#create-the-assisted-installer-pod","text":"Once you have made any adjustments to ports as necessary, you can create the assisted-installer pod. podman pod create --name assisted-installer -p 5432:5432,8080:8080,8090:8090 NOTE The ports allocated to the assisted-installer should be updated to reflect any changes required for your configuration. 5432 is the port for Database communication 8080 is the port for accessing the Assisted Installer's UI 8090 is the port referenced in SERVICE_BASE_URL ; the URL used by the Assisted Installer's agent to talk back to the assisted-service.","title":"Create the Assisted Installer Pod"},{"location":"installer-stand-alone/#start-postgresql","text":"Use podman to run postgreSQL. podman run -dt --pod assisted-installer \\ --name db \\ --env-file onprem-environment \\ --pull always \\ quay.io/ocpmetal/postgresql-12-centos7 NOTE * onprem-environment is the file downloaded and modified previously","title":"Start PostgreSQL"},{"location":"installer-stand-alone/#start-assisted-service","text":"Use podman to start the Assisted Service. podman run -dt --pod assisted-installer \\ --name installer \\ --env-file onprem-environment \\ --pull always \\ --restart always \\ quay.io/ocpmetal/assisted-service:latest /assisted-service NOTE * onprem-environment is the file downloaded and modified previously * If you modified the port for SERVICE_BASE_URL you would add --port ${SERVICE_API_PORT}","title":"Start Assisted Service"},{"location":"installer-stand-alone/#start-assisted-installer-ui","text":"podman run -dt --pod assisted-installer \\ --name ui \\ --env-file onprem-environment \\ --pull always \\ -v ${PWD}/nginx.conf:/opt/bitnami/nginx/conf/server_blocks/nginx.conf:z \\ quay.io/edge-infrastructure/assisted-installer-ui:latest NOTE * onprem-environment is the file downloaded and modified previously * $(PWD)/nginx.conf references the previously downloaded -- and potentially modified -- nginx.conf","title":"Start Assisted Installer UI"},{"location":"installer-stand-alone/#accessing-the-assisted-installer","text":"At this stage, you should be able to access the Assisted Installer UI at http://localhost:8080","title":"Accessing the Assisted Installer"},{"location":"ocm-integration/","text":"OCM Integration OpenShift Cluster Manager (OCM) is a managed service where users can install, operate and upgrade Red Hat OpenShift 4 clusters. This document describes the integration was done such that Assisted Installed (AI) clusters are shown as part of a user's clusters in the OCM UI. To achieve this, the assisted-installer users the OCM client to make several calls during a cluster's installation. AMS AMS is the micro-service in OCM which holds the users' clusters list, which is indeed a list of Subscriptions AMS objects, and handles the authZ for assisted-service API calls. Cluster lifecycle Cluster registration On cluster registration, the service will create an AMS subscription for the cluster with some initial values: status: \"Reserved\" cluster_id: The cluster id registered in AI DB. display_name: The cluster name in AI DB. product_id: \"OCP\" product_category: \"AssistedInstall\" Cluster is renamed In this case, the assisted-service will patch the subscription with the new cluster name. Cluster installation The assisted-service contacts AMS at several points during the cluster installation process. Once an openshift_cluster_id is generated during the preparing-for-installation state, the service will patch the subscription with: external_cluster_id: Openshift cluster ID Later on, when console operator is installed during the finalizing state, the service will patch the subscription with: console_url: Cluster's console URL Finally, when the cluster is successfully installed, the service will patch the subscription with: status: \"Active\" If installation fails, the subscription's status remains Reserved , therefore, the subscription stays untouched in AMS until the cluster is deleted from AI. In addition, in case external_cluster_id was already updated in the subscription, it will remain obsolete until the cluster is deleted from AI or the user restarts the installation, a new openshift_cluster_id is generated and the obsolete id that the subscription currently holds is overwritten by the new one. Cluster is sending metrics to Openshift Telemeter Once metrics from the installed cluster reach the Telemeter server, Telemeter will notify AMS and AMS will search for a subscription with a matching external_cluster_id (which is included in the metrics that the cluster sends). If it finds such a subscription it will add a metrics field to the subscription, otherwise, it will create a new subscription for that \"unsubscribed\" cluster but in this case, all the data patched by AI will be missing - this should be a bug indication. Cluster deletion On cluster deregistration, we have 2 flows: Clusters with Active subscription Those clusters are fully installed and are running on the clients' machines, therefore, when deleting those clusters from the service, whether is deleted by the user or by GC, the subscription is left untouched and the client will continue to see his cluster on the OCM UI. AMS is refreshing subscriptions periodically and if a running cluster stop delivering metrics for some time it will change its subscription status to \"Stale\" and after a longer period to Archived Clusters with Reserved subscription Those clusters where not installed for some reason and AMS won't monitor those subscriptions, therefore, it is AI's responsibility to delete those subscriptions in AMS if the cluster is deleted from the service. How to see the subscription in AMS using the OCM-cli You can sign-in to AMS using the ocm-cli in order to get information regarding your subscriptions. First, follow how to install ocm-cli. Then you need to log in to your user: ocm login --token <your token from https://console.redhat.com/openshift/token> You can also use assisted-service service-account authZ to get subscription that are owned by other users using the following command: ocm login --client-id <id> --client-secret <secret> --url=<url> For the cloud use: --client-id assisted-installer-int, --url=https://api.integration.openshift.com --client-id assisted-installer-stage, --url=https://api.stage.openshift.com --client-id assisted-installer-prod, --url=https://api.openshift.com Then you can query AMS for data (use jq to process the result): // get a list of 100 subscriptions max ocm get subscriptions // get a specific subscription, you can find 'ams_subscription_id' in the cluster metadata ocm get subscription <subscription ID> // you can filter subscriptions using the --parameter flag: ocm get subscriptions --parameter search=\"cluster_id = '<cluster ID>'\" ocm get subscriptions --parameter search=\"external_cluster_id = '<external cluster ID>'\" ocm get subscriptions --parameter search=\"status = 'Active'\" ... This is how a full subscription looks likes after all the steps above { \"id\": \"1svZIyseCY2KM9J1V0OYaAIxWjB\", \"kind\": \"Subscription\", \"href\": \"/api/accounts_mgmt/v1/subscriptions/1svZIyseCY2KM9J1V0OYaAIxWjB\", \"plan\": { \"id\": \"OCP-AssistedInstall\", \"kind\": \"Plan\", \"href\": \"/api/accounts_mgmt/v1/plans/OCP-AssistedInstall\", \"type\": \"OCP\", \"category\": \"AssistedInstall\" }, \"cluster_id\": \"fcf4c3c2-79a0-422c-8754-27cf02dfa9d2\", \"external_cluster_id\": \"da1c2141-9aaf-477b-b061-ab9cf5746ae9\", \"organization_id\": \"1gEOo7TCnW5JGwsw0ULeUH4l53m\", \"last_telemetry_date\": \"2021-05-23T08:00:18.145764Z\", \"created_at\": \"2021-05-23T07:29:07.979367Z\", \"updated_at\": \"2021-05-23T08:02:38.46325Z\", \"support_level\": \"Eval\", \"display_name\": \"assisted-test-cluster-22ac517-assisted-installer\", \"creator\": { \"id\": \"1gEOnuuPzqAUbLNV5QAoS4EhYfy\", \"kind\": \"Account\", \"href\": \"/api/accounts_mgmt/v1/accounts/1gEOnuuPzqAUbLNV5QAoS4EhYfy\" }, \"managed\": false, \"status\": \"Active\", \"provenance\": \"Provisioning\", \"last_reconcile_date\": \"0001-01-01T00:00:00Z\", \"console_url\": \"https://console-openshift-console.apps.assisted-test-cluster-22ac517-assisted-installer.redhat.com\", \"last_released_at\": \"0001-01-01T00:00:00Z\", \"metrics\": [ { \"health_state\": \"healthy\", \"query_timestamp\": \"2021-05-23T08:15:07Z\", \"memory\": { \"updated_timestamp\": \"2021-05-23T08:15:07.824Z\", \"used\": { \"value\": 23618293760, \"unit\": \"B\" }, \"total\": { \"value\": 52269928448, \"unit\": \"B\" } }, \"cpu\": { \"updated_timestamp\": \"2021-05-23T08:15:13.033Z\", \"used\": { \"value\": 2.99952380952381, \"unit\": \"\" }, \"total\": { \"value\": 12, \"unit\": \"\" } }, \"sockets\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"compute_nodes_memory\": { \"updated_timestamp\": \"1970-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"B\" }, \"total\": { \"value\": 0, \"unit\": \"B\" } }, \"compute_nodes_cpu\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"compute_nodes_sockets\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"storage\": { \"updated_timestamp\": \"1970-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"B\" }, \"total\": { \"value\": 0, \"unit\": \"B\" } }, \"nodes\": { \"total\": 3, \"master\": 3 }, \"operating_system\": \"\", \"upgrade\": { \"updated_timestamp\": \"2021-05-23T08:15:12.581Z\", \"available\": true }, \"state\": \"ready\", \"state_description\": \"\", \"openshift_version\": \"4.7.9\", \"cloud_provider\": \"baremetal\", \"region\": \"\", \"console_url\": \"https://console-openshift-console.apps.assisted-test-cluster-22ac517-assisted-installer.redhat.com\", \"critical_alerts_firing\": 0, \"operators_condition_failing\": 0, \"subscription_cpu_total\": 0, \"subscription_socket_total\": 0, \"subscription_obligation_exists\": 1, \"cluster_type\": \"\" } ], \"cloud_provider_id\": \"baremetal\", \"trial_end_date\": \"0001-01-01T00:00:00Z\" }","title":"OCM Integration"},{"location":"ocm-integration/#ocm-integration","text":"OpenShift Cluster Manager (OCM) is a managed service where users can install, operate and upgrade Red Hat OpenShift 4 clusters. This document describes the integration was done such that Assisted Installed (AI) clusters are shown as part of a user's clusters in the OCM UI. To achieve this, the assisted-installer users the OCM client to make several calls during a cluster's installation.","title":"OCM Integration"},{"location":"ocm-integration/#ams","text":"AMS is the micro-service in OCM which holds the users' clusters list, which is indeed a list of Subscriptions AMS objects, and handles the authZ for assisted-service API calls.","title":"AMS"},{"location":"ocm-integration/#cluster-lifecycle","text":"","title":"Cluster lifecycle"},{"location":"ocm-integration/#cluster-registration","text":"On cluster registration, the service will create an AMS subscription for the cluster with some initial values: status: \"Reserved\" cluster_id: The cluster id registered in AI DB. display_name: The cluster name in AI DB. product_id: \"OCP\" product_category: \"AssistedInstall\"","title":"Cluster registration"},{"location":"ocm-integration/#cluster-is-renamed","text":"In this case, the assisted-service will patch the subscription with the new cluster name.","title":"Cluster is renamed"},{"location":"ocm-integration/#cluster-installation","text":"The assisted-service contacts AMS at several points during the cluster installation process. Once an openshift_cluster_id is generated during the preparing-for-installation state, the service will patch the subscription with: external_cluster_id: Openshift cluster ID Later on, when console operator is installed during the finalizing state, the service will patch the subscription with: console_url: Cluster's console URL Finally, when the cluster is successfully installed, the service will patch the subscription with: status: \"Active\" If installation fails, the subscription's status remains Reserved , therefore, the subscription stays untouched in AMS until the cluster is deleted from AI. In addition, in case external_cluster_id was already updated in the subscription, it will remain obsolete until the cluster is deleted from AI or the user restarts the installation, a new openshift_cluster_id is generated and the obsolete id that the subscription currently holds is overwritten by the new one.","title":"Cluster installation"},{"location":"ocm-integration/#cluster-is-sending-metrics-to-openshift-telemeter","text":"Once metrics from the installed cluster reach the Telemeter server, Telemeter will notify AMS and AMS will search for a subscription with a matching external_cluster_id (which is included in the metrics that the cluster sends). If it finds such a subscription it will add a metrics field to the subscription, otherwise, it will create a new subscription for that \"unsubscribed\" cluster but in this case, all the data patched by AI will be missing - this should be a bug indication.","title":"Cluster is sending metrics to Openshift Telemeter"},{"location":"ocm-integration/#cluster-deletion","text":"On cluster deregistration, we have 2 flows:","title":"Cluster deletion"},{"location":"ocm-integration/#clusters-with-active-subscription","text":"Those clusters are fully installed and are running on the clients' machines, therefore, when deleting those clusters from the service, whether is deleted by the user or by GC, the subscription is left untouched and the client will continue to see his cluster on the OCM UI. AMS is refreshing subscriptions periodically and if a running cluster stop delivering metrics for some time it will change its subscription status to \"Stale\" and after a longer period to Archived","title":"Clusters with Active subscription"},{"location":"ocm-integration/#clusters-with-reserved-subscription","text":"Those clusters where not installed for some reason and AMS won't monitor those subscriptions, therefore, it is AI's responsibility to delete those subscriptions in AMS if the cluster is deleted from the service.","title":"Clusters with Reserved subscription"},{"location":"ocm-integration/#how-to-see-the-subscription-in-ams-using-the-ocm-cli","text":"You can sign-in to AMS using the ocm-cli in order to get information regarding your subscriptions. First, follow how to install ocm-cli. Then you need to log in to your user: ocm login --token <your token from https://console.redhat.com/openshift/token> You can also use assisted-service service-account authZ to get subscription that are owned by other users using the following command: ocm login --client-id <id> --client-secret <secret> --url=<url> For the cloud use: --client-id assisted-installer-int, --url=https://api.integration.openshift.com --client-id assisted-installer-stage, --url=https://api.stage.openshift.com --client-id assisted-installer-prod, --url=https://api.openshift.com Then you can query AMS for data (use jq to process the result): // get a list of 100 subscriptions max ocm get subscriptions // get a specific subscription, you can find 'ams_subscription_id' in the cluster metadata ocm get subscription <subscription ID> // you can filter subscriptions using the --parameter flag: ocm get subscriptions --parameter search=\"cluster_id = '<cluster ID>'\" ocm get subscriptions --parameter search=\"external_cluster_id = '<external cluster ID>'\" ocm get subscriptions --parameter search=\"status = 'Active'\" ...","title":"How to see the subscription in AMS using the OCM-cli"},{"location":"ocm-integration/#this-is-how-a-full-subscription-looks-likes-after-all-the-steps-above","text":"{ \"id\": \"1svZIyseCY2KM9J1V0OYaAIxWjB\", \"kind\": \"Subscription\", \"href\": \"/api/accounts_mgmt/v1/subscriptions/1svZIyseCY2KM9J1V0OYaAIxWjB\", \"plan\": { \"id\": \"OCP-AssistedInstall\", \"kind\": \"Plan\", \"href\": \"/api/accounts_mgmt/v1/plans/OCP-AssistedInstall\", \"type\": \"OCP\", \"category\": \"AssistedInstall\" }, \"cluster_id\": \"fcf4c3c2-79a0-422c-8754-27cf02dfa9d2\", \"external_cluster_id\": \"da1c2141-9aaf-477b-b061-ab9cf5746ae9\", \"organization_id\": \"1gEOo7TCnW5JGwsw0ULeUH4l53m\", \"last_telemetry_date\": \"2021-05-23T08:00:18.145764Z\", \"created_at\": \"2021-05-23T07:29:07.979367Z\", \"updated_at\": \"2021-05-23T08:02:38.46325Z\", \"support_level\": \"Eval\", \"display_name\": \"assisted-test-cluster-22ac517-assisted-installer\", \"creator\": { \"id\": \"1gEOnuuPzqAUbLNV5QAoS4EhYfy\", \"kind\": \"Account\", \"href\": \"/api/accounts_mgmt/v1/accounts/1gEOnuuPzqAUbLNV5QAoS4EhYfy\" }, \"managed\": false, \"status\": \"Active\", \"provenance\": \"Provisioning\", \"last_reconcile_date\": \"0001-01-01T00:00:00Z\", \"console_url\": \"https://console-openshift-console.apps.assisted-test-cluster-22ac517-assisted-installer.redhat.com\", \"last_released_at\": \"0001-01-01T00:00:00Z\", \"metrics\": [ { \"health_state\": \"healthy\", \"query_timestamp\": \"2021-05-23T08:15:07Z\", \"memory\": { \"updated_timestamp\": \"2021-05-23T08:15:07.824Z\", \"used\": { \"value\": 23618293760, \"unit\": \"B\" }, \"total\": { \"value\": 52269928448, \"unit\": \"B\" } }, \"cpu\": { \"updated_timestamp\": \"2021-05-23T08:15:13.033Z\", \"used\": { \"value\": 2.99952380952381, \"unit\": \"\" }, \"total\": { \"value\": 12, \"unit\": \"\" } }, \"sockets\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"compute_nodes_memory\": { \"updated_timestamp\": \"1970-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"B\" }, \"total\": { \"value\": 0, \"unit\": \"B\" } }, \"compute_nodes_cpu\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"compute_nodes_sockets\": { \"updated_timestamp\": \"0001-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"\" }, \"total\": { \"value\": 0, \"unit\": \"\" } }, \"storage\": { \"updated_timestamp\": \"1970-01-01T00:00:00Z\", \"used\": { \"value\": 0, \"unit\": \"B\" }, \"total\": { \"value\": 0, \"unit\": \"B\" } }, \"nodes\": { \"total\": 3, \"master\": 3 }, \"operating_system\": \"\", \"upgrade\": { \"updated_timestamp\": \"2021-05-23T08:15:12.581Z\", \"available\": true }, \"state\": \"ready\", \"state_description\": \"\", \"openshift_version\": \"4.7.9\", \"cloud_provider\": \"baremetal\", \"region\": \"\", \"console_url\": \"https://console-openshift-console.apps.assisted-test-cluster-22ac517-assisted-installer.redhat.com\", \"critical_alerts_firing\": 0, \"operators_condition_failing\": 0, \"subscription_cpu_total\": 0, \"subscription_socket_total\": 0, \"subscription_obligation_exists\": 1, \"cluster_type\": \"\" } ], \"cloud_provider_id\": \"baremetal\", \"trial_end_date\": \"0001-01-01T00:00:00Z\" }","title":"This is how a full subscription looks likes after all the steps above"},{"location":"operator/","text":"Operator build and deployment Prerequisites operator-sdk https://sdk.operatorframework.io/docs/installation/ kustomize https://github.com/kubernetes-sigs/kustomize/releases opm https://github.com/operator-framework/operator-registry/releases Building the operator bundle (optional) For development and testing purposes it may be beneficial to build the operator bundle and index images. If you don't need to build it, just skip to Deploying the Operator . Build the bundle: export BUNDLE_IMAGE=quay.io/${QUAY_NAMESPACE}/assisted-service-operator-bundle:${TAG} skipper make operator-bundle-build NOTE It is possible to run make command without using Skipper. In such a scenario all the required dependencies can be installed using the setup_env.sh script . Deploying the operator The operator must be deployed to the assisted-installer namespace. Create the namespace. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Namespace metadata: name: assisted-installer labels: name: assisted-installer EOF Having the ClusterDeployment CRD installed is a prerequisite. Install Hive, if it has not already been installed. cat <<EOF | kubectl create -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: hive-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: hive-operator source: community-operators sourceNamespace: openshift-marketplace EOF Deploy the operator using the operator-sdk: operator-sdk run bundle \\ --namespace assisted-installer \\ ${BUNDLE_IMAGE:-quay.io/ocpmetal/assisted-service-operator-bundle:latest} Now you should see the infrastructure-operator deployment running in the assisted-installer namespace. NOTE operator-sdk cleanup --namespace assisted-installer assisted-service-operator Is an effective way to remove the operator when installed via operator-sdk run . Creating an AgentServiceConfig Resource The Assisted Service is deployed by creating an AgentServiceConfig. At a minimum, you must specify the databaseStorage and filesystemStorage to be used. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi EOF Configuring the Assisted Service Deployment Via Subscription The operator subscription can be used to configure the images used in the assisted-service deployment and the installer + controller + agent images used by the assisted-service. cat <<EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: assisted-service-operator namespace: assisted-installer spec: config: env: - name: SERVICE_IMAGE value: ${SERVICE_IMAGE} - name: DATABASE_IMAGE value: ${DATABASE_IMAGE} - name: AGENT_IMAGE value: ${AGENT_IMAGE} - name: CONTROLLER_IMAGE value: ${CONTROLLER_IMAGE} - name: INSTALLER_IMAGE value: ${INSTALLER_IMAGE} EOF NOTE The default channel for the assisted-service-operator package, here and in community-operators , is \"alpha\" so we do not include it in the Subscription. Available Operator System Images Locations of OS Images to be used when generating the discovery ISOs for different OpenShift versions can be specified via the osImages field on the AgentServiceConfig. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi osImages: - openshiftVersion: \"4.6\" version: \"46.82.202012051820-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/4.6.8/rhcos-4.6.8-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/4.6.8/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\" - openshiftVersion: \"4.7\" version: \"47.83.202103251640-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-4.7.7-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\" - openshiftVersion: \"4.8\" version: \"47.83.202103251640-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-4.7.7-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\" Available Olm Operators Must Gather Images Locations of Must Gather images to be used when gathering information on failed olm operators can be specified via the mustGatherImages field on the AgentServiceConfig. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: ... mustGatherImages: - openshiftVersion: '4.8' name: \"cnv\" url: \"registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel8:v2.6.5\" - openshiftVersion: '4.8' name: \"ocs\" url: \"registry.redhat.io/ocs4/ocs-must-gather-rhel8\" - openshiftVersion: '4.8' name: \"lso\" url: \"registry.redhat.io/openshift4/ose-local-storage-mustgather-rhel8\" Specifying Environmental Variables via ConfigMap It is possible to specify a ConfigMap to be mounted into the assisted-service container as environment variables by adding an \"unsupported.agent-install.openshift.io/assisted-service-configmap\" annotation to the AgentServiceConfig specifying the name of the configmap to be used. This ConfigMap must exist in the namespace where the operator is installed. Simply create a ConfigMap in the assisted-installer namespace: cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: my-assisted-service-config namespace: assisted-installer data: LOG_LEVEL: \"debug\" EOF Add the annotation to the AgentServiceConfig: oc annotate --overwrite AgentServiceConfig agent unsupported.agent-install.openshift.io/assisted-service-configmap=my-assisted-service-config NOTE After modifying content of the ConfigMap a new rollout of the Deployment has to be forced. This can be done with oc rollout restart deployment/assisted-service -n assisted-installer Mirror Registry Configuration A ConfigMap can be used to configure assisted service to create installations using mirrored content. The ConfigMap contains two keys: ca-bundle.crt - This key contains the contents of the certificate for accessing the mirror registry, if necessary. It may be a certificate bundle and is defined as a single string. registries.conf - This key contains the contents of the registries.conf file that configures mappings to the mirror registry. The mirror registry configuration changes the discovery image's ignition config, with ca-bundle.crt written out to /etc/pki/ca-trust/source/anchors/domain.crt and with registries.conf written out to /etc/containers/registries.conf . The configuration also changes the install-config.yaml file used to install a new cluster, with the contents of ca-bundle.crt added to additionalTrustBundle and with the registries defined registries.conf added to imageContentSources as mirrors. To configure the mirror registry, first create and upload the ConfigMap containing the ca-bundle.crt and registries.conf keys. cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mirror-registry-config-map namespace: \"assisted-installer\" labels: app: assisted-service data: ca-bundle.crt: | -----BEGIN CERTIFICATE----- certificate contents -----END CERTIFICATE----- registries.conf: | unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"] [[registry]] prefix = \"\" location = \"quay.io/ocpmetal\" mirror-by-digest-only = false [[registry.mirror]] location = \"mirror1.registry.corp.com:5000/ocpmetal\" EOF NOTE The ConfigMap should be installed in the same namespace as the infrastructure-operator (ie. assisted-installer ). Registries defined in the registries.conf file should use \"mirror-by-digest-only = false\" mode. Registries defined in the registries.conf must be scoped by repository and not by registry. In the above example, quay.io/ocpmetal and mirror1.registry.corp.com:5000/ocpmetal are both scoped by the ocpmetal repository and this is a valid configuration. In the example below, removing the repository ocpmetal from location is an invalid configuration and will not pass openshift-installer validation: # invalid configuration [[registry]] prefix = \"\" location = \"quay.io\" mirror-by-digest-only = false [[registry.mirror]] location = \"mirror1.registry.corp.com:5000\" Then set the mirrorRegistryRef in the spec of AgentServiceConfig to the name of uploaded ConfigMap. Example: cat <<EOF | kubectl apply -f - apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi mirrorRegistryRef: name: mirror-registry-config-map EOF For more details on how to specify the CR, see AgentServiceConfig CRD .","title":"Operator build and deployment"},{"location":"operator/#operator-build-and-deployment","text":"","title":"Operator build and deployment"},{"location":"operator/#prerequisites","text":"operator-sdk https://sdk.operatorframework.io/docs/installation/ kustomize https://github.com/kubernetes-sigs/kustomize/releases opm https://github.com/operator-framework/operator-registry/releases","title":"Prerequisites"},{"location":"operator/#building-the-operator-bundle-optional","text":"For development and testing purposes it may be beneficial to build the operator bundle and index images. If you don't need to build it, just skip to Deploying the Operator . Build the bundle: export BUNDLE_IMAGE=quay.io/${QUAY_NAMESPACE}/assisted-service-operator-bundle:${TAG} skipper make operator-bundle-build NOTE It is possible to run make command without using Skipper. In such a scenario all the required dependencies can be installed using the setup_env.sh script .","title":"Building the operator bundle (optional)"},{"location":"operator/#deploying-the-operator","text":"The operator must be deployed to the assisted-installer namespace. Create the namespace. cat <<EOF | kubectl create -f - apiVersion: v1 kind: Namespace metadata: name: assisted-installer labels: name: assisted-installer EOF Having the ClusterDeployment CRD installed is a prerequisite. Install Hive, if it has not already been installed. cat <<EOF | kubectl create -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: hive-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: hive-operator source: community-operators sourceNamespace: openshift-marketplace EOF Deploy the operator using the operator-sdk: operator-sdk run bundle \\ --namespace assisted-installer \\ ${BUNDLE_IMAGE:-quay.io/ocpmetal/assisted-service-operator-bundle:latest} Now you should see the infrastructure-operator deployment running in the assisted-installer namespace. NOTE operator-sdk cleanup --namespace assisted-installer assisted-service-operator Is an effective way to remove the operator when installed via operator-sdk run .","title":"Deploying the operator"},{"location":"operator/#creating-an-agentserviceconfig-resource","text":"The Assisted Service is deployed by creating an AgentServiceConfig. At a minimum, you must specify the databaseStorage and filesystemStorage to be used. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi EOF","title":"Creating an AgentServiceConfig Resource"},{"location":"operator/#configuring-the-assisted-service-deployment","text":"","title":"Configuring the Assisted Service Deployment"},{"location":"operator/#via-subscription","text":"The operator subscription can be used to configure the images used in the assisted-service deployment and the installer + controller + agent images used by the assisted-service. cat <<EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: assisted-service-operator namespace: assisted-installer spec: config: env: - name: SERVICE_IMAGE value: ${SERVICE_IMAGE} - name: DATABASE_IMAGE value: ${DATABASE_IMAGE} - name: AGENT_IMAGE value: ${AGENT_IMAGE} - name: CONTROLLER_IMAGE value: ${CONTROLLER_IMAGE} - name: INSTALLER_IMAGE value: ${INSTALLER_IMAGE} EOF NOTE The default channel for the assisted-service-operator package, here and in community-operators , is \"alpha\" so we do not include it in the Subscription.","title":"Via Subscription"},{"location":"operator/#available-operator-system-images","text":"Locations of OS Images to be used when generating the discovery ISOs for different OpenShift versions can be specified via the osImages field on the AgentServiceConfig. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi osImages: - openshiftVersion: \"4.6\" version: \"46.82.202012051820-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/4.6.8/rhcos-4.6.8-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/4.6.8/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\" - openshiftVersion: \"4.7\" version: \"47.83.202103251640-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-4.7.7-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\" - openshiftVersion: \"4.8\" version: \"47.83.202103251640-0\" url: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-4.7.7-x86_64-live.x86_64.iso\" rootFSUrl: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.7/rhcos-live-rootfs.x86_64.img\", cpuArchitecture: \"x86_64\"","title":"Available Operator System Images"},{"location":"operator/#available-olm-operators-must-gather-images","text":"Locations of Must Gather images to be used when gathering information on failed olm operators can be specified via the mustGatherImages field on the AgentServiceConfig. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: ... mustGatherImages: - openshiftVersion: '4.8' name: \"cnv\" url: \"registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel8:v2.6.5\" - openshiftVersion: '4.8' name: \"ocs\" url: \"registry.redhat.io/ocs4/ocs-must-gather-rhel8\" - openshiftVersion: '4.8' name: \"lso\" url: \"registry.redhat.io/openshift4/ose-local-storage-mustgather-rhel8\"","title":"Available Olm Operators Must Gather Images"},{"location":"operator/#specifying-environmental-variables-via-configmap","text":"It is possible to specify a ConfigMap to be mounted into the assisted-service container as environment variables by adding an \"unsupported.agent-install.openshift.io/assisted-service-configmap\" annotation to the AgentServiceConfig specifying the name of the configmap to be used. This ConfigMap must exist in the namespace where the operator is installed. Simply create a ConfigMap in the assisted-installer namespace: cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: my-assisted-service-config namespace: assisted-installer data: LOG_LEVEL: \"debug\" EOF Add the annotation to the AgentServiceConfig: oc annotate --overwrite AgentServiceConfig agent unsupported.agent-install.openshift.io/assisted-service-configmap=my-assisted-service-config NOTE After modifying content of the ConfigMap a new rollout of the Deployment has to be forced. This can be done with oc rollout restart deployment/assisted-service -n assisted-installer","title":"Specifying Environmental Variables via ConfigMap"},{"location":"operator/#mirror-registry-configuration","text":"A ConfigMap can be used to configure assisted service to create installations using mirrored content. The ConfigMap contains two keys: ca-bundle.crt - This key contains the contents of the certificate for accessing the mirror registry, if necessary. It may be a certificate bundle and is defined as a single string. registries.conf - This key contains the contents of the registries.conf file that configures mappings to the mirror registry. The mirror registry configuration changes the discovery image's ignition config, with ca-bundle.crt written out to /etc/pki/ca-trust/source/anchors/domain.crt and with registries.conf written out to /etc/containers/registries.conf . The configuration also changes the install-config.yaml file used to install a new cluster, with the contents of ca-bundle.crt added to additionalTrustBundle and with the registries defined registries.conf added to imageContentSources as mirrors. To configure the mirror registry, first create and upload the ConfigMap containing the ca-bundle.crt and registries.conf keys. cat <<EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: mirror-registry-config-map namespace: \"assisted-installer\" labels: app: assisted-service data: ca-bundle.crt: | -----BEGIN CERTIFICATE----- certificate contents -----END CERTIFICATE----- registries.conf: | unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"] [[registry]] prefix = \"\" location = \"quay.io/ocpmetal\" mirror-by-digest-only = false [[registry.mirror]] location = \"mirror1.registry.corp.com:5000/ocpmetal\" EOF NOTE The ConfigMap should be installed in the same namespace as the infrastructure-operator (ie. assisted-installer ). Registries defined in the registries.conf file should use \"mirror-by-digest-only = false\" mode. Registries defined in the registries.conf must be scoped by repository and not by registry. In the above example, quay.io/ocpmetal and mirror1.registry.corp.com:5000/ocpmetal are both scoped by the ocpmetal repository and this is a valid configuration. In the example below, removing the repository ocpmetal from location is an invalid configuration and will not pass openshift-installer validation: # invalid configuration [[registry]] prefix = \"\" location = \"quay.io\" mirror-by-digest-only = false [[registry.mirror]] location = \"mirror1.registry.corp.com:5000\" Then set the mirrorRegistryRef in the spec of AgentServiceConfig to the name of uploaded ConfigMap. Example: cat <<EOF | kubectl apply -f - apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig metadata: name: agent spec: databaseStorage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi filesystemStorage: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi mirrorRegistryRef: name: mirror-registry-config-map EOF For more details on how to specify the CR, see AgentServiceConfig CRD .","title":"Mirror Registry Configuration"},{"location":"pull_request_template/","text":"Assisted Pull Request Description List all the issues related to this PR [ ] New Feature [ ] Bug fix [ ] Tests [ ] Documentation [ ] CI/CD What environments does this code impact? [ ] Automation (CI, tools, etc) [ ] Cloud [ ] Operator Managed Deployments [x] None How was this code tested? [ ] assisted-test-infra environment [ ] dev-scripts environment [ ] Reviewer's test appreciated [ ] Waiting for CI to do a full test run [ ] Manual (Elaborate on how it was tested) [x] No tests needed Assignees /cc @ /cc @ Checklist [ ] Title and description added to both, commit and PR. [ ] Relevant issues have been associated (see CONTRIBUTING guide) [ ] Reviewers have been listed [ ] This change does not require a documentation update (docstring, docs , README, etc) [ ] Does this change include unit-tests (note that code changes require unit-tests) Reviewers Checklist [ ] Are the title and description (in both PR and commit) meaningful and clear? [ ] Is there a bug required (and linked) for this change? [ ] Should this PR be backported?","title":"Assisted Pull Request"},{"location":"pull_request_template/#assisted-pull-request","text":"","title":"Assisted Pull Request"},{"location":"pull_request_template/#description","text":"","title":"Description"},{"location":"pull_request_template/#list-all-the-issues-related-to-this-pr","text":"[ ] New Feature [ ] Bug fix [ ] Tests [ ] Documentation [ ] CI/CD","title":"List all the issues related to this PR"},{"location":"pull_request_template/#what-environments-does-this-code-impact","text":"[ ] Automation (CI, tools, etc) [ ] Cloud [ ] Operator Managed Deployments [x] None","title":"What environments does this code impact?"},{"location":"pull_request_template/#how-was-this-code-tested","text":"[ ] assisted-test-infra environment [ ] dev-scripts environment [ ] Reviewer's test appreciated [ ] Waiting for CI to do a full test run [ ] Manual (Elaborate on how it was tested) [x] No tests needed","title":"How was this code tested?"},{"location":"pull_request_template/#assignees","text":"/cc @ /cc @","title":"Assignees"},{"location":"pull_request_template/#checklist","text":"[ ] Title and description added to both, commit and PR. [ ] Relevant issues have been associated (see CONTRIBUTING guide) [ ] Reviewers have been listed [ ] This change does not require a documentation update (docstring, docs , README, etc) [ ] Does this change include unit-tests (note that code changes require unit-tests)","title":"Checklist"},{"location":"pull_request_template/#reviewers-checklist","text":"[ ] Are the title and description (in both PR and commit) meaningful and clear? [ ] Is there a bug required (and linked) for this change? [ ] Should this PR be backported?","title":"Reviewers Checklist"},{"location":"set-discovery-password/","text":"Setting a password for the discovery ISO The discovery ISO generated by the Assisted Installer comes with a core user that has its password login disabled (although SSH access using the provided public key is still supported). However, it can sometimes be useful (e.g. when SSH isn't working), to login to the machine directly (with physical or BMC access), using a password. This can be done using the console login prompt that is shown after the ISO boots up. To allow that, the user must have a password set. In order to set a password, it's possible to use the Assisted Installer API which provides support for editing the discovery ISO's ignition file with custom parameters. The following script provides an example of how to use the API in order to modify the discovery ISO in a way that sets core 's password to any password of your choice. For more information about the API and its various authentication methods, see this document . Modify password script #!/bin/bash set -euo pipefail if ! ocm token 2>/dev/null >/dev/null; then echo \"Failed to run 'ocm token' command, please see the assisted_service/docs/cloud.md doc for authentication information\" exit 1 fi ## User specific configuration <----------- WANTED_PASSWORD=mypass TOKEN=$(ocm token) OCM_API_ENDPOINT=\"https://api.openshift.com/api/\" if true; then echo \"Don't forget to modify the script with your cluster ID, delete this warning after you did\" exit 1 fi CLUSTER_ID=\"243e09fb-d924-42a1-bad9-b78638b767d9\" # Copy from Assisted Installer URL DEST_ISO_FILE=\"changed_password.iso\" ############################### function log() { if [[ ! $? == 0 ]]; then echo \"Script enountered an error\" exit 1 fi } trap log EXIT echo \"Password set log\" > .set_iso_password_log DISCOVERY_IGN_URL=$OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/downloads/'files?file_name=discovery.ign' if ! curl --fail -s ${DISCOVERY_IGN_URL} -H \"Authorization: Bearer $TOKEN\" >/dev/null; then echo \"Can't seem to find a discovery.ign, please generate a discovery ISO file using the UI first\" exit 1 fi echo Downloading the original ignition file into the ORIGINAL_IGNITION variable ORIGINAL_IGNITION=$(curl --fail -s ${DISCOVERY_IGN_URL} -H \"Authorization: Bearer $TOKEN\") echo === Original ignition === >> .set_iso_password_log echo \"$ORIGINAL_IGNITION\" >> .set_iso_password_log echo ========================= >> .set_iso_password_log echo >> .set_iso_password_log echo Generating a salted hash from WANTED_PASSWORD PASS_HASH=$(mkpasswd --method=SHA-512 $WANTED_PASSWORD | tr -d '\\n') echo === Password Hash=== >> .set_iso_password_log echo \"$PASS_HASH\" >> .set_iso_password_log echo ==================== >> .set_iso_password_log echo >> .set_iso_password_log echo Modifying ORIGINAL_IGNITION to contain the new password hash rather than the previous one NEW_IGNITION=$(<<< \"$ORIGINAL_IGNITION\" jq --arg passhash $PASS_HASH '.passwd.users[0].passwordHash = $passhash') echo === New patched ignition === >> .set_iso_password_log echo \"$NEW_IGNITION\" >> .set_iso_password_log echo ============================ >> .set_iso_password_log echo >> .set_iso_password_log echo Telling service to use our patched ignition file curl --fail -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition -H \"Authorization: Bearer $TOKEN\" --request PATCH --header \"Content-Type: application/json\" --data @<(echo '{\"config\": \"replaceme\"}' | jq --rawfile ignition <(echo $NEW_IGNITION) '.config = $ignition') echo \"Done, please re-generate the ISO using the UI and download the newly generated ISO\"","title":"Setting a password for the discovery ISO"},{"location":"set-discovery-password/#setting-a-password-for-the-discovery-iso","text":"The discovery ISO generated by the Assisted Installer comes with a core user that has its password login disabled (although SSH access using the provided public key is still supported). However, it can sometimes be useful (e.g. when SSH isn't working), to login to the machine directly (with physical or BMC access), using a password. This can be done using the console login prompt that is shown after the ISO boots up. To allow that, the user must have a password set. In order to set a password, it's possible to use the Assisted Installer API which provides support for editing the discovery ISO's ignition file with custom parameters. The following script provides an example of how to use the API in order to modify the discovery ISO in a way that sets core 's password to any password of your choice. For more information about the API and its various authentication methods, see this document .","title":"Setting a password for the discovery ISO"},{"location":"set-discovery-password/#modify-password-script","text":"#!/bin/bash set -euo pipefail if ! ocm token 2>/dev/null >/dev/null; then echo \"Failed to run 'ocm token' command, please see the assisted_service/docs/cloud.md doc for authentication information\" exit 1 fi ## User specific configuration <----------- WANTED_PASSWORD=mypass TOKEN=$(ocm token) OCM_API_ENDPOINT=\"https://api.openshift.com/api/\" if true; then echo \"Don't forget to modify the script with your cluster ID, delete this warning after you did\" exit 1 fi CLUSTER_ID=\"243e09fb-d924-42a1-bad9-b78638b767d9\" # Copy from Assisted Installer URL DEST_ISO_FILE=\"changed_password.iso\" ############################### function log() { if [[ ! $? == 0 ]]; then echo \"Script enountered an error\" exit 1 fi } trap log EXIT echo \"Password set log\" > .set_iso_password_log DISCOVERY_IGN_URL=$OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/downloads/'files?file_name=discovery.ign' if ! curl --fail -s ${DISCOVERY_IGN_URL} -H \"Authorization: Bearer $TOKEN\" >/dev/null; then echo \"Can't seem to find a discovery.ign, please generate a discovery ISO file using the UI first\" exit 1 fi echo Downloading the original ignition file into the ORIGINAL_IGNITION variable ORIGINAL_IGNITION=$(curl --fail -s ${DISCOVERY_IGN_URL} -H \"Authorization: Bearer $TOKEN\") echo === Original ignition === >> .set_iso_password_log echo \"$ORIGINAL_IGNITION\" >> .set_iso_password_log echo ========================= >> .set_iso_password_log echo >> .set_iso_password_log echo Generating a salted hash from WANTED_PASSWORD PASS_HASH=$(mkpasswd --method=SHA-512 $WANTED_PASSWORD | tr -d '\\n') echo === Password Hash=== >> .set_iso_password_log echo \"$PASS_HASH\" >> .set_iso_password_log echo ==================== >> .set_iso_password_log echo >> .set_iso_password_log echo Modifying ORIGINAL_IGNITION to contain the new password hash rather than the previous one NEW_IGNITION=$(<<< \"$ORIGINAL_IGNITION\" jq --arg passhash $PASS_HASH '.passwd.users[0].passwordHash = $passhash') echo === New patched ignition === >> .set_iso_password_log echo \"$NEW_IGNITION\" >> .set_iso_password_log echo ============================ >> .set_iso_password_log echo >> .set_iso_password_log echo Telling service to use our patched ignition file curl --fail -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition -H \"Authorization: Bearer $TOKEN\" --request PATCH --header \"Content-Type: application/json\" --data @<(echo '{\"config\": \"replaceme\"}' | jq --rawfile ignition <(echo $NEW_IGNITION) '.config = $ignition') echo \"Done, please re-generate the ISO using the UI and download the newly generated ISO\"","title":"Modify password script"},{"location":"set-minimal-iso/","text":"Generating a minimal ISO The default discovery ISO generated by the Assisted Installer is a complete RHCOS ISO with a custom ignition file. Serving this large file through OOBM (e.g. BMC, iDRAC) can sometimes lead to unexpected issues. To overcome this, the Assisted Installer offers an option to generate a smaller ISO file that dynamically downloads the rest of the RHCOS rootfs via the Internet. The following script provides an example of how to use the Assisted Installer API in order to set the type of the discovery ISO to \"minimal-iso\". For more information about the API and its various authentication methods, see this document . Set discovery ISO type #!/bin/bash set -euo pipefail if ! ocm token 2>/dev/null >/dev/null; then echo \"Failed to run 'ocm token' command, please see the assisted_service/docs/cloud.md doc for authentication information\" exit 1 fi if [ -z ${SSH_KEY+x} ]; then echo 'Please set SSH_KEY to your SSH public key.' echo 'For example: export SSH_KEY=$(cat ~/.ssh/id_rsa.pub)' exit 1 fi if [ -z ${CLUSTER_ID+x} ]; then echo 'Please set CLUSTER_ID to your cluster ID, which can be found in the Assisted Installer URL' exit 1 fi ## User specific configuration <----------- TOKEN=$(ocm token) OCM_API_ENDPOINT=\"https://api.openshift.com/api/\" ############################### function log() { if [[ ! $? == 0 ]]; then echo \"Script enountered an error\" exit 1 fi } trap log EXIT echo Telling service to generate a minimal ISO with our public SSH key file curl -fail -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/downloads/image -H \"Authorization: Bearer $TOKEN\" --request POST --header \"Content-Type: application/json\" --data @<(echo '{\"image_type\": \"minimal-iso\", \"ssh_public_key\": \"\"}' | jq --rawfile pubkey <(echo -n $SSH_KEY) '.ssh_public_key = $pubkey') echo \"Done, retrieving ISO url...\" curl -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID -H \"Authorization: Bearer $TOKEN\" --request GET | jq '.image_info.download_url' -r","title":"Generating a minimal ISO"},{"location":"set-minimal-iso/#generating-a-minimal-iso","text":"The default discovery ISO generated by the Assisted Installer is a complete RHCOS ISO with a custom ignition file. Serving this large file through OOBM (e.g. BMC, iDRAC) can sometimes lead to unexpected issues. To overcome this, the Assisted Installer offers an option to generate a smaller ISO file that dynamically downloads the rest of the RHCOS rootfs via the Internet. The following script provides an example of how to use the Assisted Installer API in order to set the type of the discovery ISO to \"minimal-iso\". For more information about the API and its various authentication methods, see this document .","title":"Generating a minimal ISO"},{"location":"set-minimal-iso/#set-discovery-iso-type","text":"#!/bin/bash set -euo pipefail if ! ocm token 2>/dev/null >/dev/null; then echo \"Failed to run 'ocm token' command, please see the assisted_service/docs/cloud.md doc for authentication information\" exit 1 fi if [ -z ${SSH_KEY+x} ]; then echo 'Please set SSH_KEY to your SSH public key.' echo 'For example: export SSH_KEY=$(cat ~/.ssh/id_rsa.pub)' exit 1 fi if [ -z ${CLUSTER_ID+x} ]; then echo 'Please set CLUSTER_ID to your cluster ID, which can be found in the Assisted Installer URL' exit 1 fi ## User specific configuration <----------- TOKEN=$(ocm token) OCM_API_ENDPOINT=\"https://api.openshift.com/api/\" ############################### function log() { if [[ ! $? == 0 ]]; then echo \"Script enountered an error\" exit 1 fi } trap log EXIT echo Telling service to generate a minimal ISO with our public SSH key file curl -fail -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID/downloads/image -H \"Authorization: Bearer $TOKEN\" --request POST --header \"Content-Type: application/json\" --data @<(echo '{\"image_type\": \"minimal-iso\", \"ssh_public_key\": \"\"}' | jq --rawfile pubkey <(echo -n $SSH_KEY) '.ssh_public_key = $pubkey') echo \"Done, retrieving ISO url...\" curl -s $OCM_API_ENDPOINT/assisted-install/v1/clusters/$CLUSTER_ID -H \"Authorization: Bearer $TOKEN\" --request GET | jq '.image_info.download_url' -r","title":"Set discovery ISO type"},{"location":"dev/debug/","text":"assisted-service Debug Setup To debug the application first set the DEBUG_SERVICE environment variable to any nonempty value export DEBUG_SERVICE=true The default remote debug port is 40000 but its configurable by setting the DEBUG_SERVICE_PORT env variable: export DEBUG_SERVICE_PORT=8765 Build the image and push to your local k8s: skipper make update-local-image Note! When running this target in a DEBUG mode, \\ it updates the existing assisted-service image with the latest assisted-service code, \\ but if the Dockerfile itself has been changed you are responsible to update the image before patching it.\\ You have two options to update the image: 1. Pulling the latest assisted-service image:\\ This option is faster, but it pulls the latest master image * For a local minikube k8s: shell eval $(minikube docker-env) && docker pull IMAGE_NAME * For a local k3d k8s: shell docker pull IMAGE_NAME k3d image import IMAGE_NAME * For a local registry docker pull REMOTE_IMAGE_NAME docker tag REMOTE_IMAGE_NAME LOCAL_IMAGE_NAME docker push LOCAL_IMAGE_NAME 2. Build the image locally(Recommended) shell unset DEBUG_SERVICE skipper make update-local-image Deploy the service to your local k8s: skipper make deploy-all Deploy the service for subsystem-test to your local k8s: skipper make deploy-test Build the image, push to your local k8s and restart the pods: skipper make patch-service Compile the code with debug information and patch the image: skipper make update-debug-minimal","title":"assisted-service"},{"location":"dev/debug/#assisted-service","text":"","title":"assisted-service"},{"location":"dev/debug/#debug-setup","text":"To debug the application first set the DEBUG_SERVICE environment variable to any nonempty value export DEBUG_SERVICE=true The default remote debug port is 40000 but its configurable by setting the DEBUG_SERVICE_PORT env variable: export DEBUG_SERVICE_PORT=8765 Build the image and push to your local k8s: skipper make update-local-image Note! When running this target in a DEBUG mode, \\ it updates the existing assisted-service image with the latest assisted-service code, \\ but if the Dockerfile itself has been changed you are responsible to update the image before patching it.\\ You have two options to update the image: 1. Pulling the latest assisted-service image:\\ This option is faster, but it pulls the latest master image * For a local minikube k8s: shell eval $(minikube docker-env) && docker pull IMAGE_NAME * For a local k3d k8s: shell docker pull IMAGE_NAME k3d image import IMAGE_NAME * For a local registry docker pull REMOTE_IMAGE_NAME docker tag REMOTE_IMAGE_NAME LOCAL_IMAGE_NAME docker push LOCAL_IMAGE_NAME 2. Build the image locally(Recommended) shell unset DEBUG_SERVICE skipper make update-local-image Deploy the service to your local k8s: skipper make deploy-all Deploy the service for subsystem-test to your local k8s: skipper make deploy-test Build the image, push to your local k8s and restart the pods: skipper make patch-service Compile the code with debug information and patch the image: skipper make update-debug-minimal","title":"Debug Setup"},{"location":"dev/events/","text":"Events Events generation is designed to expose a uniformed method of initializing the assisted-service events. Each event definition requires a set of properties based on its type. The definition is used for generating a function for emitting the event with the required parameters. Adding an Event In order to add a new event, follow the next steps: Add event definition to docs/events.yaml Generate the code for creating the event by: skipper make generate-events Use the generated function for emitting the event from internal/common/events/event.go Event Definition Event definition should specify the following attributes: 1. name : A unique name of the event. The name needs to remain unique and constant as it may be referred by the service's clients (e.g. by the UI). The name should match the structure <event-context>_<past_tense> . 2. message : A template of the message that will be rendered if it contains any references to the properties. E.g. the message \"Install cluster {cluster_id}\" expects the existence of a property named cluster_id . 3. event_type : Can be either cluster , host or infra_env . 1. \"cluster\" type requires the existence of cluster_id in properties. 2. \"host\" type requires the existence of host_id and infra_env_id in properties. 3. \"infra_env\" type requires the existence of infra_env_id in properties. 4. severity : Any of \"info\", \"warning\", \"error\" or \"critical\". See more info about severity levels here . 5. properties : A list of properties to be rendered into the message (if referred by) or metadata of the event (e.g. cluster_id , host_id ). Testing Having an explicit event per scenario assists in setting expectations in tests for the events. An event-matcher ( internal/events/eventstest/events_test_utils.go ) simplifies the verification of expectations for each test. E.g.: mockEvents.EXPECT().SendHostEvent(gomock.Any(), eventstest.NewEventMatcher( eventstest.WithNameMatcher(eventgen.QuickDiskFormatEventName), eventstest.WithInfraEnvIdMatcher(host.InfraEnvID.String()), eventstest.WithClusterIdMatcher(host.ClusterID.String()), eventstest.WithMessageMatcher(message), eventstest.WithHostIdMatcher(host.ID.String()))).Times(times)","title":"Events"},{"location":"dev/events/#events","text":"Events generation is designed to expose a uniformed method of initializing the assisted-service events. Each event definition requires a set of properties based on its type. The definition is used for generating a function for emitting the event with the required parameters.","title":"Events"},{"location":"dev/events/#adding-an-event","text":"In order to add a new event, follow the next steps: Add event definition to docs/events.yaml Generate the code for creating the event by: skipper make generate-events Use the generated function for emitting the event from internal/common/events/event.go","title":"Adding an Event"},{"location":"dev/events/#event-definition","text":"Event definition should specify the following attributes: 1. name : A unique name of the event. The name needs to remain unique and constant as it may be referred by the service's clients (e.g. by the UI). The name should match the structure <event-context>_<past_tense> . 2. message : A template of the message that will be rendered if it contains any references to the properties. E.g. the message \"Install cluster {cluster_id}\" expects the existence of a property named cluster_id . 3. event_type : Can be either cluster , host or infra_env . 1. \"cluster\" type requires the existence of cluster_id in properties. 2. \"host\" type requires the existence of host_id and infra_env_id in properties. 3. \"infra_env\" type requires the existence of infra_env_id in properties. 4. severity : Any of \"info\", \"warning\", \"error\" or \"critical\". See more info about severity levels here . 5. properties : A list of properties to be rendered into the message (if referred by) or metadata of the event (e.g. cluster_id , host_id ).","title":"Event Definition"},{"location":"dev/events/#testing","text":"Having an explicit event per scenario assists in setting expectations in tests for the events. An event-matcher ( internal/events/eventstest/events_test_utils.go ) simplifies the verification of expectations for each test. E.g.: mockEvents.EXPECT().SendHostEvent(gomock.Any(), eventstest.NewEventMatcher( eventstest.WithNameMatcher(eventgen.QuickDiskFormatEventName), eventstest.WithInfraEnvIdMatcher(host.InfraEnvID.String()), eventstest.WithClusterIdMatcher(host.ClusterID.String()), eventstest.WithMessageMatcher(message), eventstest.WithHostIdMatcher(host.ID.String()))).Times(times)","title":"Testing"},{"location":"dev/hardware-requirements/","text":"Hardware requirements Hardware requirements are configured with HW_VALIDATOR_REQUIREMENTS environment variable, which must contain JSON mapping OpenShift version to specific master and worker hardware requirements. For example: [{ \"version\": \"default\", \"master\": { \"cpu_cores\": 4, \"ram_mib\": 16384, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\": 100, \"packet_loss_percentage\":0 }, \"worker\": { \"cpu_cores\": 2, \"ram_mib\": 8192, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\": 1000, \"packet_loss_percentage\":10 }, \"sno\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10 } }, { \"version\": \"x.y.z\", \"master\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 150, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\":100, \"packet_loss_percentage\":0 }, \"worker\": { \"cpu_cores\": 4, \"ram_mib\": 16384, \"disk_size_gb\": 150, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\":1000, \"packet_loss_percentage\":10 }, \"sno\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10 } }] default requirements are used if version can't be found. If any overrides are needed, they have to be done in that JSON. For example: Changing disk size requirement in all versions in shell with jq : HW_VALIDATOR_REQUIREMENTS=$(echo $HW_VALIDATOR_REQUIREMENTS | jq '(.[].worker.disk_size_gb, .[].master.disk_size_gb) |= 20' | tr -d \"\\n\\t \")","title":"Hardware requirements"},{"location":"dev/hardware-requirements/#hardware-requirements","text":"Hardware requirements are configured with HW_VALIDATOR_REQUIREMENTS environment variable, which must contain JSON mapping OpenShift version to specific master and worker hardware requirements. For example: [{ \"version\": \"default\", \"master\": { \"cpu_cores\": 4, \"ram_mib\": 16384, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\": 100, \"packet_loss_percentage\":0 }, \"worker\": { \"cpu_cores\": 2, \"ram_mib\": 8192, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\": 1000, \"packet_loss_percentage\":10 }, \"sno\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10 } }, { \"version\": \"x.y.z\", \"master\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 150, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\":100, \"packet_loss_percentage\":0 }, \"worker\": { \"cpu_cores\": 4, \"ram_mib\": 16384, \"disk_size_gb\": 150, \"installation_disk_speed_threshold_ms\": 10, \"network_latency_threshold_ms\":1000, \"packet_loss_percentage\":10 }, \"sno\": { \"cpu_cores\": 8, \"ram_mib\": 32768, \"disk_size_gb\": 120, \"installation_disk_speed_threshold_ms\": 10 } }] default requirements are used if version can't be found. If any overrides are needed, they have to be done in that JSON. For example: Changing disk size requirement in all versions in shell with jq : HW_VALIDATOR_REQUIREMENTS=$(echo $HW_VALIDATOR_REQUIREMENTS | jq '(.[].worker.disk_size_gb, .[].master.disk_size_gb) |= 20' | tr -d \"\\n\\t \")","title":"Hardware requirements"},{"location":"dev/kube-api-webhook/","text":"Webhook validation Read about admission webhooks here . Webhook validation enable adding validations based on the state of the CRDs. For example, changing the Spec of AgentClusterInstall during the installation is not permitted as these changes cannot be applied to the cluster installation configuration once it started. Implementation The generic-admission-server library is used to run the webhooks as an Kubernetes aggregated API server . The admission web hook is deployed with the (operator)[../operator.md]. The following components will be deployed: - ValidatingWebhookConfiguration: The configuration defining on which CRD the webhook is called, on which operations, what is the URL path that will be called, etc... - Deployment: The configuration of the pod running the HTTPS server accepting admission requests. - Service and Service Account: Expose the HTTPS server. - APIService: Register the above service as an aggregated API server. - ClusterRole and ClusterRoleBinding: Assign needed permission to the Service. In OpenShift deployment, the certificates are handled by the OpenShift Service CA Operator by annotations on the Service and APIService. In order to check if the APIService is available, run: # kubectl get apiservice v1.admission.agentinstall.openshift.io NAME SERVICE AVAILABLE AGE v1.admission.agentinstall.openshift.io assisted-installer/agentinstalladmission True 22h Adding a new webhook In order to add a new webhook, the following steps are needed: Add a new ValidatingWebhookConfiguration yaml in the deploy dir with the required CRD resource, group, version and define the URL path. Also add it to the web hook deploy script . Create an admission hook: see example for ACI . The needed functions are: Validate(admissionSpec *admissionv1.AdmissionRequest) *admissionv1.AdmissionResponse : Handle AdmissionRequests ValidatingResource() (plural schema.GroupVersionResource, singular string) : Declare the CRD this hook wants to handle Add the new hook to the Admission Server main .","title":"Webhook validation"},{"location":"dev/kube-api-webhook/#webhook-validation","text":"Read about admission webhooks here . Webhook validation enable adding validations based on the state of the CRDs. For example, changing the Spec of AgentClusterInstall during the installation is not permitted as these changes cannot be applied to the cluster installation configuration once it started.","title":"Webhook validation"},{"location":"dev/kube-api-webhook/#implementation","text":"The generic-admission-server library is used to run the webhooks as an Kubernetes aggregated API server . The admission web hook is deployed with the (operator)[../operator.md]. The following components will be deployed: - ValidatingWebhookConfiguration: The configuration defining on which CRD the webhook is called, on which operations, what is the URL path that will be called, etc... - Deployment: The configuration of the pod running the HTTPS server accepting admission requests. - Service and Service Account: Expose the HTTPS server. - APIService: Register the above service as an aggregated API server. - ClusterRole and ClusterRoleBinding: Assign needed permission to the Service. In OpenShift deployment, the certificates are handled by the OpenShift Service CA Operator by annotations on the Service and APIService. In order to check if the APIService is available, run: # kubectl get apiservice v1.admission.agentinstall.openshift.io NAME SERVICE AVAILABLE AGE v1.admission.agentinstall.openshift.io assisted-installer/agentinstalladmission True 22h","title":"Implementation"},{"location":"dev/kube-api-webhook/#adding-a-new-webhook","text":"In order to add a new webhook, the following steps are needed: Add a new ValidatingWebhookConfiguration yaml in the deploy dir with the required CRD resource, group, version and define the URL path. Also add it to the web hook deploy script . Create an admission hook: see example for ACI . The needed functions are: Validate(admissionSpec *admissionv1.AdmissionRequest) *admissionv1.AdmissionResponse : Handle AdmissionRequests ValidatingResource() (plural schema.GroupVersionResource, singular string) : Declare the CRD this hook wants to handle Add the new hook to the Admission Server main .","title":"Adding a new webhook"},{"location":"dev/migrations/","text":"Migrations Why? There are some database changes that GORM auto migration will not handle for us . These need to be dealt with separately. How? To do this we are using a migration module called gormigrate . Migrations are run at startup right after the automigration is run. Adding a new migration Each migration should have a separate file in /internal/migrations prefixed with a timestamp which will also be the migration ID Each migration should have a single function named to describe the change which returns a *gormigrate.Migration Both migrate (up) and rollback (down) should be implemented if possible Every migration should have a corresponding test (especially for migrations which are changing data) A new migration scaffold can be created using MIGRATION_NAME=someMigrationNameHere make generate-migration This will give the migration files a proper timestamp as well as (empty) tests To be run, every migration function should be added to the list in postMigrations Pre-migrations We currently have a single migration in preMigrations , i.e. running before AutoMigrate step that is responsible for synchronizing the database schema. The use of preMigrations is (currently) discouraged, as it may cause unexpected failures due to the existing usage of application code ( common.GetClustersFromDBWhere ) in some of the already-released migrations.","title":"Migrations"},{"location":"dev/migrations/#migrations","text":"","title":"Migrations"},{"location":"dev/migrations/#why","text":"There are some database changes that GORM auto migration will not handle for us . These need to be dealt with separately.","title":"Why?"},{"location":"dev/migrations/#how","text":"To do this we are using a migration module called gormigrate . Migrations are run at startup right after the automigration is run.","title":"How?"},{"location":"dev/migrations/#adding-a-new-migration","text":"Each migration should have a separate file in /internal/migrations prefixed with a timestamp which will also be the migration ID Each migration should have a single function named to describe the change which returns a *gormigrate.Migration Both migrate (up) and rollback (down) should be implemented if possible Every migration should have a corresponding test (especially for migrations which are changing data) A new migration scaffold can be created using MIGRATION_NAME=someMigrationNameHere make generate-migration This will give the migration files a proper timestamp as well as (empty) tests To be run, every migration function should be added to the list in postMigrations","title":"Adding a new migration"},{"location":"dev/migrations/#pre-migrations","text":"We currently have a single migration in preMigrations , i.e. running before AutoMigrate step that is responsible for synchronizing the database schema. The use of preMigrations is (currently) discouraged, as it may cause unexpected failures due to the existing usage of application code ( common.GetClustersFromDBWhere ) in some of the already-released migrations.","title":"Pre-migrations"},{"location":"dev/olm-operator-plugins/","text":"OLM operator plugins development Existing plugins Local Storage Operator (LSO) OpenShift Container Storage (OCS) OpenShift Virtualization (CNV) How to implement a new OLM operator plugin To implement support for a new OLM operator plugin you need to make following changes: Introduce new validation IDs for the new operator in the swagger specification : for host validation: ```yaml host-validation-id: type: string enum: 'connected' ... 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied' ``` for cluster validation: ```yaml cluster-validation-id: type: string enum: 'machine-cidr-defined' ... 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied' ``` Regenerate code by running shell script skipper make generate-all Add the new validation IDs to proper category - \"operators\": for cluster validation : go func (v validationID) category() (string, error) { ... case IsCnvRequirementsSatisfied, IsOcsRequirementsSatisfied, IsLsoRequirementsSatisfied: return \"operators\", nil for host validaton : go func (v validationID) category() (string, error) { ... case AreLsoRequirementsSatisfied, AreOcsRequirementsSatisfied, AreCnvRequirementsSatisfied: return \"operators\", nil Modify the installation state machine by adding the new validationIDs to the list of required checks: for cluster : go var requiredForInstall = stateswitch.And(..., ..., If(IsOcsRequirementsSatisfied), If(IsLsoRequirementsSatisfied), If(IsCnvRequirementsSatisfied)) for host : go var isSufficientForInstall = stateswitch.And(..., ..., If(AreOcsRequirementsSatisfied), If(AreLsoRequirementsSatisfied), If(AreCnvRequirementsSatisfied)) Implement the Operator interface Plug the new Operator implementation in the OperatorManager constructor : go func NewManager(log logrus.FieldLogger) Manager { return NewManagerWithOperators(log, lso.NewLSOperator(), ocs.NewOcsOperator(log), cnv.NewCnvOperator(log)) } Implement tests verifying new OLM operator installation and validation, i.e. in internal/bminventory/inventory_test.go Make sure all the tests are green","title":"OLM operator plugins development"},{"location":"dev/olm-operator-plugins/#olm-operator-plugins-development","text":"","title":"OLM operator plugins development"},{"location":"dev/olm-operator-plugins/#existing-plugins","text":"Local Storage Operator (LSO) OpenShift Container Storage (OCS) OpenShift Virtualization (CNV)","title":"Existing plugins"},{"location":"dev/olm-operator-plugins/#how-to-implement-a-new-olm-operator-plugin","text":"To implement support for a new OLM operator plugin you need to make following changes: Introduce new validation IDs for the new operator in the swagger specification : for host validation: ```yaml host-validation-id: type: string enum: 'connected' ... 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied' ``` for cluster validation: ```yaml cluster-validation-id: type: string enum: 'machine-cidr-defined' ... 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied' ``` Regenerate code by running shell script skipper make generate-all Add the new validation IDs to proper category - \"operators\": for cluster validation : go func (v validationID) category() (string, error) { ... case IsCnvRequirementsSatisfied, IsOcsRequirementsSatisfied, IsLsoRequirementsSatisfied: return \"operators\", nil for host validaton : go func (v validationID) category() (string, error) { ... case AreLsoRequirementsSatisfied, AreOcsRequirementsSatisfied, AreCnvRequirementsSatisfied: return \"operators\", nil Modify the installation state machine by adding the new validationIDs to the list of required checks: for cluster : go var requiredForInstall = stateswitch.And(..., ..., If(IsOcsRequirementsSatisfied), If(IsLsoRequirementsSatisfied), If(IsCnvRequirementsSatisfied)) for host : go var isSufficientForInstall = stateswitch.And(..., ..., If(AreOcsRequirementsSatisfied), If(AreLsoRequirementsSatisfied), If(AreCnvRequirementsSatisfied)) Implement the Operator interface Plug the new Operator implementation in the OperatorManager constructor : go func NewManager(log logrus.FieldLogger) Manager { return NewManagerWithOperators(log, lso.NewLSOperator(), ocs.NewOcsOperator(log), cnv.NewCnvOperator(log)) } Implement tests verifying new OLM operator installation and validation, i.e. in internal/bminventory/inventory_test.go Make sure all the tests are green","title":"How to implement a new OLM operator plugin"},{"location":"dev/running-test/","text":"How to run Assisted-service subsystem tests Assisted-service subsystem tests require deploying the assisted-service on a k8s cluster together with DB and storage services. The subsystem tests are located on the subsystem directory. Subsystem tests pre-configuration Run minikube on your system Enable registry addon on your minikube: minikube start minikube addons enable registry Set LOCAL_ASSISTED_ORG to point to your local registry address export LOCAL_ASSISTED_ORG=localhost:5000 Redirect the local registry to the minikube registry: nohup kubectl port-forward svc/registry 5000:80 -n kube-system &>/dev/null & Make a tunnel to make minikube services reachable (the command will ask for root password): nohup minikube tunnel &>/dev/null & Deploy services: skipper make deploy-test Run tests skipper make [test|unit-test] [environment variables] test - Runs subsystem tests. unit-test - Runs unit tests. Environment variables: FOCUS=\"install_cluster\" - An optional flag used for focused specs with regular expression. SKIP=\"install_cluster\" - An optional flag to skip scopes with regular expressions. TEST=\"./internal/host\" - An optional flag used for testing a specific package. VERBOSE=true - An optional flag to print verbosed data. Update service for the subsystem tests if you are making changes and don't want to deploy everything once again you can simply run this command: skipper make patch-service It will build and push a new image of the service to your Docker registry, then delete the service pod from minikube, the deployment will handle the update and pull the new image to start the service again.","title":"How to run Assisted-service subsystem tests"},{"location":"dev/running-test/#how-to-run-assisted-service-subsystem-tests","text":"Assisted-service subsystem tests require deploying the assisted-service on a k8s cluster together with DB and storage services. The subsystem tests are located on the subsystem directory.","title":"How to run Assisted-service subsystem tests"},{"location":"dev/running-test/#subsystem-tests-pre-configuration","text":"Run minikube on your system Enable registry addon on your minikube: minikube start minikube addons enable registry Set LOCAL_ASSISTED_ORG to point to your local registry address export LOCAL_ASSISTED_ORG=localhost:5000 Redirect the local registry to the minikube registry: nohup kubectl port-forward svc/registry 5000:80 -n kube-system &>/dev/null & Make a tunnel to make minikube services reachable (the command will ask for root password): nohup minikube tunnel &>/dev/null & Deploy services: skipper make deploy-test","title":"Subsystem tests pre-configuration"},{"location":"dev/running-test/#run-tests","text":"skipper make [test|unit-test] [environment variables] test - Runs subsystem tests. unit-test - Runs unit tests. Environment variables: FOCUS=\"install_cluster\" - An optional flag used for focused specs with regular expression. SKIP=\"install_cluster\" - An optional flag to skip scopes with regular expressions. TEST=\"./internal/host\" - An optional flag used for testing a specific package. VERBOSE=true - An optional flag to print verbosed data.","title":"Run tests"},{"location":"dev/running-test/#update-service-for-the-subsystem-tests","text":"if you are making changes and don't want to deploy everything once again you can simply run this command: skipper make patch-service It will build and push a new image of the service to your Docker registry, then delete the service pod from minikube, the deployment will handle the update and pull the new image to start the service again.","title":"Update service for the subsystem tests"},{"location":"dev/testing/","text":"Assisted Installer Testing The assisted-installer tests are divided into 3 categories: Unit tests - Focused on a module/function level while other modules are mocked. Unit tests are located in the package, where the code they are testing resides, using the pattern <module_name>_test.go . Subsystem tests - Focused on the component while mocking other component. For example, assisted-service subsystem tests mock the agent responses. System tests (a.k.a e2e) - Running full flows with all components. The e2e tests are divided into u/s (upstream) basic workflows on assisted-test-infra and d/s (downstream) extended regression tests maintained by both DEV and QE teams on kni-assisted-installer-auto . Repository CI Our CI jobs are currently managed and ran by two CI tools - a Jenkins hosted on http://assisted-jenkins.usersys.redhat.com and a Prow hosts on https://prow.ci.openshift.org . Jenkins Prow Local for Assisted ecosystem Company-wide Checks comments for JIRA Runs e2e Manages images in quay.io/ocpmetal Runs all testing checks (lint, unit, etc) Assisted-service CI jobs are defined under openshift/release repository on openshift-assisted-service-master.yaml . Read more about OpenShift CI infrastructure on OpenShift CI Docs . All the currently available jobs for the openshift/assisted-service repository can be viewed on Openshift CI Step Registry . Adding a new CI job When adding a new job the following rules of thumbs should be taken into account: Test logic needs to be maintained in the repository under test and not under openshift/release. It would allow easier integration with other tools, less dependency of the CI infrastructure, and most importantly the availability to run it locally. When introducing a new job it should be both a presubmit job and a periodic job. A presubmit job needs to be available so contributors would be able to run it on their PRs before merging. The presubmit job needs to be configured as always_run: false and optional: true (not blocking a merge) until proving stability. New OCP releases might break one of Assisted workflows since Assisted isn't part of OCP. The periodic job needs to run on a frequent basis (e.g. daily) and have a reporter_config configured, in order to be notified on Slack whenever there's a breakage. In case the new job affects multiple repositories - every repository should have the same presubmit job so it could be tested for every component change. For example, you can see that the e2e-metal-assisted-olm job is defined on several different repositories in this link . An example of a PR adding a new job FAQ How can I debug CI failures? A CI job can be debugged only in runtime. Once a job terminates it can no longer be debugged because the cluster / machines used to run the job get torn down at the end of it. However, each job produces artifacts such as (logs, SOS reports, must-gather logs) which can be used to try to analyze what went wrong in retrospect. Those artifacts can be accessed by going to the job artifacts. You can follow the OpenShift CI doc \"Interact With Running Jobs\" guide or try to run the experimental Debug Prow Jobs live gist in order to connect to the OCP cluster running your prow tests. Contributions are welcome. :) If a CI job fails, where should I look for assisted-related failures? When a PR job fails there's a \"details\" button next to the GitHub context. It will show the Spyglass view. In there, you should look if there are other builds that failed recently for the same job using the \"Job History\" button. When is it ok to retest? Whenever there's a failure - first, you should look for its root cause before hitting the \"/retest\" command. It should only be used when there's a known flaky issue. Using the retest feature for no reason just wastes the project CI resources and money. How does the retest bot works? When a PR is ready to be merged (approved and not held) all the jobs will be retested for every new master that's being updated. In case any of the job fails - the openshift-bot will try to retest it automatically. The retest job is defined under infra-periodics.yaml Where do these jobs run? Depends on the job. Single-stage tests (e.g. lint, unit tests) run inside of a scheduled container. Read more Jobs that require a cluster (e.g. subsystem) run on a claimed OCP cluster from an hibernated pool of clusters. Read more Baremetal jobs (i.e. e2e) run on a provisioned baremetal machine by Equnix . How to run Assisted-service subsystem tests More information is available here: Assisted Installer Testing .","title":"Assisted Installer Testing"},{"location":"dev/testing/#assisted-installer-testing","text":"The assisted-installer tests are divided into 3 categories: Unit tests - Focused on a module/function level while other modules are mocked. Unit tests are located in the package, where the code they are testing resides, using the pattern <module_name>_test.go . Subsystem tests - Focused on the component while mocking other component. For example, assisted-service subsystem tests mock the agent responses. System tests (a.k.a e2e) - Running full flows with all components. The e2e tests are divided into u/s (upstream) basic workflows on assisted-test-infra and d/s (downstream) extended regression tests maintained by both DEV and QE teams on kni-assisted-installer-auto .","title":"Assisted Installer Testing"},{"location":"dev/testing/#repository-ci","text":"Our CI jobs are currently managed and ran by two CI tools - a Jenkins hosted on http://assisted-jenkins.usersys.redhat.com and a Prow hosts on https://prow.ci.openshift.org . Jenkins Prow Local for Assisted ecosystem Company-wide Checks comments for JIRA Runs e2e Manages images in quay.io/ocpmetal Runs all testing checks (lint, unit, etc) Assisted-service CI jobs are defined under openshift/release repository on openshift-assisted-service-master.yaml . Read more about OpenShift CI infrastructure on OpenShift CI Docs . All the currently available jobs for the openshift/assisted-service repository can be viewed on Openshift CI Step Registry .","title":"Repository CI"},{"location":"dev/testing/#adding-a-new-ci-job","text":"When adding a new job the following rules of thumbs should be taken into account: Test logic needs to be maintained in the repository under test and not under openshift/release. It would allow easier integration with other tools, less dependency of the CI infrastructure, and most importantly the availability to run it locally. When introducing a new job it should be both a presubmit job and a periodic job. A presubmit job needs to be available so contributors would be able to run it on their PRs before merging. The presubmit job needs to be configured as always_run: false and optional: true (not blocking a merge) until proving stability. New OCP releases might break one of Assisted workflows since Assisted isn't part of OCP. The periodic job needs to run on a frequent basis (e.g. daily) and have a reporter_config configured, in order to be notified on Slack whenever there's a breakage. In case the new job affects multiple repositories - every repository should have the same presubmit job so it could be tested for every component change. For example, you can see that the e2e-metal-assisted-olm job is defined on several different repositories in this link . An example of a PR adding a new job","title":"Adding a new CI job"},{"location":"dev/testing/#faq","text":"","title":"FAQ"},{"location":"dev/testing/#how-can-i-debug-ci-failures","text":"A CI job can be debugged only in runtime. Once a job terminates it can no longer be debugged because the cluster / machines used to run the job get torn down at the end of it. However, each job produces artifacts such as (logs, SOS reports, must-gather logs) which can be used to try to analyze what went wrong in retrospect. Those artifacts can be accessed by going to the job artifacts. You can follow the OpenShift CI doc \"Interact With Running Jobs\" guide or try to run the experimental Debug Prow Jobs live gist in order to connect to the OCP cluster running your prow tests. Contributions are welcome. :)","title":"How can I debug CI failures?"},{"location":"dev/testing/#if-a-ci-job-fails-where-should-i-look-for-assisted-related-failures","text":"When a PR job fails there's a \"details\" button next to the GitHub context. It will show the Spyglass view. In there, you should look if there are other builds that failed recently for the same job using the \"Job History\" button.","title":"If a CI job fails, where should I look for assisted-related failures?"},{"location":"dev/testing/#when-is-it-ok-to-retest","text":"Whenever there's a failure - first, you should look for its root cause before hitting the \"/retest\" command. It should only be used when there's a known flaky issue. Using the retest feature for no reason just wastes the project CI resources and money.","title":"When is it ok to retest?"},{"location":"dev/testing/#how-does-the-retest-bot-works","text":"When a PR is ready to be merged (approved and not held) all the jobs will be retested for every new master that's being updated. In case any of the job fails - the openshift-bot will try to retest it automatically. The retest job is defined under infra-periodics.yaml","title":"How does the retest bot works?"},{"location":"dev/testing/#where-do-these-jobs-run","text":"Depends on the job. Single-stage tests (e.g. lint, unit tests) run inside of a scheduled container. Read more Jobs that require a cluster (e.g. subsystem) run on a claimed OCP cluster from an hibernated pool of clusters. Read more Baremetal jobs (i.e. e2e) run on a provisioned baremetal machine by Equnix .","title":"Where do these jobs run?"},{"location":"dev/testing/#how-to-run-assisted-service-subsystem-tests","text":"More information is available here: Assisted Installer Testing .","title":"How to run Assisted-service subsystem tests"},{"location":"enhancements/","text":"Enhancements Tracking for Assisted Installer Inspired by the Kubernetes enhancement process. This directory provides a rally point to discuss, debate, and reach consensus for how Assisted Installer (AI) enhancements are introduced. Given that the AI is composed of multiple projects: assisted-service, assisted-installer, and assisted-installer-agent to name a few, it is useful to have a centralized place to describe AI enhancements via an actionable design proposal. Should I Create an Enhancement? A rough heuristic for an enhancement is anything that: impacts multiple Assisted Installer projects requires significant effort to complete requires consensus across multiple domains of Assisted Installer impacts the UX or operation of Assisted Installer substantially users of Assisted Installer will notice and come to rely on A rough heuristic for when an enhancement should be made in openshift/enhancements instead: requires changes to OpenShift requires changes and/or consensus with other components related to OpenShift substanitally impacts the requirements for and/or experience of installing and provisioning OpenShift would benefit from approval by OpenShift architects It is unlikely to require an enhancement if it: is covered by an existing OpenShift enhancement proposal fixes a bug adds more testing internally refactors a code or component only visible to that components domain minimal impact to Assisted Installer as a whole Getting Started Follow the process outlined in the enhancement template","title":"Enhancements Tracking for Assisted Installer"},{"location":"enhancements/#enhancements-tracking-for-assisted-installer","text":"Inspired by the Kubernetes enhancement process. This directory provides a rally point to discuss, debate, and reach consensus for how Assisted Installer (AI) enhancements are introduced. Given that the AI is composed of multiple projects: assisted-service, assisted-installer, and assisted-installer-agent to name a few, it is useful to have a centralized place to describe AI enhancements via an actionable design proposal.","title":"Enhancements Tracking for Assisted Installer"},{"location":"enhancements/#should-i-create-an-enhancement","text":"A rough heuristic for an enhancement is anything that: impacts multiple Assisted Installer projects requires significant effort to complete requires consensus across multiple domains of Assisted Installer impacts the UX or operation of Assisted Installer substantially users of Assisted Installer will notice and come to rely on A rough heuristic for when an enhancement should be made in openshift/enhancements instead: requires changes to OpenShift requires changes and/or consensus with other components related to OpenShift substanitally impacts the requirements for and/or experience of installing and provisioning OpenShift would benefit from approval by OpenShift architects It is unlikely to require an enhancement if it: is covered by an existing OpenShift enhancement proposal fixes a bug adds more testing internally refactors a code or component only visible to that components domain minimal impact to Assisted Installer as a whole","title":"Should I Create an Enhancement?"},{"location":"enhancements/#getting-started","text":"Follow the process outlined in the enhancement template","title":"Getting Started"},{"location":"enhancements/add-external-providers/","text":"Define Providers Interface Summary Currently, the assisted installer enables you to install on any platform where you can boot the discovery ISO resulting in a cluster with platform set to none, and also supports vSphere and Bare-Metal platforms. Openshift can also be installed on other on-prem providers such as: - Red Hat Virtualization (oVirt). - OpenStack. Currently, the assisted installer supports only the Bare-Metal and vSphere platforms, but for future on-prem providers we need to make sure there is a clear interface each on prem provider can implement to add its platform. This enhancement will define a clear interface to extend the assisted installer to add on prem provider integrations. Motivation Assisted Installer can be a great tool to perform User Provisioned installation (UPI) installation on external providers, providing the user with clear UI to see its provisioned machines joining the cluster, and lowering installation resources requirements. It is only natural that on-prem providers will want to follow vSphere path and extend assisted installer to support their platform, to make sure that the provider specific code is easy to maintain, doesn't burden the assisted installer team, and the providers have a clear understanding of what they need to implement to add support to their platform we need to define a clear interface which each provider can implement and maintain. Goals Define a clear way to add a specific platform provider to the assisted installer. Migrate all existing supported providers to implement the same unified interface - and test it with assisted-test-infra. Non-Goals Support IPI installations. Integrating any relevant Openshift installer types into the assisted installer code - this will require a large refactor of the assisted installer code. Remove \"Baremetal\" or \"BM\" words from inappropriate struct/function names. (as this is how the assisted installer was originally implemented) Adding Support for new providers. Proposal Create a Provider interface, and hooks in the assisted installer code which separates the assisted installer core code from the provider-specific code. Add clear documentation for each provider specific function. Document steps to add a provider to the assisted installer. User Stories Story 1 As an Openshift developer working on provider X I want to add support for installing Openshift with assisted installer on top of platform X. Story 2 As an Openshift developer working on the assisted installer team I want to let providers hook into the assisted installer flow natural without having to worry about provider specific implementation. Implementation Details/Notes/Constraints Changes to the assisted-service project: Step 1: Detect platform specific logic Go over the assisted installer flow and detect parts of the code which require provider specific logic. This step is based mostly on the work that already been done on the vSphere provider addition (MGMT-7067). Step 2: Create a Go interface We should create a Go interface that will contain each provider specific logic in a function for each provider to implement. Prototype: // Provider contains functions which are required to support installing on a specific platform. type Provider interface { // Name returns the name of the platform. Name() models.PlatformType // AddPlatformToInstallConfig adds the provider platform to the installconfig platform field, // sets platform fields from values within the cluster model. AddPlatformToInstallConfig(cfg *installcfg.InstallerConfigBaremetal, cluster *common.Cluster) error // SetPlatformValuesInDBUpdates updates the `updates` data structure with platform specific values SetPlatformValuesInDBUpdates(platformParams *models.Platform, updates map[string]interface{}) error // CleanPlatformValuesFromDBUpdates remove platform specific values from the `updates` data structure CleanPlatformValuesFromDBUpdates(updates map[string]interface{}) error // SetPlatformUsages uses the usageApi to update platform specific usages SetPlatformUsages(platformParams *models.Platform, usages map[string]models.Usage, usageApi usage.API) error // IsHostSupported checks if the provider supports the host IsHostSupported(hosts *models.Host) (bool, error) // AreHostsSupported checks if the provider supports the hosts AreHostsSupported(host []*models.Host) (bool, error) } Step 3: Implement the provider registry Implement the registry pattern so providers can register with their names: type Registry interface { // Register registers a provider. Register(provider provider.Provider) // Get returns a provider registered to a name. // if provider is not registered returns an ErrNoSuchProvider Get(name string) (provider.Provider, error) } Implement the provider registry that will act as an API to all the implemented providers, it will contain alll the metods: type ProviderRegistry interface { Registry // GetSupportedProvidersByHosts returns a slice of all the providers names which support // installation with the given hosts GetSupportedProvidersByHosts(hosts []*models.Host) ([]models.PlatformType, error) // AddPlatformToInstallConfig adds the provider platform to the installconfig platform field, // sets platform fields from values within the cluster model. AddPlatformToInstallConfig(p models.PlatformType, cfg *installcfg.InstallerConfigBaremetal, cluster *common.Cluster) error // SetPlatformValuesInDBUpdates updates the `updates` data structure with platform specific values SetPlatformValuesInDBUpdates(p models.PlatformType, platformParams *models.Platform, updates map[string]interface{}) error // SetPlatformUsages uses the usageApi to update platform specific usages SetPlatformUsages(p models.PlatformType, platformParams *models.Platform, usages map[string]models.Usage, usageApi usage.API) error // IsHostSupported checks if the provider supports the host IsHostSupported(p models.PlatformType, host *models.Host) (bool, error) // AreHostsSupported checks if the provider supports the hosts AreHostsSupported(p models.PlatformType, hosts []*models.Host) (bool, error) } Step 4: Add the provider registry to relevant structures and call it We should add the provider registry to structs that needs to call it on their initialization and call the relevant function(create a hook) at the appropriate places. Step 5: Migrate existing provider code to new Provider interface Make sure all the existing provider specific(baremetal/vsphere) code is ported to the new interface. Step 6: Adjust existing/add unit tests Make sure all the existing test cases work and add new tests if required. Changes to the assisted-agent project: After the agent is running on the host, the provider should be determined from the OS details of the node. The GetVendor method calculates if a node is virtual or not and populates the SystemVendor model which is later being used by the assisted service to determine if a node is running on a certain provider. We need to make sure that the provider is listed in the [isVirtual](https://github.com/openshift/assisted-installer-agent/blob/master/src/inventory/system_vendor.go#L15-L19] list and that it can be detected using the fields in SystemVendor . See PR as an example. Changes to the Openshift installer project: A small change is required in the installer project. Since the openshift installer is generating a random InfraID the names of the Nodes(in the manifest) are different from the nodes the user provisions, and since we have a provider set then the provider will try to start new machines due to the naming mismatch. Since the InfraID is random we can't tell the user to create the Nodes with specific names. Also, the InfraID is later used by the platforms in various ways to identify the cluster, for example in oVirt we use it to set an oVirt tag which helps us group the cluster resources in oVirt, and we rely on it in various places such as cluster destroy or CSI/Machine provider logic to filter cluster resources quickly and prevent unnecessary API calls. We need to add an env var to allow us to override the generated InfraID. We started a PR on the installer project . Risks and Mitigations As with every large code change, this change contains the risk of breaking existing functionality. This can be mitigated by sanity testing and making sure each provider implements unit tests and e2e test suite. Design Details Closed Questions Throughout the assisted installer code we have structs and functions that are specific to baremetal(at least from the name), Are they safe to be extended to support providers? Yes, of course they need to be extented without breaking anything. Is there anything baremetal specific we should be concerned about? No Is the assisted installer team aims to modify the names to be generic? for example I don't see why the InstallerConfig struct is BareMetal. No plans but we can modify them if it make sense as we develop. Some examples but it is on every file: - BMACReconciler - bareMetalInventory - InstallerConfigBaremetal - bmhIsMaster Is there a use case in which the user will provision the Nodes on a certain provider but want to disable provider integration? meaning nodes will be provider will be discovered but we want platform none anyway? in which case what would be the best way to handle this in terms of user interaction? would an env var flag to disable provider integration is enough? We have to support that use case, currently the selection is done in the UI, this will be answered per provider implementation and it is out of scope for this enhancement UI Impact Currently, no UI changes are required. Each provider added should consider UI changes like vSphere(MGMT-7102) but this is out of scope for this enhancement Test Plan Each provider will be in charge of implementing it's own e2e test suite. Each provider QE team will be in charge of the testing. Drawbacks Alternatives","title":"Define-Providers-Interface"},{"location":"enhancements/add-external-providers/#define-providers-interface","text":"","title":"Define Providers Interface"},{"location":"enhancements/add-external-providers/#summary","text":"Currently, the assisted installer enables you to install on any platform where you can boot the discovery ISO resulting in a cluster with platform set to none, and also supports vSphere and Bare-Metal platforms. Openshift can also be installed on other on-prem providers such as: - Red Hat Virtualization (oVirt). - OpenStack. Currently, the assisted installer supports only the Bare-Metal and vSphere platforms, but for future on-prem providers we need to make sure there is a clear interface each on prem provider can implement to add its platform. This enhancement will define a clear interface to extend the assisted installer to add on prem provider integrations.","title":"Summary"},{"location":"enhancements/add-external-providers/#motivation","text":"Assisted Installer can be a great tool to perform User Provisioned installation (UPI) installation on external providers, providing the user with clear UI to see its provisioned machines joining the cluster, and lowering installation resources requirements. It is only natural that on-prem providers will want to follow vSphere path and extend assisted installer to support their platform, to make sure that the provider specific code is easy to maintain, doesn't burden the assisted installer team, and the providers have a clear understanding of what they need to implement to add support to their platform we need to define a clear interface which each provider can implement and maintain.","title":"Motivation"},{"location":"enhancements/add-external-providers/#goals","text":"Define a clear way to add a specific platform provider to the assisted installer. Migrate all existing supported providers to implement the same unified interface - and test it with assisted-test-infra.","title":"Goals"},{"location":"enhancements/add-external-providers/#non-goals","text":"Support IPI installations. Integrating any relevant Openshift installer types into the assisted installer code - this will require a large refactor of the assisted installer code. Remove \"Baremetal\" or \"BM\" words from inappropriate struct/function names. (as this is how the assisted installer was originally implemented) Adding Support for new providers.","title":"Non-Goals"},{"location":"enhancements/add-external-providers/#proposal","text":"Create a Provider interface, and hooks in the assisted installer code which separates the assisted installer core code from the provider-specific code. Add clear documentation for each provider specific function. Document steps to add a provider to the assisted installer.","title":"Proposal"},{"location":"enhancements/add-external-providers/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/add-external-providers/#story-1","text":"As an Openshift developer working on provider X I want to add support for installing Openshift with assisted installer on top of platform X.","title":"Story 1"},{"location":"enhancements/add-external-providers/#story-2","text":"As an Openshift developer working on the assisted installer team I want to let providers hook into the assisted installer flow natural without having to worry about provider specific implementation.","title":"Story 2"},{"location":"enhancements/add-external-providers/#implementation-detailsnotesconstraints","text":"","title":"Implementation Details/Notes/Constraints"},{"location":"enhancements/add-external-providers/#changes-to-the-assisted-service-project","text":"","title":"Changes to the assisted-service project:"},{"location":"enhancements/add-external-providers/#step-1-detect-platform-specific-logic","text":"Go over the assisted installer flow and detect parts of the code which require provider specific logic. This step is based mostly on the work that already been done on the vSphere provider addition (MGMT-7067).","title":"Step 1: Detect platform specific logic"},{"location":"enhancements/add-external-providers/#step-2-create-a-go-interface","text":"We should create a Go interface that will contain each provider specific logic in a function for each provider to implement. Prototype: // Provider contains functions which are required to support installing on a specific platform. type Provider interface { // Name returns the name of the platform. Name() models.PlatformType // AddPlatformToInstallConfig adds the provider platform to the installconfig platform field, // sets platform fields from values within the cluster model. AddPlatformToInstallConfig(cfg *installcfg.InstallerConfigBaremetal, cluster *common.Cluster) error // SetPlatformValuesInDBUpdates updates the `updates` data structure with platform specific values SetPlatformValuesInDBUpdates(platformParams *models.Platform, updates map[string]interface{}) error // CleanPlatformValuesFromDBUpdates remove platform specific values from the `updates` data structure CleanPlatformValuesFromDBUpdates(updates map[string]interface{}) error // SetPlatformUsages uses the usageApi to update platform specific usages SetPlatformUsages(platformParams *models.Platform, usages map[string]models.Usage, usageApi usage.API) error // IsHostSupported checks if the provider supports the host IsHostSupported(hosts *models.Host) (bool, error) // AreHostsSupported checks if the provider supports the hosts AreHostsSupported(host []*models.Host) (bool, error) }","title":"Step 2: Create a Go interface"},{"location":"enhancements/add-external-providers/#step-3-implement-the-provider-registry","text":"Implement the registry pattern so providers can register with their names: type Registry interface { // Register registers a provider. Register(provider provider.Provider) // Get returns a provider registered to a name. // if provider is not registered returns an ErrNoSuchProvider Get(name string) (provider.Provider, error) } Implement the provider registry that will act as an API to all the implemented providers, it will contain alll the metods: type ProviderRegistry interface { Registry // GetSupportedProvidersByHosts returns a slice of all the providers names which support // installation with the given hosts GetSupportedProvidersByHosts(hosts []*models.Host) ([]models.PlatformType, error) // AddPlatformToInstallConfig adds the provider platform to the installconfig platform field, // sets platform fields from values within the cluster model. AddPlatformToInstallConfig(p models.PlatformType, cfg *installcfg.InstallerConfigBaremetal, cluster *common.Cluster) error // SetPlatformValuesInDBUpdates updates the `updates` data structure with platform specific values SetPlatformValuesInDBUpdates(p models.PlatformType, platformParams *models.Platform, updates map[string]interface{}) error // SetPlatformUsages uses the usageApi to update platform specific usages SetPlatformUsages(p models.PlatformType, platformParams *models.Platform, usages map[string]models.Usage, usageApi usage.API) error // IsHostSupported checks if the provider supports the host IsHostSupported(p models.PlatformType, host *models.Host) (bool, error) // AreHostsSupported checks if the provider supports the hosts AreHostsSupported(p models.PlatformType, hosts []*models.Host) (bool, error) }","title":"Step 3: Implement the provider registry"},{"location":"enhancements/add-external-providers/#step-4-add-the-provider-registry-to-relevant-structures-and-call-it","text":"We should add the provider registry to structs that needs to call it on their initialization and call the relevant function(create a hook) at the appropriate places.","title":"Step 4: Add the provider registry to relevant structures and call it"},{"location":"enhancements/add-external-providers/#step-5-migrate-existing-provider-code-to-new-provider-interface","text":"Make sure all the existing provider specific(baremetal/vsphere) code is ported to the new interface.","title":"Step 5: Migrate existing provider code to new Provider interface"},{"location":"enhancements/add-external-providers/#step-6-adjust-existingadd-unit-tests","text":"Make sure all the existing test cases work and add new tests if required.","title":"Step 6: Adjust existing/add unit tests"},{"location":"enhancements/add-external-providers/#changes-to-the-assisted-agent-project","text":"After the agent is running on the host, the provider should be determined from the OS details of the node. The GetVendor method calculates if a node is virtual or not and populates the SystemVendor model which is later being used by the assisted service to determine if a node is running on a certain provider. We need to make sure that the provider is listed in the [isVirtual](https://github.com/openshift/assisted-installer-agent/blob/master/src/inventory/system_vendor.go#L15-L19] list and that it can be detected using the fields in SystemVendor . See PR as an example.","title":"Changes to the assisted-agent project:"},{"location":"enhancements/add-external-providers/#changes-to-the-openshift-installer-project","text":"A small change is required in the installer project. Since the openshift installer is generating a random InfraID the names of the Nodes(in the manifest) are different from the nodes the user provisions, and since we have a provider set then the provider will try to start new machines due to the naming mismatch. Since the InfraID is random we can't tell the user to create the Nodes with specific names. Also, the InfraID is later used by the platforms in various ways to identify the cluster, for example in oVirt we use it to set an oVirt tag which helps us group the cluster resources in oVirt, and we rely on it in various places such as cluster destroy or CSI/Machine provider logic to filter cluster resources quickly and prevent unnecessary API calls. We need to add an env var to allow us to override the generated InfraID. We started a PR on the installer project .","title":"Changes to the Openshift installer project:"},{"location":"enhancements/add-external-providers/#risks-and-mitigations","text":"As with every large code change, this change contains the risk of breaking existing functionality. This can be mitigated by sanity testing and making sure each provider implements unit tests and e2e test suite.","title":"Risks and Mitigations"},{"location":"enhancements/add-external-providers/#design-details","text":"","title":"Design Details"},{"location":"enhancements/add-external-providers/#closed-questions","text":"Throughout the assisted installer code we have structs and functions that are specific to baremetal(at least from the name), Are they safe to be extended to support providers? Yes, of course they need to be extented without breaking anything. Is there anything baremetal specific we should be concerned about? No Is the assisted installer team aims to modify the names to be generic? for example I don't see why the InstallerConfig struct is BareMetal. No plans but we can modify them if it make sense as we develop. Some examples but it is on every file: - BMACReconciler - bareMetalInventory - InstallerConfigBaremetal - bmhIsMaster Is there a use case in which the user will provision the Nodes on a certain provider but want to disable provider integration? meaning nodes will be provider will be discovered but we want platform none anyway? in which case what would be the best way to handle this in terms of user interaction? would an env var flag to disable provider integration is enough? We have to support that use case, currently the selection is done in the UI, this will be answered per provider implementation and it is out of scope for this enhancement","title":"Closed Questions"},{"location":"enhancements/add-external-providers/#ui-impact","text":"Currently, no UI changes are required. Each provider added should consider UI changes like vSphere(MGMT-7102) but this is out of scope for this enhancement","title":"UI Impact"},{"location":"enhancements/add-external-providers/#test-plan","text":"Each provider will be in charge of implementing it's own e2e test suite. Each provider QE team will be in charge of the testing.","title":"Test Plan"},{"location":"enhancements/add-external-providers/#drawbacks","text":"","title":"Drawbacks"},{"location":"enhancements/add-external-providers/#alternatives","text":"","title":"Alternatives"},{"location":"enhancements/agent-late-binding/","text":"Agent Late Binding Summary In the current implementation of Assisted Installer, each Discovery ISO is associated with an existing cluster resource (a cluster definition that may not yet be an installed OCP cluster). When an agent comes up, it registers with the Assisted Service and is automatically bound to that same cluster resource. The only way to bind that host to a different cluster is to boot it with an ISO associated with the desired cluster. We can see that the process of configuring the infrastructure is closely tied to cluster creation. The late binding feature splits the process into two parts, each potentially performed by a different persona. The Discovery ISO is created by an infrastructure admin, who then boots the hosts. Each host becomes visible to the cluster creators but not bound to any specific cluster resource. At a later time, a cluster creator can create or expand a cluster by binding available hosts to it. Motivation The benefits of late binding are as follows: Adjusts the flow to suit the two different personas (infrastructure administrator and cloud creator). Allows for a more cloud-like use case where compute resources of different flavors can be consumed, similar to IBM Satellite. Allows agent-based installation to fit more naturally into a MAPI or CAPI model. While the details of such an integration are not in the scope of this enhancement, the idea is that hosts are booted, thus creating a collection of unassigned agents, and the machine creation involves choosing one such agent and beginning its installation. Reduces the number of Discovery ISOs, as there is no need to generate and download one for each cluster installation. Each ISO incurs capacity overhead for the service, and management overhead for the user which conflicts with our ease-of-use goal. Allows the user to see their inventory and then make decisions on how to build clusters based on hardware and connectivity. At a later stage, the service can make recommendations. This would be a layer of logic above late binding and is out of scope for this enhancement. Enables the user story where the Discovery ISO is loaded at the factory, and then discovered at a later time. Goals Note: The language here refers to CRDs, but also holds true for the corresponding REST resources Make the InfraEnv and Agent resources independent from the ClusterDeployment. A single InfraEnv should have the potential to boot any number of hosts, whose Agents register with no relation to any particular ClusterDeployment. An Agent may be assigned to a ClusterDeployment at a later time. An Agent may be associated with a different ClusterDeployment at any time, unless it is part of an ongoing cluster installation. Non-Goals Note: The language here refers to CRDs, but also holds true for the corresponding REST resources Once a host boots from disk, booting the Discovery ISO again to return it to the collection of free Agents is considered to be a manual step. There are several options that we will explore whereby this can be done automatically, but that discussion is out of scope for this enhancement. At a later stage, Agents may be automatically assigned to clusters based on their discovered inventory and/or network configurations. This is future work and not in the scope of this enhancement. For SaaS, there are discussions about changing the tenancy model to include organizations and not only users. Those decisions are out of scope for this document. Proposal There are two main (non-breaking) changes to the CRDs: 1. ClusterRef in the InfraEnv Spec will no longer be mandatory. 1. ClusterRef in the Agent Spec will no longer be mandatory. A Discovery ISO with no associated cluster means that hosts booted with that ISO will have agents that are not bound to any cluster. Thus, Agent CRs can be created with no cluster binding. It will still be possible to retain the previous behavior by providing a ClusterRef. The tenancy model will remain unchanged, with Agents being created in the namespace of the InfraEnv they booted from. The Agent's ClusterRef may now be deleted or changed dynamically, unless the Agent is part of an ongoing cluster installation, in which case the installation must first be cancelled (not currently possible via the k8s API). If the host had started/completed its installation, a new Agent Condition would indicate that the user needs to boot the host into the Discovery ISO once again (as mentioned in the non-goals, this can be done automatically but is not in the scope of this enhancement). The REST API will require breaking changes, so we will create a v2 API. At the same time we will better align with the k8s API (change image to infra-env, but don't change host to agent because it will require too many changes). 1. Move images (infra-envs) to be a standalone resource, rather than part of the cluster resource ( /clusters/{cluster_id}/downloads/image to /infra-envs ). This removes the dependency of an image on a specific cluster, removes the current limitation of one image per cluster, and aligns with InfraEnv CRD. 1. An agent is now always referenced using a combination of the infra-env ID and host ID, and the REST API path will be /infra-envs/{infra_env_id}/hosts/{host_id} rather than /clusters/{cluster_id}/hosts/{host_id} . This means that a server that is booted from the ISO of infra-env1 and then from the ISO of infra-env2 will generate two Agents (the first's state should be disconnected ). The Agent will have an optional cluster-id property (foreign key in the DB) that signifies which cluster the Agent is bound to, if any. 1. The InfraEnv resource will have an additional optional cluster-id property that will be used for non-late-binding flows. When an Agent registers itself, the assisted-service should check if the Agent's InfraEnv has the cluster-id property set, and if so, set that property on the Agent accordingly. For the SaaS, the service will record ownership (user and org ID) for InfraEnvs, and Agents will inherit the same ownership. Agent validations that do not depend on being part of a cluster will run for Agents that are both associated with clusters and those that are not. Examples of these include minimum hardware for any role, valid hostname, and registry access. Validations that do depend on being part of a cluster, such as connectivity checks, will run only for Agents associated with clusters. Events are currently always scoped to a cluster. We will now need two types of events: 1. The events that we have today, which should be visible to the Cluster Creator. 1. The events for an Agent which is not associated with a cluster, for example a history of when it registered and which clusters it was associated with (should be visible to the infrastructure admin). These events should be scoped to an InfraEnv. User Stories Story 1 As an Infrastructure Admin, I want to create a Discovery ISO and use it to boot hosts for use by Cluster Creators to install OpenShift on. Story 2 As an Infrastructure Admin, I want to add Discovery ISOs to hosts as a boot option, and then ship the hosts to remote locations where they will be booted at a later time for use by Cluster Creators to install OpenShift on. Story 3 As a Cluster Creator, I want to view a collection of available Agents and use them for OpenShift cluster creation or expansion. Story 4 As a Cluster Creator, I want to reassign an Agent from one OpenShift cluster to another (not during installation). Implementation Details/Notes/Constraints It would be beneficial to move the discovery image management to a separate service at some point, both to allow it to be scaled independently and to reduce the very large scope of the existing service. The late binding work should be done with this separation in mind, even if the actual separation is an orthogonal task. Risks and Mitigations Design Details Currently, a cluster ID is baked into the ISO, and the agent relays that to the service upon registration. With late binding, the InfraEnv ID should replace the cluster ID in this context. The controller will also use this information to create the Agent CRs in the same namespaces as their associated InfraEnvs. Internally, in the SQL DB, hosts currently have two primary keys - the host ID and the cluster ID. Now the primary keys will be host ID and InfraEnv ID. Currently the agent writes the image from the ISO to disk, which means all Agents that form a cluster need to be running with the same version. Instead, the agent should download the correct version for the cluster being formed if it doesn't match what's in the ISO. When moving a host from a source to dest cluster the service needs to make sure that the source cluster is not installing (can be before or after) and that the dest cluster is before installation. If the source cluster is before installation, the host will automatically go to disconnected state, which is the correct behavior. However, the service should update its state manually so it doesn't take 3+ minutes (e.g., by resetting the timestamp of when it last checked in). If the host doesn't exist in the dest cluster a new host will be created (register_host). If it does exist, we reset its state and properties but not its events. There should be one pull secret for the ISO to pull the necessary images (Infrastructure Admin's'), and another pull secret in the cluster definition for the installed cluster to use (Cluster Creator's). Token revocation currently relies on the cluster ID. It should now rely on the InfraEnv ID. Open Questions UI Impact The UI will need to use the new REST APIs, but can keep the same flow and functionality. New screens will need to be created to add late binding functionality if we decide to expose it in the SaaS. Test Plan New tests will be needed at all levels. Drawbacks Alternatives Rather than keeping the existing DB schema, we could make image and host tables that are independent of clusters. This would be a much more difficult migration. It would also mean that we have no separation of host state between different cluster associations (e.g., we would need to clear host files and state when binding to a new cluster).","title":"agent-late-binding"},{"location":"enhancements/agent-late-binding/#agent-late-binding","text":"","title":"Agent Late Binding"},{"location":"enhancements/agent-late-binding/#summary","text":"In the current implementation of Assisted Installer, each Discovery ISO is associated with an existing cluster resource (a cluster definition that may not yet be an installed OCP cluster). When an agent comes up, it registers with the Assisted Service and is automatically bound to that same cluster resource. The only way to bind that host to a different cluster is to boot it with an ISO associated with the desired cluster. We can see that the process of configuring the infrastructure is closely tied to cluster creation. The late binding feature splits the process into two parts, each potentially performed by a different persona. The Discovery ISO is created by an infrastructure admin, who then boots the hosts. Each host becomes visible to the cluster creators but not bound to any specific cluster resource. At a later time, a cluster creator can create or expand a cluster by binding available hosts to it.","title":"Summary"},{"location":"enhancements/agent-late-binding/#motivation","text":"The benefits of late binding are as follows: Adjusts the flow to suit the two different personas (infrastructure administrator and cloud creator). Allows for a more cloud-like use case where compute resources of different flavors can be consumed, similar to IBM Satellite. Allows agent-based installation to fit more naturally into a MAPI or CAPI model. While the details of such an integration are not in the scope of this enhancement, the idea is that hosts are booted, thus creating a collection of unassigned agents, and the machine creation involves choosing one such agent and beginning its installation. Reduces the number of Discovery ISOs, as there is no need to generate and download one for each cluster installation. Each ISO incurs capacity overhead for the service, and management overhead for the user which conflicts with our ease-of-use goal. Allows the user to see their inventory and then make decisions on how to build clusters based on hardware and connectivity. At a later stage, the service can make recommendations. This would be a layer of logic above late binding and is out of scope for this enhancement. Enables the user story where the Discovery ISO is loaded at the factory, and then discovered at a later time.","title":"Motivation"},{"location":"enhancements/agent-late-binding/#goals","text":"Note: The language here refers to CRDs, but also holds true for the corresponding REST resources Make the InfraEnv and Agent resources independent from the ClusterDeployment. A single InfraEnv should have the potential to boot any number of hosts, whose Agents register with no relation to any particular ClusterDeployment. An Agent may be assigned to a ClusterDeployment at a later time. An Agent may be associated with a different ClusterDeployment at any time, unless it is part of an ongoing cluster installation.","title":"Goals"},{"location":"enhancements/agent-late-binding/#non-goals","text":"Note: The language here refers to CRDs, but also holds true for the corresponding REST resources Once a host boots from disk, booting the Discovery ISO again to return it to the collection of free Agents is considered to be a manual step. There are several options that we will explore whereby this can be done automatically, but that discussion is out of scope for this enhancement. At a later stage, Agents may be automatically assigned to clusters based on their discovered inventory and/or network configurations. This is future work and not in the scope of this enhancement. For SaaS, there are discussions about changing the tenancy model to include organizations and not only users. Those decisions are out of scope for this document.","title":"Non-Goals"},{"location":"enhancements/agent-late-binding/#proposal","text":"There are two main (non-breaking) changes to the CRDs: 1. ClusterRef in the InfraEnv Spec will no longer be mandatory. 1. ClusterRef in the Agent Spec will no longer be mandatory. A Discovery ISO with no associated cluster means that hosts booted with that ISO will have agents that are not bound to any cluster. Thus, Agent CRs can be created with no cluster binding. It will still be possible to retain the previous behavior by providing a ClusterRef. The tenancy model will remain unchanged, with Agents being created in the namespace of the InfraEnv they booted from. The Agent's ClusterRef may now be deleted or changed dynamically, unless the Agent is part of an ongoing cluster installation, in which case the installation must first be cancelled (not currently possible via the k8s API). If the host had started/completed its installation, a new Agent Condition would indicate that the user needs to boot the host into the Discovery ISO once again (as mentioned in the non-goals, this can be done automatically but is not in the scope of this enhancement). The REST API will require breaking changes, so we will create a v2 API. At the same time we will better align with the k8s API (change image to infra-env, but don't change host to agent because it will require too many changes). 1. Move images (infra-envs) to be a standalone resource, rather than part of the cluster resource ( /clusters/{cluster_id}/downloads/image to /infra-envs ). This removes the dependency of an image on a specific cluster, removes the current limitation of one image per cluster, and aligns with InfraEnv CRD. 1. An agent is now always referenced using a combination of the infra-env ID and host ID, and the REST API path will be /infra-envs/{infra_env_id}/hosts/{host_id} rather than /clusters/{cluster_id}/hosts/{host_id} . This means that a server that is booted from the ISO of infra-env1 and then from the ISO of infra-env2 will generate two Agents (the first's state should be disconnected ). The Agent will have an optional cluster-id property (foreign key in the DB) that signifies which cluster the Agent is bound to, if any. 1. The InfraEnv resource will have an additional optional cluster-id property that will be used for non-late-binding flows. When an Agent registers itself, the assisted-service should check if the Agent's InfraEnv has the cluster-id property set, and if so, set that property on the Agent accordingly. For the SaaS, the service will record ownership (user and org ID) for InfraEnvs, and Agents will inherit the same ownership. Agent validations that do not depend on being part of a cluster will run for Agents that are both associated with clusters and those that are not. Examples of these include minimum hardware for any role, valid hostname, and registry access. Validations that do depend on being part of a cluster, such as connectivity checks, will run only for Agents associated with clusters. Events are currently always scoped to a cluster. We will now need two types of events: 1. The events that we have today, which should be visible to the Cluster Creator. 1. The events for an Agent which is not associated with a cluster, for example a history of when it registered and which clusters it was associated with (should be visible to the infrastructure admin). These events should be scoped to an InfraEnv.","title":"Proposal"},{"location":"enhancements/agent-late-binding/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/agent-late-binding/#story-1","text":"As an Infrastructure Admin, I want to create a Discovery ISO and use it to boot hosts for use by Cluster Creators to install OpenShift on.","title":"Story 1"},{"location":"enhancements/agent-late-binding/#story-2","text":"As an Infrastructure Admin, I want to add Discovery ISOs to hosts as a boot option, and then ship the hosts to remote locations where they will be booted at a later time for use by Cluster Creators to install OpenShift on.","title":"Story 2"},{"location":"enhancements/agent-late-binding/#story-3","text":"As a Cluster Creator, I want to view a collection of available Agents and use them for OpenShift cluster creation or expansion.","title":"Story 3"},{"location":"enhancements/agent-late-binding/#story-4","text":"As a Cluster Creator, I want to reassign an Agent from one OpenShift cluster to another (not during installation).","title":"Story 4"},{"location":"enhancements/agent-late-binding/#implementation-detailsnotesconstraints","text":"It would be beneficial to move the discovery image management to a separate service at some point, both to allow it to be scaled independently and to reduce the very large scope of the existing service. The late binding work should be done with this separation in mind, even if the actual separation is an orthogonal task.","title":"Implementation Details/Notes/Constraints"},{"location":"enhancements/agent-late-binding/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"enhancements/agent-late-binding/#design-details","text":"Currently, a cluster ID is baked into the ISO, and the agent relays that to the service upon registration. With late binding, the InfraEnv ID should replace the cluster ID in this context. The controller will also use this information to create the Agent CRs in the same namespaces as their associated InfraEnvs. Internally, in the SQL DB, hosts currently have two primary keys - the host ID and the cluster ID. Now the primary keys will be host ID and InfraEnv ID. Currently the agent writes the image from the ISO to disk, which means all Agents that form a cluster need to be running with the same version. Instead, the agent should download the correct version for the cluster being formed if it doesn't match what's in the ISO. When moving a host from a source to dest cluster the service needs to make sure that the source cluster is not installing (can be before or after) and that the dest cluster is before installation. If the source cluster is before installation, the host will automatically go to disconnected state, which is the correct behavior. However, the service should update its state manually so it doesn't take 3+ minutes (e.g., by resetting the timestamp of when it last checked in). If the host doesn't exist in the dest cluster a new host will be created (register_host). If it does exist, we reset its state and properties but not its events. There should be one pull secret for the ISO to pull the necessary images (Infrastructure Admin's'), and another pull secret in the cluster definition for the installed cluster to use (Cluster Creator's). Token revocation currently relies on the cluster ID. It should now rely on the InfraEnv ID.","title":"Design Details"},{"location":"enhancements/agent-late-binding/#open-questions","text":"","title":"Open Questions"},{"location":"enhancements/agent-late-binding/#ui-impact","text":"The UI will need to use the new REST APIs, but can keep the same flow and functionality. New screens will need to be created to add late binding functionality if we decide to expose it in the SaaS.","title":"UI Impact"},{"location":"enhancements/agent-late-binding/#test-plan","text":"New tests will be needed at all levels.","title":"Test Plan"},{"location":"enhancements/agent-late-binding/#drawbacks","text":"","title":"Drawbacks"},{"location":"enhancements/agent-late-binding/#alternatives","text":"Rather than keeping the existing DB schema, we could make image and host tables that are independent of clusters. This would be a much more difficult migration. It would also mean that we have no separation of host state between different cluster associations (e.g., we would need to clear host files and state when binding to a new cluster).","title":"Alternatives"},{"location":"enhancements/agents-back-to-infraenv/","text":"Return Agents back to InfraEnv - late binding Summary When a user deletes a ClusterDeployment/AgentClusterInstall resource, the Agents resources bound to that CD should not be deleted. In that case, the Agents should be unbound and if needed the host should be rebooted with the Discovery ISO. In addition, the user should be able to unbind a single Agent from a ClusterDeployment so that this Agent should be available back in the InfraEnv running with the Discovery ISO even if the Agent is already installed or in error/cancelled phases. Note that this mechanism should be available only for Agents created from an InFraEnv that is not associated to a ClusterDeployment. Motivation In the late binding flow, the cluster creator selects the Agents he wants to include in a new cluster from a collection of free Agents managed by the infrastructure administrator. Once the cluster creator deletes the cluster or unbinds a specific Agent, the Agents should be available back to the collection of free Agents for use in future clusters. Goals Return Agents to the InfraEnv after the CD/ACI they are bound to is deleted. Return Agents to the InfraEnv after the Agent is unbound even if the Agent is already installed. If needed, the host should be rebooted with the Discovery ISO. Non-Goals Graceful removal of nodes from an installed OpenShift cluster is out of scope of this proposal. Unbinding a single Agent that is in installation phases is not supported for now. Proposal Once a user deletes a CD/ACI created with late binding, the assisted-service will not delete the Agents/Hosts. It will remove the CD reference from the Agents and in case that it needs to be rebooted, the host will move to a new state unbinding-requires-user-action . If the user unbinds a single Agent that is already installed or in error/cancelled phases, the host will move to the state unbinding-requires-user-action . The host will move to the state unbinding-requires-user-action if it was in one of the following state: Installed , Cancelled , Error or Added To Existing Cluster . In case of Zero Touch Provisioning, the Bare Metal Agent Controller will detect the state and it will reboot the corresponding BareMetalHost with the Discovery ISO. In case of Boot It YourSelf, it is the user responsibility to reboot the host. assisted-service When a cluster is deleted: - For each of the hosts: - If the Host's InfraEnv is associated to the Cluster (not late binding): - Delete the Host - If the Host's InfraEnv is not associated to the Cluster (late binding): - Unbind the Host. assisted-service ClusterDeployment controller When a ClusterDeployment/AgentClusterInstall is deleted: For each of the agents: If the Host's InfraEnv is associated to the Cluster (not late binding): Delete the Agent CR If the Host's InfraEnv is not associated to the Cluster (late binding): Unbind by updating the Agent CR CD's reference to nil. Agent Controller Support new Reason for Unbound condition in case the host is in unbinding-requires-user-action state. assisted-service host state machine When UnBindHost is called, move to the state of unbinding-requires-user-action if the current state is in one of the following states: Installed , Cancelled , Error or Added To Existing Cluster . Bare Metal Agent Controller The Bare Metal Agent Controller will watch for Agents and monitor the Bound condition. If the Reason is unbinding-requires-user-action , the BareMetalHost will be rebooted with the Discovery ISO. User Stories Story 1 As an Infrastructure Admin, I want that hosts that are not used anymore by the Cluster Creator to be available back as unbound Agents in the original InfraEnv. Story 2 As a Cluster Creator, I want to be able to delete a ClusterDeployment with Agents so that these Agents will be available to create a new Cluster. Story 3 As a Cluster Creator, I want to be able to unbind an Agent from a ClusterDeployment so that this Agent will be available to use in a new Cluster even if the Agent is already installed or in error/cancelled stages. Implementation Details/Notes/Constraints On Boot It Yourself flow, it is the user responsibility to reboot the host and remove the need-boot label. Risks and Mitigations Design Details [optional] Open Questions Should we support in the future to Unbind a single Host that is in installation phases? If the installation already started, should there be a mechanism to signal the Agent to stop the installation and register again? (Optimization path, instead of rebooting the host if Agent is still running) UI Impact There should be no impact for UI in Central Management Infrastructure flows. Test Plan Test Cases: * Create a cluster with Agents not using late-binding - Delete the cluster. Agents should be deleted. * Create a cluster with Agents using late-binding - Delete the cluster. Agents should not be deleted. - The Agents should not have a reference to the CD. - Bound condition should be if false with reason UnbindingPendingUserAction if the agents were in Installed , Cancelled , Error or Added To Existing Cluster . - If BMH is used, the host should be rebooted. * Install a cluster with Agents using late-binding - Unbind an installed Agent - Bound condition should be if false with reason UnbindingPendingUserAction if the agents were in Installed , Cancelled , Error or Added To Existing Cluster . - If BMH is used, the host should be rebooted. Drawbacks Alternatives Instead of the keeping the Agent, the controller can delete it, and mark the BMH to reboot from ISO. The Agent will be recreated after the reboot of the host. In this case, the spec of the Agent will be lost (Hostname, Role, Installation Disk) and all the hosts will be rebooted regardless of the Agent state. Note, that having the Agent CR provides easier debug source for the user.","title":"agents-back-to-infraenv"},{"location":"enhancements/agents-back-to-infraenv/#return-agents-back-to-infraenv-late-binding","text":"","title":"Return Agents back to InfraEnv - late binding"},{"location":"enhancements/agents-back-to-infraenv/#summary","text":"When a user deletes a ClusterDeployment/AgentClusterInstall resource, the Agents resources bound to that CD should not be deleted. In that case, the Agents should be unbound and if needed the host should be rebooted with the Discovery ISO. In addition, the user should be able to unbind a single Agent from a ClusterDeployment so that this Agent should be available back in the InfraEnv running with the Discovery ISO even if the Agent is already installed or in error/cancelled phases. Note that this mechanism should be available only for Agents created from an InFraEnv that is not associated to a ClusterDeployment.","title":"Summary"},{"location":"enhancements/agents-back-to-infraenv/#motivation","text":"In the late binding flow, the cluster creator selects the Agents he wants to include in a new cluster from a collection of free Agents managed by the infrastructure administrator. Once the cluster creator deletes the cluster or unbinds a specific Agent, the Agents should be available back to the collection of free Agents for use in future clusters.","title":"Motivation"},{"location":"enhancements/agents-back-to-infraenv/#goals","text":"Return Agents to the InfraEnv after the CD/ACI they are bound to is deleted. Return Agents to the InfraEnv after the Agent is unbound even if the Agent is already installed. If needed, the host should be rebooted with the Discovery ISO.","title":"Goals"},{"location":"enhancements/agents-back-to-infraenv/#non-goals","text":"Graceful removal of nodes from an installed OpenShift cluster is out of scope of this proposal. Unbinding a single Agent that is in installation phases is not supported for now.","title":"Non-Goals"},{"location":"enhancements/agents-back-to-infraenv/#proposal","text":"Once a user deletes a CD/ACI created with late binding, the assisted-service will not delete the Agents/Hosts. It will remove the CD reference from the Agents and in case that it needs to be rebooted, the host will move to a new state unbinding-requires-user-action . If the user unbinds a single Agent that is already installed or in error/cancelled phases, the host will move to the state unbinding-requires-user-action . The host will move to the state unbinding-requires-user-action if it was in one of the following state: Installed , Cancelled , Error or Added To Existing Cluster . In case of Zero Touch Provisioning, the Bare Metal Agent Controller will detect the state and it will reboot the corresponding BareMetalHost with the Discovery ISO. In case of Boot It YourSelf, it is the user responsibility to reboot the host.","title":"Proposal"},{"location":"enhancements/agents-back-to-infraenv/#assisted-service","text":"When a cluster is deleted: - For each of the hosts: - If the Host's InfraEnv is associated to the Cluster (not late binding): - Delete the Host - If the Host's InfraEnv is not associated to the Cluster (late binding): - Unbind the Host.","title":"assisted-service"},{"location":"enhancements/agents-back-to-infraenv/#assisted-service-clusterdeployment-controller","text":"When a ClusterDeployment/AgentClusterInstall is deleted: For each of the agents: If the Host's InfraEnv is associated to the Cluster (not late binding): Delete the Agent CR If the Host's InfraEnv is not associated to the Cluster (late binding): Unbind by updating the Agent CR CD's reference to nil.","title":"assisted-service ClusterDeployment controller"},{"location":"enhancements/agents-back-to-infraenv/#agent-controller","text":"Support new Reason for Unbound condition in case the host is in unbinding-requires-user-action state.","title":"Agent Controller"},{"location":"enhancements/agents-back-to-infraenv/#assisted-service-host-state-machine","text":"When UnBindHost is called, move to the state of unbinding-requires-user-action if the current state is in one of the following states: Installed , Cancelled , Error or Added To Existing Cluster .","title":"assisted-service host state machine"},{"location":"enhancements/agents-back-to-infraenv/#bare-metal-agent-controller","text":"The Bare Metal Agent Controller will watch for Agents and monitor the Bound condition. If the Reason is unbinding-requires-user-action , the BareMetalHost will be rebooted with the Discovery ISO.","title":"Bare Metal Agent Controller"},{"location":"enhancements/agents-back-to-infraenv/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/agents-back-to-infraenv/#story-1","text":"As an Infrastructure Admin, I want that hosts that are not used anymore by the Cluster Creator to be available back as unbound Agents in the original InfraEnv.","title":"Story 1"},{"location":"enhancements/agents-back-to-infraenv/#story-2","text":"As a Cluster Creator, I want to be able to delete a ClusterDeployment with Agents so that these Agents will be available to create a new Cluster.","title":"Story 2"},{"location":"enhancements/agents-back-to-infraenv/#story-3","text":"As a Cluster Creator, I want to be able to unbind an Agent from a ClusterDeployment so that this Agent will be available to use in a new Cluster even if the Agent is already installed or in error/cancelled stages.","title":"Story 3"},{"location":"enhancements/agents-back-to-infraenv/#implementation-detailsnotesconstraints","text":"On Boot It Yourself flow, it is the user responsibility to reboot the host and remove the need-boot label.","title":"Implementation Details/Notes/Constraints"},{"location":"enhancements/agents-back-to-infraenv/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"enhancements/agents-back-to-infraenv/#design-details-optional","text":"","title":"Design Details [optional]"},{"location":"enhancements/agents-back-to-infraenv/#open-questions","text":"Should we support in the future to Unbind a single Host that is in installation phases? If the installation already started, should there be a mechanism to signal the Agent to stop the installation and register again? (Optimization path, instead of rebooting the host if Agent is still running)","title":"Open Questions"},{"location":"enhancements/agents-back-to-infraenv/#ui-impact","text":"There should be no impact for UI in Central Management Infrastructure flows.","title":"UI Impact"},{"location":"enhancements/agents-back-to-infraenv/#test-plan","text":"Test Cases: * Create a cluster with Agents not using late-binding - Delete the cluster. Agents should be deleted. * Create a cluster with Agents using late-binding - Delete the cluster. Agents should not be deleted. - The Agents should not have a reference to the CD. - Bound condition should be if false with reason UnbindingPendingUserAction if the agents were in Installed , Cancelled , Error or Added To Existing Cluster . - If BMH is used, the host should be rebooted. * Install a cluster with Agents using late-binding - Unbind an installed Agent - Bound condition should be if false with reason UnbindingPendingUserAction if the agents were in Installed , Cancelled , Error or Added To Existing Cluster . - If BMH is used, the host should be rebooted.","title":"Test Plan"},{"location":"enhancements/agents-back-to-infraenv/#drawbacks","text":"","title":"Drawbacks"},{"location":"enhancements/agents-back-to-infraenv/#alternatives","text":"Instead of the keeping the Agent, the controller can delete it, and mark the BMH to reboot from ISO. The Agent will be recreated after the reboot of the host. In this case, the spec of the Agent will be lost (Hostname, Role, Installation Disk) and all the hosts will be rebooted regardless of the Agent state. Note, that having the Agent CR provides easier debug source for the user.","title":"Alternatives"},{"location":"enhancements/arm64-cpu-architecture/","text":"ARM64 CPU architecture support Summary The default host CPU architecture supported for installation is x86_64 in Assisted Installer. In order to support additional CPU architectures, the assisted-service should accept and handle selection of arm64 CPU architecture as well. Motivation Although x86_64 is popular in many use cases, other CPU architectures are common in other segments. For example, ARM is popular in edge use cases. OpenShift has recently added support for arm64 and it is important to add this support to Assisted Installer. Goals Allow installation of clusters that are running on arm64 hosts. The introduced infrastructure should facilitate future support of more architectures (e.g. PPC). Non-Goals The assisted-service and assisted-image-service will not be built for arm64. No support for clusters with hosts of multiple architectures (e.g. x86_64 + arm64). Implementation for kube-api as ClusterImageSet and AgentServiceConfig CRDs. Though the feature design should consider implications. Day2 and Late binding won't be supported in first phase. On-Prem flows, for now the focus is for SaaS users. No baremetal platform support for arm64 in 4.9. I.e. only SNO for now. Proposal Host images The following images should also be built for arm64: * assisted-installer * assisted-installer-controller * assisted-installer-agent This should be handled in similar to the current flows by modifying relevant Dockerfiles to support arm64 build. The images should be published to the same registries and follow same naming conventions. I.e. The best approach would be using the same image URI for any architecture. Which means, minimal changes in the assisted-service, as the architecture specific image would be fetched according to the machine pulling the image. OCP versions Versions should have CPU architecture granularity for each API. So that OCP release images and RHCOS images of multiple architectures could be supported. The assisted-service should support the arm64 variant of RHCOS images and OCP release image (for OpenShift >= 4.9). As a preparation for supporting multiple CPU architectures in Version, the RHCOS images and release images information from OPENSHIFT_VERSIONS environment variable should be extracted into new variables: OS_IMAGES and RELEASE_IMAGES. We should probably use a simple array structure instead of mapping, to align with the k8s API convention in AgentServiceConfig structure. To simplify backwards compatibility, we can still support the old format of OPENSHIFT_VERSIONS by keeping the current properties for old versions (<4.9). OPENSHIFT_VERSIONS This environment variable should be deprecated and kept only for backwards compatibility. I.e. Instead, RELEASE_IMAGES and OS_IMAGES should be set (if missing, fallback to previous behavior). RELEASE_IMAGES A list of available release images (one for each minor OCP version and CPU architecture): [ { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"x86_64\", \"url\": \"quay.io/openshift-release-dev/ocp-release:4.9.0-rc.4-x86_64\", \"version\": \"4.9.0-rc.4\" }, { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"arm64\", \"url\": \"quay.io/openshift-release-dev/ocp-release:4.9.0-rc.4-aarch64\", \"version\": \"4.9.0-rc.4\" } ] OS_IMAGES A list of available OS images (one for each minor OCP version and CPU architecture): [ { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"x86_64\", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live.x86_64.iso\", \"rootfs_url\": \"https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live-rootfs.x86_64.img\", \"version\": \"49.84.202107032011-0\" }, { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"arm64\", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/aarch64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live.aarch64.iso\", \"rootfs_url\": \"https://mirror.openshift.com/pub/openshift-v4/aarch64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live-rootfs.aarch64.img\", \"version\": \"49.84.202106272247-0\" } ] Cluster creation and ISO generation When creating a new cluster, cpu_architecture property should be supported to define which hosts can be registered to it. This property would be added to cluster-create-params and stored in cluster upon creation. If not specified, defaulted to x86_64 as a fallback. An example for the swagger schema: cpu_architecture: type: string default: 'x86_64' description: The CPU architecture of the image (x86_64/arm64/etc) When generating a discovery ISO, the proper RHCOS/Release images should be used according to CPU architecture specified in the cluster. The cpu_architecture property should be stored in infraenv after the ISO is generated. User Stories Story 1 As an Assisted-Installer user, I need to install a cluster on non x86_64 hosts. Story 2 As an Assisted-Installer user, I need to install a Single-Node OpenShift cluster on an arm64 host. Implementation Details/Notes/Constraints [optional] The current focus of the enhancement is for SaaS users. Only SNO is supported as there's no Baremetal support for arm64 in 4.9 (arm packages for Ironic, etc). Late binding is not covered by this enhancement, but decoupling the discovery ISO generation from the cluster should be simple enough. I.e. adding cpu_architecture property to infraenv_create_params and override the architecture defined in the cluster. Also, architecture validation should be probably added to host discovery conditions (i.e. to ensure that the host CPU architecture is similar to the cluster's architecture). When supporting Day2, it should be handled by adding cpu_architecture property in AddHostsClusterCreateParams , which is needed for generating the proper discovery ISO. We could probably rely on the current validations of host discovery, i.e. the user should ensure architecture when generating the ISO. Need to verify IPv6 flows: OVN-Kubernetes LSO/OCS/CNV operators: add a validation for x86_64 CPU architecture. See: 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied' Open Questions kube-api considerations ClusterImageSet Since we extract the openshift-baremetal-install binary in the assisted-service (to create the manifests of the cluster), the x86_64 OCP release image has to be used. Which means that both arm64 and x86_64 release images must be provided. The OCP release image probably won't support the manifest list feature, so we couldn't rely on it either. Alternatives: * We could add another URL for the x86_64 release image in ClusterImageSet. * As a workaround, we could use something like qemu-user-static , which enables invocation of multi-architecture containers using QEMU. I.e it might be sufficient just for running the arm64 openshift-baremetal-install binary, though not sure we want to rely on such solution. For now, we'll try to start with supporting None platform only. So we could use the x86_64 openshift-install binary embedded in the arm release. See: extract Linux binaries for multiple architectures AgentServiceConfig osImages property currently contains a single set of images for each OCP version. E.g. osImages: - openshiftVersion: '4.8' rootFSUrl: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.8/4.8.2/rhcos-live-rootfs.x86_64.img url: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.8/4.8.2/rhcos-4.8.2-x86_64-live.x86_64.iso version: 48.84.202107202156-0 We should consider API changes to support multiple architectures for each version. Perhaps just add cpuArchitecture property for each image, i.e. multiple images for each version. Would need to modify parsing handling obviously. UI Impact New cluster dialog: Add 'CPU Architecture' select box. Should be displayed only for versions >= 4.9. Should be disabled on update cluster dialog. Generate Discovery ISO: Add an indication for the selected CPU architecture. Test Plan Need dedicated arm64 machines for running tests in the CI. Need to consider which tests to run for arm64 clusters in the CI (using assisted-test-infra). We could probably start with merely the positive flow of full SNO installation, and add tests afterwards if required. Alternatives Rather than explicitly defining CPU architecture in the cluster resource, it can be inferred by the firstly added host. I.e. the first registered host would the determine the cluster's architecture, so any additional hosts would have to validate against that host. That means the architecture should be set only when creating an ISO, which might be clearer to the user. However, the drawback of having validation warnings is probably worse UX any way.","title":"arm64-cpu-architecture"},{"location":"enhancements/arm64-cpu-architecture/#arm64-cpu-architecture-support","text":"","title":"ARM64 CPU architecture support"},{"location":"enhancements/arm64-cpu-architecture/#summary","text":"The default host CPU architecture supported for installation is x86_64 in Assisted Installer. In order to support additional CPU architectures, the assisted-service should accept and handle selection of arm64 CPU architecture as well.","title":"Summary"},{"location":"enhancements/arm64-cpu-architecture/#motivation","text":"Although x86_64 is popular in many use cases, other CPU architectures are common in other segments. For example, ARM is popular in edge use cases. OpenShift has recently added support for arm64 and it is important to add this support to Assisted Installer.","title":"Motivation"},{"location":"enhancements/arm64-cpu-architecture/#goals","text":"Allow installation of clusters that are running on arm64 hosts. The introduced infrastructure should facilitate future support of more architectures (e.g. PPC).","title":"Goals"},{"location":"enhancements/arm64-cpu-architecture/#non-goals","text":"The assisted-service and assisted-image-service will not be built for arm64. No support for clusters with hosts of multiple architectures (e.g. x86_64 + arm64). Implementation for kube-api as ClusterImageSet and AgentServiceConfig CRDs. Though the feature design should consider implications. Day2 and Late binding won't be supported in first phase. On-Prem flows, for now the focus is for SaaS users. No baremetal platform support for arm64 in 4.9. I.e. only SNO for now.","title":"Non-Goals"},{"location":"enhancements/arm64-cpu-architecture/#proposal","text":"","title":"Proposal"},{"location":"enhancements/arm64-cpu-architecture/#host-images","text":"The following images should also be built for arm64: * assisted-installer * assisted-installer-controller * assisted-installer-agent This should be handled in similar to the current flows by modifying relevant Dockerfiles to support arm64 build. The images should be published to the same registries and follow same naming conventions. I.e. The best approach would be using the same image URI for any architecture. Which means, minimal changes in the assisted-service, as the architecture specific image would be fetched according to the machine pulling the image.","title":"Host images"},{"location":"enhancements/arm64-cpu-architecture/#ocp-versions","text":"Versions should have CPU architecture granularity for each API. So that OCP release images and RHCOS images of multiple architectures could be supported. The assisted-service should support the arm64 variant of RHCOS images and OCP release image (for OpenShift >= 4.9). As a preparation for supporting multiple CPU architectures in Version, the RHCOS images and release images information from OPENSHIFT_VERSIONS environment variable should be extracted into new variables: OS_IMAGES and RELEASE_IMAGES. We should probably use a simple array structure instead of mapping, to align with the k8s API convention in AgentServiceConfig structure. To simplify backwards compatibility, we can still support the old format of OPENSHIFT_VERSIONS by keeping the current properties for old versions (<4.9).","title":"OCP versions"},{"location":"enhancements/arm64-cpu-architecture/#openshift_versions","text":"This environment variable should be deprecated and kept only for backwards compatibility. I.e. Instead, RELEASE_IMAGES and OS_IMAGES should be set (if missing, fallback to previous behavior).","title":"OPENSHIFT_VERSIONS"},{"location":"enhancements/arm64-cpu-architecture/#release_images","text":"A list of available release images (one for each minor OCP version and CPU architecture): [ { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"x86_64\", \"url\": \"quay.io/openshift-release-dev/ocp-release:4.9.0-rc.4-x86_64\", \"version\": \"4.9.0-rc.4\" }, { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"arm64\", \"url\": \"quay.io/openshift-release-dev/ocp-release:4.9.0-rc.4-aarch64\", \"version\": \"4.9.0-rc.4\" } ]","title":"RELEASE_IMAGES"},{"location":"enhancements/arm64-cpu-architecture/#os_images","text":"A list of available OS images (one for each minor OCP version and CPU architecture): [ { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"x86_64\", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live.x86_64.iso\", \"rootfs_url\": \"https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live-rootfs.x86_64.img\", \"version\": \"49.84.202107032011-0\" }, { \"openshift_version\": \"4.9\", \"cpu_architecture\": \"arm64\", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/aarch64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live.aarch64.iso\", \"rootfs_url\": \"https://mirror.openshift.com/pub/openshift-v4/aarch64/dependencies/rhcos/pre-release/latest-4.9/rhcos-live-rootfs.aarch64.img\", \"version\": \"49.84.202106272247-0\" } ]","title":"OS_IMAGES"},{"location":"enhancements/arm64-cpu-architecture/#cluster-creation-and-iso-generation","text":"When creating a new cluster, cpu_architecture property should be supported to define which hosts can be registered to it. This property would be added to cluster-create-params and stored in cluster upon creation. If not specified, defaulted to x86_64 as a fallback. An example for the swagger schema: cpu_architecture: type: string default: 'x86_64' description: The CPU architecture of the image (x86_64/arm64/etc) When generating a discovery ISO, the proper RHCOS/Release images should be used according to CPU architecture specified in the cluster. The cpu_architecture property should be stored in infraenv after the ISO is generated.","title":"Cluster creation and ISO generation"},{"location":"enhancements/arm64-cpu-architecture/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/arm64-cpu-architecture/#story-1","text":"As an Assisted-Installer user, I need to install a cluster on non x86_64 hosts.","title":"Story 1"},{"location":"enhancements/arm64-cpu-architecture/#story-2","text":"As an Assisted-Installer user, I need to install a Single-Node OpenShift cluster on an arm64 host.","title":"Story 2"},{"location":"enhancements/arm64-cpu-architecture/#implementation-detailsnotesconstraints-optional","text":"The current focus of the enhancement is for SaaS users. Only SNO is supported as there's no Baremetal support for arm64 in 4.9 (arm packages for Ironic, etc). Late binding is not covered by this enhancement, but decoupling the discovery ISO generation from the cluster should be simple enough. I.e. adding cpu_architecture property to infraenv_create_params and override the architecture defined in the cluster. Also, architecture validation should be probably added to host discovery conditions (i.e. to ensure that the host CPU architecture is similar to the cluster's architecture). When supporting Day2, it should be handled by adding cpu_architecture property in AddHostsClusterCreateParams , which is needed for generating the proper discovery ISO. We could probably rely on the current validations of host discovery, i.e. the user should ensure architecture when generating the ISO. Need to verify IPv6 flows: OVN-Kubernetes LSO/OCS/CNV operators: add a validation for x86_64 CPU architecture. See: 'lso-requirements-satisfied' 'ocs-requirements-satisfied' 'cnv-requirements-satisfied'","title":"Implementation Details/Notes/Constraints [optional]"},{"location":"enhancements/arm64-cpu-architecture/#open-questions","text":"","title":"Open Questions"},{"location":"enhancements/arm64-cpu-architecture/#kube-api-considerations","text":"","title":"kube-api considerations"},{"location":"enhancements/arm64-cpu-architecture/#clusterimageset","text":"Since we extract the openshift-baremetal-install binary in the assisted-service (to create the manifests of the cluster), the x86_64 OCP release image has to be used. Which means that both arm64 and x86_64 release images must be provided. The OCP release image probably won't support the manifest list feature, so we couldn't rely on it either. Alternatives: * We could add another URL for the x86_64 release image in ClusterImageSet. * As a workaround, we could use something like qemu-user-static , which enables invocation of multi-architecture containers using QEMU. I.e it might be sufficient just for running the arm64 openshift-baremetal-install binary, though not sure we want to rely on such solution. For now, we'll try to start with supporting None platform only. So we could use the x86_64 openshift-install binary embedded in the arm release. See: extract Linux binaries for multiple architectures","title":"ClusterImageSet"},{"location":"enhancements/arm64-cpu-architecture/#agentserviceconfig","text":"osImages property currently contains a single set of images for each OCP version. E.g. osImages: - openshiftVersion: '4.8' rootFSUrl: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.8/4.8.2/rhcos-live-rootfs.x86_64.img url: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.8/4.8.2/rhcos-4.8.2-x86_64-live.x86_64.iso version: 48.84.202107202156-0 We should consider API changes to support multiple architectures for each version. Perhaps just add cpuArchitecture property for each image, i.e. multiple images for each version. Would need to modify parsing handling obviously.","title":"AgentServiceConfig"},{"location":"enhancements/arm64-cpu-architecture/#ui-impact","text":"New cluster dialog: Add 'CPU Architecture' select box. Should be displayed only for versions >= 4.9. Should be disabled on update cluster dialog. Generate Discovery ISO: Add an indication for the selected CPU architecture.","title":"UI Impact"},{"location":"enhancements/arm64-cpu-architecture/#test-plan","text":"Need dedicated arm64 machines for running tests in the CI. Need to consider which tests to run for arm64 clusters in the CI (using assisted-test-infra). We could probably start with merely the positive flow of full SNO installation, and add tests afterwards if required.","title":"Test Plan"},{"location":"enhancements/arm64-cpu-architecture/#alternatives","text":"Rather than explicitly defining CPU architecture in the cluster resource, it can be inferred by the firstly added host. I.e. the first registered host would the determine the cluster's architecture, so any additional hosts would have to validate against that host. That means the architecture should be set only when creating an ISO, which might be clearer to the user. However, the drawback of having validation warnings is probably worse UX any way.","title":"Alternatives"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/","text":"BMAC reboots only those hosts impacted by an ISO change Summary When an InfraEnv's ISO gets regenerated based on a change such as adding or changing an NMStateConfig resource, there may be only a subset of that InfraEnv's Agents that are affected by the change. Rather than reboot all of them with the newly-generated ISO, only those hosts that are affected by the new or modified NMStateConfig should be rebooted. It is assumed that in all cases, Agents that have started installing will not be restarted due to an ISO change. Motivation If there are 500 NMStateConfigs and 500 Agents all related to the same InfraEnv, and then someone adds the 501st NMStateConfig, we do not want to reboot all 500 Agents. Rebooting is an expensive and time-consuming operation. We need a way to determine which Agents or hosts should be rebooted in response to an NMStateConfig. Goals Avoid rebooting un-provisioned BareMetalHosts when it is not needed. Leave the door open for other platforms (include None/boot-it-yourself) to also avoid host reboots that are not needed. Non-Goals Proposal The assisted-service will track whether a particular Agent was booted using an old InfraEnv or NMStateConfig. It will ensure that any Agent that was booted using obsolete artifacts will have: * a label indicating that it was booted from obsolete artifacts * an array in its Status of references to the resources that are obsolete assisted-service InfraEnv controller The InfraEnv ISO URL changes when the ISO contents change. This makes it obvious to an API user that they have booted a host from an obsolete ISO, because the URLs won't match. When assisted-service creates an ISO, it will capture and include in the ISO: * the name, namespace, UID, and generation of the InfraEnv * the name, namespace, UID, and generation of each NMStateConfig At runtime, those resource identifiers for the InfraEnv and the NMStateConfig it used will be passed by the agent to assisted-service and added to the Agent's Status. assisted-service Agent controller When the Agent resource gets created, its status should include the above-described reference details for: * The InfraEnv * The NMStateConfig it utilized during its boot Upon comparing those references to the resources that currently exist, if any of the referenced resources have changed, then the controller will add a label to the Agent resource indicating such. It will also add a reference to each obsolete resource to an array in the Agent's Status. Bare Metal Agent Controller The Bare Metal Agent Controller will watch for Agents to have that label. If the Agent's NMStateConfig is obsolete, then it will reboot the corresponding BareMetalHost with the latest ISO URL. Else if the Agent's InfraEnv is obsolete, and the Agent does not have an associated NMStateConfig, then the BMAC will reboot the host only if finds a new NMStateConfig that matches the Agent. The BMAC will also watch for the InfraEnv URL to change, and if there are any BareMetalHosts that were booted with an obsolete URL but do not yet have an Agent resource, those BareMetalHosts will be rebooted with the latest ISO URL. +---------------------------+ | | | Agent has the label? | | | +----+-----------+----------+ | | v v +----+--+ +---+---+ +-------------+ | | | | | | | Yes | | No +----->+ No action | | | | | | | +---+---+ +-------+ +-------------+ | v +---+----------------+ | | | Agent Status | | cites an obsolete | | NMStateConfig? | | | +---+------------+---+ | | v v +---+---+ +---+---+ | | | | | Yes | | No | | | | | +---+---+ +---+---+ | | | v | +--+-------------------------+ | | | | | Is there a NMStateConfig | | | that matches the Agent? | | | | | +-----------+------------+---+ | | | | v v +----v-----+ +---+---+ +---+---+ +-------------+ | | | | | | | | | Reboot +<----------+ Yes | | No +---->+ No action | | | | | | | | | +----------+ +-------+ +-------+ +-------------+ User Stories Story 1 As a user of Zero Touch Provisioning, when I add and modify NMStateConfig resources, unaffected hosts will not reboot. Only those hosts directly impacted will be rebooted with a current ISO URL. Story 2 As a boot-it-yourself user, I will manually or with automation watch for Agents to have the obsolete artifact label and restart them with a fresh ISO. Implementation Details/Notes/Constraints [optional] Any Agent that has already started provisioning will be exempt from rebooting. Similar to current behavior, a new ISO URL will not be used until a quiet period has elapsed (currently 1 minute). This ensures that if multiple changes are happening in serial, for example many NMStateConfig resources are getting created or modified, that there will not be continuous churn of rebooting hosts with each change. Label Algorithm Given: * The Agent's Status includes a reference to the InfraEnv resource that was used to create its ISO. * The Agent's Status includes an optional reference to a NMStateConfig resource that was used to create the network config the agent used. IF: The existing InfraEnv generation or UID don't match the corresponding values reported in the Agent's Status. OR The existing NMStateConfig generation or UID don't match the corresponding values reported in the Agent's Status, or the NMStateConfig does not exist. THEN Ensure that a label exists on the Agent signifying that it was booted using obsolete artifacts. Ensure that the obsolete InfraEnv and/or NMStateConfig are listed in an array of obsolete resource references in the Agent status. ELSE Ensure that a label does not exist on the Agent signifying that it was booted using obsolete artifacts. Ensure that the Agent Status does not have an array of references to obsolete resources. Risks and Mitigations Design Details [optional] Open Questions What should the label's key and value be? How exactly would the resource references get embedded into the ISO, and how would the Agent communicate those back to assisted-service? UI Impact Test Plan Test Cases: * Create InfraEnv, add NMStateConfigs one by one. * Create InfraEnv, add invalid NMStateConfig, fix NMStateConfig * Update NMStateConfig for existing Agent * Have multiple NMStateConfigs for single Agent * Remove NMStateConfig for existing Agent * Add NMStateConfig after Agent is already up * Create InfraEnv and NMStateConfig, wait for the Agent to be up, then update proxy in InfraEnv Drawbacks Alternatives Each NMStateConfig resource could have a unique ISO URL. The InfraEnv would provide a base URL, and the NMStateConfig could be identified by query parameter or similar. This way, each host would get a unique URL and a unique ISO. When a particular NMStateConfig gets added or modified, it would implicitly only affect any BMH with that NMStateConfig's specific URL. Then BMAC would have to determine the correct NMStateConfig for a BareMetalHost in advance and assign the correct URL to the BMH. This option is more complicated, especially from an API perspective, than the proposal.","title":"bmac-reboot-hosts-on-iso-change"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#bmac-reboots-only-those-hosts-impacted-by-an-iso-change","text":"","title":"BMAC reboots only those hosts impacted by an ISO change"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#summary","text":"When an InfraEnv's ISO gets regenerated based on a change such as adding or changing an NMStateConfig resource, there may be only a subset of that InfraEnv's Agents that are affected by the change. Rather than reboot all of them with the newly-generated ISO, only those hosts that are affected by the new or modified NMStateConfig should be rebooted. It is assumed that in all cases, Agents that have started installing will not be restarted due to an ISO change.","title":"Summary"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#motivation","text":"If there are 500 NMStateConfigs and 500 Agents all related to the same InfraEnv, and then someone adds the 501st NMStateConfig, we do not want to reboot all 500 Agents. Rebooting is an expensive and time-consuming operation. We need a way to determine which Agents or hosts should be rebooted in response to an NMStateConfig.","title":"Motivation"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#goals","text":"Avoid rebooting un-provisioned BareMetalHosts when it is not needed. Leave the door open for other platforms (include None/boot-it-yourself) to also avoid host reboots that are not needed.","title":"Goals"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#non-goals","text":"","title":"Non-Goals"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#proposal","text":"The assisted-service will track whether a particular Agent was booted using an old InfraEnv or NMStateConfig. It will ensure that any Agent that was booted using obsolete artifacts will have: * a label indicating that it was booted from obsolete artifacts * an array in its Status of references to the resources that are obsolete","title":"Proposal"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#assisted-service-infraenv-controller","text":"The InfraEnv ISO URL changes when the ISO contents change. This makes it obvious to an API user that they have booted a host from an obsolete ISO, because the URLs won't match. When assisted-service creates an ISO, it will capture and include in the ISO: * the name, namespace, UID, and generation of the InfraEnv * the name, namespace, UID, and generation of each NMStateConfig At runtime, those resource identifiers for the InfraEnv and the NMStateConfig it used will be passed by the agent to assisted-service and added to the Agent's Status.","title":"assisted-service InfraEnv controller"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#assisted-service-agent-controller","text":"When the Agent resource gets created, its status should include the above-described reference details for: * The InfraEnv * The NMStateConfig it utilized during its boot Upon comparing those references to the resources that currently exist, if any of the referenced resources have changed, then the controller will add a label to the Agent resource indicating such. It will also add a reference to each obsolete resource to an array in the Agent's Status.","title":"assisted-service Agent controller"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#bare-metal-agent-controller","text":"The Bare Metal Agent Controller will watch for Agents to have that label. If the Agent's NMStateConfig is obsolete, then it will reboot the corresponding BareMetalHost with the latest ISO URL. Else if the Agent's InfraEnv is obsolete, and the Agent does not have an associated NMStateConfig, then the BMAC will reboot the host only if finds a new NMStateConfig that matches the Agent. The BMAC will also watch for the InfraEnv URL to change, and if there are any BareMetalHosts that were booted with an obsolete URL but do not yet have an Agent resource, those BareMetalHosts will be rebooted with the latest ISO URL. +---------------------------+ | | | Agent has the label? | | | +----+-----------+----------+ | | v v +----+--+ +---+---+ +-------------+ | | | | | | | Yes | | No +----->+ No action | | | | | | | +---+---+ +-------+ +-------------+ | v +---+----------------+ | | | Agent Status | | cites an obsolete | | NMStateConfig? | | | +---+------------+---+ | | v v +---+---+ +---+---+ | | | | | Yes | | No | | | | | +---+---+ +---+---+ | | | v | +--+-------------------------+ | | | | | Is there a NMStateConfig | | | that matches the Agent? | | | | | +-----------+------------+---+ | | | | v v +----v-----+ +---+---+ +---+---+ +-------------+ | | | | | | | | | Reboot +<----------+ Yes | | No +---->+ No action | | | | | | | | | +----------+ +-------+ +-------+ +-------------+","title":"Bare Metal Agent Controller"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#story-1","text":"As a user of Zero Touch Provisioning, when I add and modify NMStateConfig resources, unaffected hosts will not reboot. Only those hosts directly impacted will be rebooted with a current ISO URL.","title":"Story 1"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#story-2","text":"As a boot-it-yourself user, I will manually or with automation watch for Agents to have the obsolete artifact label and restart them with a fresh ISO.","title":"Story 2"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#implementation-detailsnotesconstraints-optional","text":"Any Agent that has already started provisioning will be exempt from rebooting. Similar to current behavior, a new ISO URL will not be used until a quiet period has elapsed (currently 1 minute). This ensures that if multiple changes are happening in serial, for example many NMStateConfig resources are getting created or modified, that there will not be continuous churn of rebooting hosts with each change.","title":"Implementation Details/Notes/Constraints [optional]"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#label-algorithm","text":"Given: * The Agent's Status includes a reference to the InfraEnv resource that was used to create its ISO. * The Agent's Status includes an optional reference to a NMStateConfig resource that was used to create the network config the agent used. IF: The existing InfraEnv generation or UID don't match the corresponding values reported in the Agent's Status. OR The existing NMStateConfig generation or UID don't match the corresponding values reported in the Agent's Status, or the NMStateConfig does not exist. THEN Ensure that a label exists on the Agent signifying that it was booted using obsolete artifacts. Ensure that the obsolete InfraEnv and/or NMStateConfig are listed in an array of obsolete resource references in the Agent status. ELSE Ensure that a label does not exist on the Agent signifying that it was booted using obsolete artifacts. Ensure that the Agent Status does not have an array of references to obsolete resources.","title":"Label Algorithm"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#design-details-optional","text":"","title":"Design Details [optional]"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#open-questions","text":"What should the label's key and value be? How exactly would the resource references get embedded into the ISO, and how would the Agent communicate those back to assisted-service?","title":"Open Questions"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#ui-impact","text":"","title":"UI Impact"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#test-plan","text":"Test Cases: * Create InfraEnv, add NMStateConfigs one by one. * Create InfraEnv, add invalid NMStateConfig, fix NMStateConfig * Update NMStateConfig for existing Agent * Have multiple NMStateConfigs for single Agent * Remove NMStateConfig for existing Agent * Add NMStateConfig after Agent is already up * Create InfraEnv and NMStateConfig, wait for the Agent to be up, then update proxy in InfraEnv","title":"Test Plan"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#drawbacks","text":"","title":"Drawbacks"},{"location":"enhancements/bmac-reboot-hosts-on-iso-change/#alternatives","text":"Each NMStateConfig resource could have a unique ISO URL. The InfraEnv would provide a base URL, and the NMStateConfig could be identified by query parameter or similar. This way, each host would get a unique URL and a unique ISO. When a particular NMStateConfig gets added or modified, it would implicitly only affect any BMH with that NMStateConfig's specific URL. Then BMAC would have to determine the correct NMStateConfig for a BareMetalHost in advance and assign the correct URL to the BMH. This option is more complicated, especially from an API perspective, than the proposal.","title":"Alternatives"},{"location":"enhancements/image-handling-service/","text":"Image Handling Service Summary Creating and serving the discovery image is a resource intensive operation which is significantly different from the other responsibilities of the assisted service. To remove the performance hit on the rest of the application and to independently scale these two operations the image handling functionality should be split into a separate service. This service will be responsible for creating and serving the discovery image. Motivation Most the assisted service functionality is based on many quick API calls with relatively small responses. Handling images is all about larger chunks of data and potentially longer operations which impacts the performance of the rest of the application. Splitting these responsibilities would allow us to manage and scale these deployments separately. Goals Allow image serving to be scaled independently of the rest of the application. Remove performance impact due to generating and serving images from the assisted service. Simplify assisted service codebase Make minimal changes to existing (or proposed) APIs. Non-Goals Serve a discovery image directly to a BMC Proposal The new service (image service) will handle customizing and serving the customized image to users. It will query assisted service to retrieve the information that needs to be added to the image based on the information passed in the request by the user. The assisted service will retain the POST /clusters/{cluster_id}/downloads/image endpoint which will save the image information to its database. In the case of the kube API, the controllers will continue to save image information from the various CRs as they do currently. The image service will expose a single API endpoint to download an image. Initially this should be compatible with the existing GET /clusters/{cluster_id}/downloads/image assisted service endpoint. When a user makes a request to this endpoint the image service will fetch the ignition and ramdisk (if necessary) from the assisted service and stream the customized iso to the client using the appropriate template image as a base. This will involve adding an endpoint to assisted service to expose the ramdisk data. The image service will be built as a standalone application with its own repository and tests. This will allow for the cleanest separation of responsibilities between the two services as well as make the distinction more clear to developers and reviewers. This should also make tests faster for assisted service as it won't have to handle iso download/upload on service startup or image generation tests. User Stories Story 1 As a user deploying and running assisted service, I need image serving to be horizontally scalable and to not impact performance of the assisted service API. Story 2 An additional deployment and service will be managed for all deployment methods. This includes SaaS, operator, and CI (minikube). Whether an additional image is created is still an open question. Implementation Details/Notes/Constraints Request Flow Image service receives a request to download an image This request contains as query parameters, the assisted service api key, the image type (minimal/full), and image version Image service queries assisted service for the image ignition Assisted service generates the ignition and serves it If the iso is minimal, the image service queries the assisted service for the initrd Assisted service generates the initrd and serves it Image service streams base iso with ignition and initrd embedded to the user The image service will receive requests through the existing assisted service route, but will add path based route configuration to move the traffic to the new service. Communication between the assisted service and the image service will be encrypted using service serving certificates and will use the cluster local service names (rather than the route). API Endpoint The image service will expose a single API endpoint: GET /images/{cluster_id}?version={base_image_verison}&type={image_type}&api_key={api_key} The cluster ID and api key will allow the image service to query the assisted service for the ignition and ramdisk. The image type and version will dictate which base image should be used. This request handler should also read any Authorization header and pass that on to any query to the assisted service to ensure authentication works correctly for downloads in the cloud. Accordingly this means that api_key should be optional as it will only be used for non-cloud deployments. Authentication and Authorization The image service will not need to implement authentication or authorization directly as it doesn't manage any user data. The user will provide a token to the image service and the image service will directly pass that token to assisted service on each call. Assisted service will validate the token before giving the image service any user information. Template Images Currently, when the assisted service starts, it downloads and caches each RHCOS image specified in the OPENSHIFT_VERSIONS environment variable. It also creates a minimal iso image for each version by removing the root filesystem. These \"template\" images are the base for creating the discovery iso. The image service will take on the responsibility of managing the RHCOS template images and creating the minimal ISO template images. This means that the image service will require persistent storage (when not using S3). Image Streaming The version of the image service described here would require editing the image while streaming the download rather than the current behavior which stores the customized image to be downloaded in a separate call. Allowing the customized discovery ISO to be stored would complicate the proposal and is discussed in the \"Alternatives\" section. Assisted Service API Changes For the image service to create the iso it needs some information from the assisted service. In the minimal ISO case, it will need the ignition and, if the user has configured static networking, an additional ramdisk image. For the full ISO, only the ignition is needed. Today the ignition is created and uploaded separately to S3 by the assisted service when the discovery image is generated. The ISO can then be downloaded using a presigned S3 URL and the ignition is made available for download through the /clusters/{cluster_id}/downloads/files endpoint. This proposal involves changing that behavior. As mentioned above, the ISO will be downloaded through the image service. The ignition will no longer be uploaded to storage, but will be rendered and served by the assisted service on-demand. An additional API for the minimal ISO ramdisk will behave in the same way. Upgrade Considerations For the operator-managed deployment, the new version of the operator will deploy the image service alongside assisted service and configure both to be properly aware of the other as well as alter the route to push download traffic to the image service. On the first InfraEnv controller reconcile, all the iso download URLs will be updated to reference the image service path, this change will then propagate to the BMH through the Bare Metal Agent controller. Any active agents will be restarted and will boot from the new image created by the image service. For the cloud, the existing images will continue to exist through direct presigned S3 links, but as this proposal removes the need for a \"generate image\" step, we will need to update those saved download URLs using a migration. Risks and Mitigations Relying strictly on streaming will increase the load on the image service in the SaaS deployment as presigned links directly to S3 would no longer be provided. Downloading the image through the service is already required when running on a local cluster so this shouldn't be a significant problem, but should be load tested. This will be further mitigated by splitting the service as many downloads won't affect the other operations of assisted service. Design Details [optional] Open Questions UI Impact Allowing only streaming would mean that the discovery image download modal would need to change to remove the presigned S3 URL and instead point users to the service directly. Any existing workarounds regarding time between generating images can be removed when images are no longer being generated in the background. Test Plan The image service can, and should, be tested in isolation using a mock assisted service API which will return some image info. As mentioned previously, load testing will also be needed on the SaaS deployment to ensure it can handle whatever peak image download load is expected. Drawbacks Maintaining and deploying an entirely separate service is complicated. No longer saving the image makes any future requirement to serve directly to a BMC much more difficult to implement. Adds traffic and load to the SaaS by removing images from S3. Alternatives Streaming vs Storing This proposal could be amended to handle either streaming or storing an image, but it would complicate authentication and authorization. In the current proposal every call to the image service would require a call to assisted service to fetch customization information. If the image is generated and stored by the image service, the download call will not need a call to assisted service and would require some other means of authentication and authorization. Scaling Assisted Service as-is Many of the goals of this proposal could be achieved by running multiple assisted service instances in parallel. One for handling image generation/download and another for serving all other requests. This behavior would to be dictated by special routing rules and feature gates. While possible, this would involve adding more logic and behavior switches to assisted service rather than simplifying the application. Additionally, assisted service is already difficult to scale in its current form. Each replica needs to take part in a leader election process to deal with database migration, downloading template images, and running controllers. Additionally, creating customized CoreOS ISOs is a fairly common task and splitting this into its own service and repository allows the logic to be as consumable as possible by other projects. Embedding this behavior into assisted service would limit collaboration and adoption by other teams.","title":"image-handling-service"},{"location":"enhancements/image-handling-service/#image-handling-service","text":"","title":"Image Handling Service"},{"location":"enhancements/image-handling-service/#summary","text":"Creating and serving the discovery image is a resource intensive operation which is significantly different from the other responsibilities of the assisted service. To remove the performance hit on the rest of the application and to independently scale these two operations the image handling functionality should be split into a separate service. This service will be responsible for creating and serving the discovery image.","title":"Summary"},{"location":"enhancements/image-handling-service/#motivation","text":"Most the assisted service functionality is based on many quick API calls with relatively small responses. Handling images is all about larger chunks of data and potentially longer operations which impacts the performance of the rest of the application. Splitting these responsibilities would allow us to manage and scale these deployments separately.","title":"Motivation"},{"location":"enhancements/image-handling-service/#goals","text":"Allow image serving to be scaled independently of the rest of the application. Remove performance impact due to generating and serving images from the assisted service. Simplify assisted service codebase Make minimal changes to existing (or proposed) APIs.","title":"Goals"},{"location":"enhancements/image-handling-service/#non-goals","text":"Serve a discovery image directly to a BMC","title":"Non-Goals"},{"location":"enhancements/image-handling-service/#proposal","text":"The new service (image service) will handle customizing and serving the customized image to users. It will query assisted service to retrieve the information that needs to be added to the image based on the information passed in the request by the user. The assisted service will retain the POST /clusters/{cluster_id}/downloads/image endpoint which will save the image information to its database. In the case of the kube API, the controllers will continue to save image information from the various CRs as they do currently. The image service will expose a single API endpoint to download an image. Initially this should be compatible with the existing GET /clusters/{cluster_id}/downloads/image assisted service endpoint. When a user makes a request to this endpoint the image service will fetch the ignition and ramdisk (if necessary) from the assisted service and stream the customized iso to the client using the appropriate template image as a base. This will involve adding an endpoint to assisted service to expose the ramdisk data. The image service will be built as a standalone application with its own repository and tests. This will allow for the cleanest separation of responsibilities between the two services as well as make the distinction more clear to developers and reviewers. This should also make tests faster for assisted service as it won't have to handle iso download/upload on service startup or image generation tests.","title":"Proposal"},{"location":"enhancements/image-handling-service/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/image-handling-service/#story-1","text":"As a user deploying and running assisted service, I need image serving to be horizontally scalable and to not impact performance of the assisted service API.","title":"Story 1"},{"location":"enhancements/image-handling-service/#story-2","text":"An additional deployment and service will be managed for all deployment methods. This includes SaaS, operator, and CI (minikube). Whether an additional image is created is still an open question.","title":"Story 2"},{"location":"enhancements/image-handling-service/#implementation-detailsnotesconstraints","text":"","title":"Implementation Details/Notes/Constraints"},{"location":"enhancements/image-handling-service/#request-flow","text":"Image service receives a request to download an image This request contains as query parameters, the assisted service api key, the image type (minimal/full), and image version Image service queries assisted service for the image ignition Assisted service generates the ignition and serves it If the iso is minimal, the image service queries the assisted service for the initrd Assisted service generates the initrd and serves it Image service streams base iso with ignition and initrd embedded to the user The image service will receive requests through the existing assisted service route, but will add path based route configuration to move the traffic to the new service. Communication between the assisted service and the image service will be encrypted using service serving certificates and will use the cluster local service names (rather than the route).","title":"Request Flow"},{"location":"enhancements/image-handling-service/#api-endpoint","text":"The image service will expose a single API endpoint: GET /images/{cluster_id}?version={base_image_verison}&type={image_type}&api_key={api_key} The cluster ID and api key will allow the image service to query the assisted service for the ignition and ramdisk. The image type and version will dictate which base image should be used. This request handler should also read any Authorization header and pass that on to any query to the assisted service to ensure authentication works correctly for downloads in the cloud. Accordingly this means that api_key should be optional as it will only be used for non-cloud deployments.","title":"API Endpoint"},{"location":"enhancements/image-handling-service/#authentication-and-authorization","text":"The image service will not need to implement authentication or authorization directly as it doesn't manage any user data. The user will provide a token to the image service and the image service will directly pass that token to assisted service on each call. Assisted service will validate the token before giving the image service any user information.","title":"Authentication and Authorization"},{"location":"enhancements/image-handling-service/#template-images","text":"Currently, when the assisted service starts, it downloads and caches each RHCOS image specified in the OPENSHIFT_VERSIONS environment variable. It also creates a minimal iso image for each version by removing the root filesystem. These \"template\" images are the base for creating the discovery iso. The image service will take on the responsibility of managing the RHCOS template images and creating the minimal ISO template images. This means that the image service will require persistent storage (when not using S3).","title":"Template Images"},{"location":"enhancements/image-handling-service/#image-streaming","text":"The version of the image service described here would require editing the image while streaming the download rather than the current behavior which stores the customized image to be downloaded in a separate call. Allowing the customized discovery ISO to be stored would complicate the proposal and is discussed in the \"Alternatives\" section.","title":"Image Streaming"},{"location":"enhancements/image-handling-service/#assisted-service-api-changes","text":"For the image service to create the iso it needs some information from the assisted service. In the minimal ISO case, it will need the ignition and, if the user has configured static networking, an additional ramdisk image. For the full ISO, only the ignition is needed. Today the ignition is created and uploaded separately to S3 by the assisted service when the discovery image is generated. The ISO can then be downloaded using a presigned S3 URL and the ignition is made available for download through the /clusters/{cluster_id}/downloads/files endpoint. This proposal involves changing that behavior. As mentioned above, the ISO will be downloaded through the image service. The ignition will no longer be uploaded to storage, but will be rendered and served by the assisted service on-demand. An additional API for the minimal ISO ramdisk will behave in the same way.","title":"Assisted Service API Changes"},{"location":"enhancements/image-handling-service/#upgrade-considerations","text":"For the operator-managed deployment, the new version of the operator will deploy the image service alongside assisted service and configure both to be properly aware of the other as well as alter the route to push download traffic to the image service. On the first InfraEnv controller reconcile, all the iso download URLs will be updated to reference the image service path, this change will then propagate to the BMH through the Bare Metal Agent controller. Any active agents will be restarted and will boot from the new image created by the image service. For the cloud, the existing images will continue to exist through direct presigned S3 links, but as this proposal removes the need for a \"generate image\" step, we will need to update those saved download URLs using a migration.","title":"Upgrade Considerations"},{"location":"enhancements/image-handling-service/#risks-and-mitigations","text":"Relying strictly on streaming will increase the load on the image service in the SaaS deployment as presigned links directly to S3 would no longer be provided. Downloading the image through the service is already required when running on a local cluster so this shouldn't be a significant problem, but should be load tested. This will be further mitigated by splitting the service as many downloads won't affect the other operations of assisted service.","title":"Risks and Mitigations"},{"location":"enhancements/image-handling-service/#design-details-optional","text":"","title":"Design Details [optional]"},{"location":"enhancements/image-handling-service/#open-questions","text":"","title":"Open Questions"},{"location":"enhancements/image-handling-service/#ui-impact","text":"Allowing only streaming would mean that the discovery image download modal would need to change to remove the presigned S3 URL and instead point users to the service directly. Any existing workarounds regarding time between generating images can be removed when images are no longer being generated in the background.","title":"UI Impact"},{"location":"enhancements/image-handling-service/#test-plan","text":"The image service can, and should, be tested in isolation using a mock assisted service API which will return some image info. As mentioned previously, load testing will also be needed on the SaaS deployment to ensure it can handle whatever peak image download load is expected.","title":"Test Plan"},{"location":"enhancements/image-handling-service/#drawbacks","text":"Maintaining and deploying an entirely separate service is complicated. No longer saving the image makes any future requirement to serve directly to a BMC much more difficult to implement. Adds traffic and load to the SaaS by removing images from S3.","title":"Drawbacks"},{"location":"enhancements/image-handling-service/#alternatives","text":"","title":"Alternatives"},{"location":"enhancements/image-handling-service/#streaming-vs-storing","text":"This proposal could be amended to handle either streaming or storing an image, but it would complicate authentication and authorization. In the current proposal every call to the image service would require a call to assisted service to fetch customization information. If the image is generated and stored by the image service, the download call will not need a call to assisted service and would require some other means of authentication and authorization.","title":"Streaming vs Storing"},{"location":"enhancements/image-handling-service/#scaling-assisted-service-as-is","text":"Many of the goals of this proposal could be achieved by running multiple assisted service instances in parallel. One for handling image generation/download and another for serving all other requests. This behavior would to be dictated by special routing rules and feature gates. While possible, this would involve adding more logic and behavior switches to assisted service rather than simplifying the application. Additionally, assisted service is already difficult to scale in its current form. Each replica needs to take part in a leader election process to deal with database migration, downloading template images, and running controllers. Additionally, creating customized CoreOS ISOs is a fairly common task and splitting this into its own service and repository allows the logic to be as consumable as possible by other projects. Embedding this behavior into assisted service would limit collaboration and adoption by other teams.","title":"Scaling Assisted Service as-is"},{"location":"enhancements/image-service-cloud-authentication/","text":"Image Service Cloud Authentication Summary Authentication between the image service and assisted service when deployed in the cloud requires a new strategy because the existing tokens used in the cloud are too large and too short lived to be included in a pre-signed URL. Instead of the existing authentication used in the cloud, assisted service will issue JWTs using a symmetric key specific to each infraEnv. This token will be used as authentication for requests to assisted service, specifically for the endpoints required by the image service. Motivation When the assisted service is deployed in the cloud, it relies on Red Hat SSO for user authentication and authentication tokens from Red Hat SSO are not usable with pre-signed URLs for two main reasons: Size - these tokens are typically ~2000 characters long Expiration - these tokens have a maximum lifespan of 15 minutes Goals Define an authentication strategy suitable for pre-signed URLs in the cloud. This includes a token with a longer expiration (several hours) and a reasonable size (10s to 100s of characters) which will be included directly in a download URL Non-Goals Tie an image download URL to a Red Hat account or organization Maintain public/private key pairs as service configuration to generate and issue tokens Add additional external identity management components Proposal The new authentication mechanism will be a JWT signed by the assisted service using a randomly generated key stored with the infraEnv. The JWT will include an exp claim as defined in RFC7519 and a sub claim containing the infraEnv ID. A token will be included as a parameter in the image download URL and assisted service authentication will pass if the token in the URL is not expired and validates using the key in the infraEnv record. Managing the signed URL and key will require a few API changes. The REST API will no longer return a download_url as a part of an infraEnv as the token in the URL could be expired at the time of the call. A new API will be added to fetch pre-signed URL which will include a new token. Another new API will be added to force a new key to be generated. This will allow a user to invalidate all existing non-expired tokens in case a download URL is leaked. User Stories Story 1 As a user of assisted service via console.redhat.com, I want a simple download URL to access the discovery ISO. This URL should be usable with standard download utilities (i.e. curl , wget ) without the need for request headers or additional tools like the ocm CLI. Story 2 As a user of assisted service via console.redhat.com, I want my sensitive credentials and infrastructure information contained within the discovery ISO to be kept secure. Story 3 As a user of assisted service via console.redhat.com, I want the discovery ISO download URL to be usable by anyone I choose to give it to without disclosing my personal Red Hat account credentials or API tokens. Implementation Details/Notes/Constraints [optional] Assisted service will implement a new security definition to serve this purpose. The existing definitions already have well-defined use cases and scopes that do not fully match the requirements of this proposal. Specifically the existing urlAuth security definition applies to more endpoints than we would want to expose for this enhancement. This new security definition will apply only to the following endpoints: /v2/infra-envs/{infra_env_id}/downloads/files /v2/infra-envs/{infra_env_id}/downloads/minimal-initrd These are the endpoints that the image service uses to fetch image customization information. The symmetric key will be stored in a new column in the infra_envs database table and will not be accessible through the infraEnv API. The expiration time for tokens will be 4 hours by default (the same as the current image expiration time), but will also continue to be configurable. New API endpoints: GET /v2/infra-envs/{infra_env_id}/downloads/image-url Get a new pre-signed download URL for an infraEnv's discovery image Example response: {\"url\": \"https://image-service.example.com/images/{infra_env_id}?image_token=<some-jwt>&version=4.9&arch=x86_64&type=full-iso\", \"exp\": \"1634571105\"} POST /v2/infra-envs/{infra_env_id}/regenerate-signing-key Create a new symmetric key for an infraEnv (invalidates all existing JWTs) These new endpoints and will be protected by SSO user credential authentication and authorization. The image service will accept a new URL parameter, image_token , which will then be forwarded to assisted service in the Image-Token header key. Assisted Service Authentication and Authorization Flow Request arrives to one of the infraEnv download endpoints with an Image-Token header Assisted service middleware stores infraEnv ID from the request path in the request context Authentication validates the token using the key associated with the infraEnv for the id in the token sub claim If a token is valid, authentication stores the sub claim in the request context Authorization ensures the infraEnv ID from the sub claim in the request context matches the infraEnv ID in the request path (also from the context) Request is processed Risks and Mitigations Accessing a database during authentication has a performance impact, but we should be able to cache keys by infraEnv id which would mitigate most of this. Additionally we already check the database for authorization and the added time to fetch the key should be small compared to the total download time so it likely won't be something users notice. Introducing a new authentication system always involves some risk that it will be implemented incorrectly, but this addition should be easy enough to understand and address any issues that could come up. Design Details [optional] Open Questions Should we also support RHSSO user tokens in a header for these endpoints? The image service could also pass a header through the assisted service UI Impact The UI is currently reading the image download URL from the infraEnv. This will need to change to request the URL separately after the infraEnv is created. Test Plan Image downloads from the cloud with the image service and the new authentication work correctly. An image can not be downloaded without a token or with an invalid token. The image-url endpoint returns a URL with a new token with a new expiration claim. The regenerate signing key endpoint invalidates all existing tokens (requests no longer succeed with any previously generated URL). URLs with an existing token expire after 4 hours Drawbacks Implementing our own authentication system is generally risky. We already have a system (RHSSO) that works in the cloud, gives us authorization as well, and we don't maintain. The only reason we can't use this system for this use case is that we want to make the download easier for users. The most secure, and most flexible option would be to require an OCM JWT token for this download, just as we do for every other API call. Alternatives Use RHSSO and a token in the header as we do for all other API requests Discussed in \"Drawbacks\", but this would be much less work and one less authentication system to maintain. Downsides are UX","title":"image-service-cloud-authentication"},{"location":"enhancements/image-service-cloud-authentication/#image-service-cloud-authentication","text":"","title":"Image Service Cloud Authentication"},{"location":"enhancements/image-service-cloud-authentication/#summary","text":"Authentication between the image service and assisted service when deployed in the cloud requires a new strategy because the existing tokens used in the cloud are too large and too short lived to be included in a pre-signed URL. Instead of the existing authentication used in the cloud, assisted service will issue JWTs using a symmetric key specific to each infraEnv. This token will be used as authentication for requests to assisted service, specifically for the endpoints required by the image service.","title":"Summary"},{"location":"enhancements/image-service-cloud-authentication/#motivation","text":"When the assisted service is deployed in the cloud, it relies on Red Hat SSO for user authentication and authentication tokens from Red Hat SSO are not usable with pre-signed URLs for two main reasons: Size - these tokens are typically ~2000 characters long Expiration - these tokens have a maximum lifespan of 15 minutes","title":"Motivation"},{"location":"enhancements/image-service-cloud-authentication/#goals","text":"Define an authentication strategy suitable for pre-signed URLs in the cloud. This includes a token with a longer expiration (several hours) and a reasonable size (10s to 100s of characters) which will be included directly in a download URL","title":"Goals"},{"location":"enhancements/image-service-cloud-authentication/#non-goals","text":"Tie an image download URL to a Red Hat account or organization Maintain public/private key pairs as service configuration to generate and issue tokens Add additional external identity management components","title":"Non-Goals"},{"location":"enhancements/image-service-cloud-authentication/#proposal","text":"The new authentication mechanism will be a JWT signed by the assisted service using a randomly generated key stored with the infraEnv. The JWT will include an exp claim as defined in RFC7519 and a sub claim containing the infraEnv ID. A token will be included as a parameter in the image download URL and assisted service authentication will pass if the token in the URL is not expired and validates using the key in the infraEnv record. Managing the signed URL and key will require a few API changes. The REST API will no longer return a download_url as a part of an infraEnv as the token in the URL could be expired at the time of the call. A new API will be added to fetch pre-signed URL which will include a new token. Another new API will be added to force a new key to be generated. This will allow a user to invalidate all existing non-expired tokens in case a download URL is leaked.","title":"Proposal"},{"location":"enhancements/image-service-cloud-authentication/#user-stories","text":"","title":"User Stories"},{"location":"enhancements/image-service-cloud-authentication/#story-1","text":"As a user of assisted service via console.redhat.com, I want a simple download URL to access the discovery ISO. This URL should be usable with standard download utilities (i.e. curl , wget ) without the need for request headers or additional tools like the ocm CLI.","title":"Story 1"},{"location":"enhancements/image-service-cloud-authentication/#story-2","text":"As a user of assisted service via console.redhat.com, I want my sensitive credentials and infrastructure information contained within the discovery ISO to be kept secure.","title":"Story 2"},{"location":"enhancements/image-service-cloud-authentication/#story-3","text":"As a user of assisted service via console.redhat.com, I want the discovery ISO download URL to be usable by anyone I choose to give it to without disclosing my personal Red Hat account credentials or API tokens.","title":"Story 3"},{"location":"enhancements/image-service-cloud-authentication/#implementation-detailsnotesconstraints-optional","text":"Assisted service will implement a new security definition to serve this purpose. The existing definitions already have well-defined use cases and scopes that do not fully match the requirements of this proposal. Specifically the existing urlAuth security definition applies to more endpoints than we would want to expose for this enhancement. This new security definition will apply only to the following endpoints: /v2/infra-envs/{infra_env_id}/downloads/files /v2/infra-envs/{infra_env_id}/downloads/minimal-initrd These are the endpoints that the image service uses to fetch image customization information. The symmetric key will be stored in a new column in the infra_envs database table and will not be accessible through the infraEnv API. The expiration time for tokens will be 4 hours by default (the same as the current image expiration time), but will also continue to be configurable. New API endpoints: GET /v2/infra-envs/{infra_env_id}/downloads/image-url Get a new pre-signed download URL for an infraEnv's discovery image Example response: {\"url\": \"https://image-service.example.com/images/{infra_env_id}?image_token=<some-jwt>&version=4.9&arch=x86_64&type=full-iso\", \"exp\": \"1634571105\"} POST /v2/infra-envs/{infra_env_id}/regenerate-signing-key Create a new symmetric key for an infraEnv (invalidates all existing JWTs) These new endpoints and will be protected by SSO user credential authentication and authorization. The image service will accept a new URL parameter, image_token , which will then be forwarded to assisted service in the Image-Token header key.","title":"Implementation Details/Notes/Constraints [optional]"},{"location":"enhancements/image-service-cloud-authentication/#assisted-service-authentication-and-authorization-flow","text":"Request arrives to one of the infraEnv download endpoints with an Image-Token header Assisted service middleware stores infraEnv ID from the request path in the request context Authentication validates the token using the key associated with the infraEnv for the id in the token sub claim If a token is valid, authentication stores the sub claim in the request context Authorization ensures the infraEnv ID from the sub claim in the request context matches the infraEnv ID in the request path (also from the context) Request is processed","title":"Assisted Service Authentication and Authorization Flow"},{"location":"enhancements/image-service-cloud-authentication/#risks-and-mitigations","text":"Accessing a database during authentication has a performance impact, but we should be able to cache keys by infraEnv id which would mitigate most of this. Additionally we already check the database for authorization and the added time to fetch the key should be small compared to the total download time so it likely won't be something users notice. Introducing a new authentication system always involves some risk that it will be implemented incorrectly, but this addition should be easy enough to understand and address any issues that could come up.","title":"Risks and Mitigations"},{"location":"enhancements/image-service-cloud-authentication/#design-details-optional","text":"","title":"Design Details [optional]"},{"location":"enhancements/image-service-cloud-authentication/#open-questions","text":"Should we also support RHSSO user tokens in a header for these endpoints? The image service could also pass a header through the assisted service","title":"Open Questions"},{"location":"enhancements/image-service-cloud-authentication/#ui-impact","text":"The UI is currently reading the image download URL from the infraEnv. This will need to change to request the URL separately after the infraEnv is created.","title":"UI Impact"},{"location":"enhancements/image-service-cloud-authentication/#test-plan","text":"Image downloads from the cloud with the image service and the new authentication work correctly. An image can not be downloaded without a token or with an invalid token. The image-url endpoint returns a URL with a new token with a new expiration claim. The regenerate signing key endpoint invalidates all existing tokens (requests no longer succeed with any previously generated URL). URLs with an existing token expire after 4 hours","title":"Test Plan"},{"location":"enhancements/image-service-cloud-authentication/#drawbacks","text":"Implementing our own authentication system is generally risky. We already have a system (RHSSO) that works in the cloud, gives us authorization as well, and we don't maintain. The only reason we can't use this system for this use case is that we want to make the download easier for users. The most secure, and most flexible option would be to require an OCM JWT token for this download, just as we do for every other API call.","title":"Drawbacks"},{"location":"enhancements/image-service-cloud-authentication/#alternatives","text":"Use RHSSO and a token in the header as we do for all other API requests Discussed in \"Drawbacks\", but this would be much less work and one less authentication system to maintain. Downsides are UX","title":"Alternatives"},{"location":"enhancements/installation-progress-bar/","text":"Cluster installation progress bar Goals Progress should be computed in the BE by design and not in the UI that will also make it usable from other APIs as kube-api Installed clusters with failed non-essential workers or OLM-operators should still reach 100% progress. Current implementation Computed in UI progressPercent = hostsProgressPercent * 0.75 + operatorsProgressPercent * 0.25 operatorsProgressPercent =(completedOperators / monitoredOperators) * 100 A completedOperator is one of the following: A built-in operator with available status An OLM operator with available or failed status hostsProgressPercent = (\u03a3completedStages / \u03a3allStages) * 100 For each host the data is retrieved by using the following APIs GET api/assisted-install/v1/clusters/{cluster_id} | jq '.hosts[0].progress' { \"current_stage\": \"Done\", \"stage_started_at\": \"2021-05-31T13:37:26.224Z\", \"stage_updated_at\": \"2021-05-31T13:37:26.224Z\" } GET /api/assisted-install/v1/clusters/{cluster_id} | jq '.hosts[0].progress_stages' [ \"Starting installation\", \"Installing\", \"Writing image to disk\", \"Waiting for control plane\", \"Rebooting\", \"Configuring\", \"Joined\", \"Done\" ] New implementation Will be computed in BE progres =\u03a3clusterStageProgress * Wi = preparingForInstallation * Wpi + installing * Wi + finalizing * Wf while \u03a3Wi=1 We can manually compute an avg duration for the previous installations based on events to choose those weights and hardcode them. Preparing for installation This stage mostly generates manifests and installation configs, therefore, we will set preparingForInstallationProgress={1, 0} depending on if it is done or not. Installing This stage refers to hosts' installation We will \u201cassume\u201d all host\u2019s stages have a similar duration, therfore, they will be equally weighted. Hosts' progresses will be computed the same way we it is done in the current implementation, aka hostsProgressPercent = \u03a3completedStages / \u03a3allStages Finalizing This stage refers to operators' installation We will \u201cassume\u201d all operatos have a similar installation duration. Operators' progresses will be computed the same way we it is done in the current implementation, aka operatorsProgressPercent = completedOperators / monitoredOperators Suggested APIs: * Note that those new percentage values should be reset on clusters' installation reset diff --git a/swagger.yaml b/swagger.yaml index d53ddf59..ddbc7e9b 100644 --- a/swagger.yaml +++ b/swagger.yaml @@ -4171,6 +4171,9 @@ definitions: format: date-time x-go-custom-tag: gorm:\"type:timestamp with time zone\" description: The last time that the cluster status was updated. + progress: + type: object + ref: '#/definitions/cluster-progress-info' hosts: x-go-custom-tag: gorm:\"foreignkey:ClusterID;association_foreignkey:ID\" type: array @@ -4458,8 +4461,12 @@ definitions: host-progress-info: type: object required: + - installation_percentage - current_stage properties: + installation_percentage: + type: integer current_stage: type: string $ref: '#/definitions/host-stage' @@ -4477,6 +4484,21 @@ definitions: x-go-custom-tag: gorm:\"type:timestamp with time zone\" description: Time at which the current progress stage was last updated. + cluster-progress-info: + type: object + required: + - total_percentage + properties: + total_percentage: + type: integer + preparing_for_installation_stage_percentage: + type: integer + installing_stage_percentage: + type: integer + finalizing_stage_percentage: + type: integer + host-stage: type: string enum: GET /api/assisted-install/v1/clusters/{cluster_id} { \"id\": \"3a745598-dde4-46dd-bdb1-ce7e1dea119b\", \"progress\": <-------- NEW { \"total_percentage\": 21, \"preparing_for_installation_stage_percentage\": 100, \"installing_stage_percentage\": 38, \"finalizing_stage_percentage\": 0, } \"hosts\": [ { \"kind\": \"Host\", \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", \"role\": \"master\", \"progress\": { \"installation_percentage\": 15, <-------- NEW (but coming from hosts API) \"current_stage\": \"Starting installation\", }, }, { \"kind\": \"Host\", \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa7\", \"role\": \"worker\", \"progress\": { \"installation_percentage\": 13, <-------- NEW (but coming from hosts API) \"current_stage\": \"Starting installation\", }, }, ] } GET /api/assisted-install/v1/clusters/{cluster_id}/hosts/{host_id} { \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", \"progress\": { \"installation_percentage\": 15, <-------- NEW \"current_stage\": \"Starting installation\", \"progress_info\": \"string\", }, }","title":"Cluster installation progress bar"},{"location":"enhancements/installation-progress-bar/#cluster-installation-progress-bar","text":"","title":"Cluster installation progress bar"},{"location":"enhancements/installation-progress-bar/#goals","text":"Progress should be computed in the BE by design and not in the UI that will also make it usable from other APIs as kube-api Installed clusters with failed non-essential workers or OLM-operators should still reach 100% progress.","title":"Goals"},{"location":"enhancements/installation-progress-bar/#current-implementation","text":"Computed in UI progressPercent = hostsProgressPercent * 0.75 + operatorsProgressPercent * 0.25 operatorsProgressPercent =(completedOperators / monitoredOperators) * 100 A completedOperator is one of the following: A built-in operator with available status An OLM operator with available or failed status hostsProgressPercent = (\u03a3completedStages / \u03a3allStages) * 100 For each host the data is retrieved by using the following APIs GET api/assisted-install/v1/clusters/{cluster_id} | jq '.hosts[0].progress' { \"current_stage\": \"Done\", \"stage_started_at\": \"2021-05-31T13:37:26.224Z\", \"stage_updated_at\": \"2021-05-31T13:37:26.224Z\" } GET /api/assisted-install/v1/clusters/{cluster_id} | jq '.hosts[0].progress_stages' [ \"Starting installation\", \"Installing\", \"Writing image to disk\", \"Waiting for control plane\", \"Rebooting\", \"Configuring\", \"Joined\", \"Done\" ]","title":"Current implementation"},{"location":"enhancements/installation-progress-bar/#new-implementation","text":"Will be computed in BE progres =\u03a3clusterStageProgress * Wi = preparingForInstallation * Wpi + installing * Wi + finalizing * Wf while \u03a3Wi=1 We can manually compute an avg duration for the previous installations based on events to choose those weights and hardcode them.","title":"New implementation"},{"location":"enhancements/installation-progress-bar/#preparing-for-installation","text":"This stage mostly generates manifests and installation configs, therefore, we will set preparingForInstallationProgress={1, 0} depending on if it is done or not.","title":"Preparing for installation"},{"location":"enhancements/installation-progress-bar/#installing","text":"This stage refers to hosts' installation We will \u201cassume\u201d all host\u2019s stages have a similar duration, therfore, they will be equally weighted. Hosts' progresses will be computed the same way we it is done in the current implementation, aka hostsProgressPercent = \u03a3completedStages / \u03a3allStages","title":"Installing"},{"location":"enhancements/installation-progress-bar/#finalizing","text":"This stage refers to operators' installation We will \u201cassume\u201d all operatos have a similar installation duration. Operators' progresses will be computed the same way we it is done in the current implementation, aka operatorsProgressPercent = completedOperators / monitoredOperators Suggested APIs: * Note that those new percentage values should be reset on clusters' installation reset diff --git a/swagger.yaml b/swagger.yaml index d53ddf59..ddbc7e9b 100644 --- a/swagger.yaml +++ b/swagger.yaml @@ -4171,6 +4171,9 @@ definitions: format: date-time x-go-custom-tag: gorm:\"type:timestamp with time zone\" description: The last time that the cluster status was updated. + progress: + type: object + ref: '#/definitions/cluster-progress-info' hosts: x-go-custom-tag: gorm:\"foreignkey:ClusterID;association_foreignkey:ID\" type: array @@ -4458,8 +4461,12 @@ definitions: host-progress-info: type: object required: + - installation_percentage - current_stage properties: + installation_percentage: + type: integer current_stage: type: string $ref: '#/definitions/host-stage' @@ -4477,6 +4484,21 @@ definitions: x-go-custom-tag: gorm:\"type:timestamp with time zone\" description: Time at which the current progress stage was last updated. + cluster-progress-info: + type: object + required: + - total_percentage + properties: + total_percentage: + type: integer + preparing_for_installation_stage_percentage: + type: integer + installing_stage_percentage: + type: integer + finalizing_stage_percentage: + type: integer + host-stage: type: string enum: GET /api/assisted-install/v1/clusters/{cluster_id} { \"id\": \"3a745598-dde4-46dd-bdb1-ce7e1dea119b\", \"progress\": <-------- NEW { \"total_percentage\": 21, \"preparing_for_installation_stage_percentage\": 100, \"installing_stage_percentage\": 38, \"finalizing_stage_percentage\": 0, } \"hosts\": [ { \"kind\": \"Host\", \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", \"role\": \"master\", \"progress\": { \"installation_percentage\": 15, <-------- NEW (but coming from hosts API) \"current_stage\": \"Starting installation\", }, }, { \"kind\": \"Host\", \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa7\", \"role\": \"worker\", \"progress\": { \"installation_percentage\": 13, <-------- NEW (but coming from hosts API) \"current_stage\": \"Starting installation\", }, }, ] } GET /api/assisted-install/v1/clusters/{cluster_id}/hosts/{host_id} { \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", \"progress\": { \"installation_percentage\": 15, <-------- NEW \"current_stage\": \"Starting installation\", \"progress_info\": \"string\", }, }","title":"Finalizing"},{"location":"enhancements/template/","text":"Neat Enhancement Idea This is the title of the enhancement. Keep it simple and descriptive. A good title can help communicate what the enhancement is and should be considered as part of any review. The YAML title should be lowercased and spaces/punctuation should be replaced with - . To get started with this template: 1. Make a copy of this template. Copy this template. 1. Fill out the \"overview\" sections. This includes the Summary and Motivation sections. These should be easy and explain why the community should desire this enhancement. 1. Create a PR. Assign it to folks with expertise in that domain to help sponsor the process. The Metadata section above is intended to support the creation of tooling around the enhancement process. Summary The Summary section is incredibly important for producing high quality user-focused documentation such as release notes or a development roadmap. It should be possible to collect this information before implementation begins in order to avoid requiring implementors to split their attention between writing release notes and implementing the feature itself. A good summary is probably at least a paragraph in length. Motivation This section is for explicitly listing the motivation, goals and non-goals of this proposal. Describe why the change is important and the benefits to users. Goals List the specific goals of the proposal. How will we know that this has succeeded? Non-Goals What is out of scope for this proposal? Listing non-goals helps to focus discussion and make progress. Proposal This is where we get down to the nitty gritty of what the proposal actually is. User Stories Detail the things that people will be able to do if this is implemented. Include as much detail as possible so that people can understand the \"how\" of the system. The goal here is to make this feel real for users without getting bogged down. Include a story on how this proposal will be deployed in production: lifecycle, monitoring and scale requirements or benefits. Story 1 Story 2 Implementation Details/Notes/Constraints [optional] What are the caveats to the implementation? What are some important details that didn't come across above. Go in to as much detail as necessary here. This might be a good place to talk about core concepts and how they relate. This is an excellent place to call out changes that need to be made in projects other than openshift/assisted-service. For example, if a change will need to be made in the agent (openshift/assisted-installer-agent) and the installer (openshift/assisted-installer; it should be mentioned here. Risks and Mitigations What are the risks of this proposal and how do we mitigate. Think broadly. For example, consider both security and how this will impact the larger OKD ecosystem. Will choices made here affect adoption of assisted-installer? Will this work make it harder to integrate with other upstream projects? How will security be reviewed and by whom? How will UX be reviewed and by whom? Consider including folks that also work outside your immediate sub-project. Design Details [optional] If an enhancement is complex enough, design details should be included. When not included, reviewers reserve the right to ask for this section to be filled in to enable more thoughtful discussion about the enhancement and it's impact. Open Questions This is where to call out areas of the design that require closure before deciding to implement the design. For instance, This requires exposing previously private resources which contain sensitive information. Can we do this? UI Impact No need to go into great detail about the UI changes that will need to be made. However, this is an excellent time to mention 1) if UI changes are required if this enhancement were accepted and 2) at a high-level what those UI changes would be. Test Plan Consider the following in developing a test plan for this enhancement: - Will there be e2e and integration tests, in addition to unit tests? - How will it be tested in isolation vs with other components? No need to outline all of the test cases, just the general strategy. Anything that would count as tricky in the implementation and anything particularly challenging to test should be called out. Drawbacks The idea is to find the best form of an argument why this enhancement should not be implemented. Alternatives Similar to the Drawbacks section the Alternatives section is used to highlight and record other possible approaches to delivering the value proposed by an enhancement.","title":"neat-enhancement-idea"},{"location":"enhancements/template/#neat-enhancement-idea","text":"This is the title of the enhancement. Keep it simple and descriptive. A good title can help communicate what the enhancement is and should be considered as part of any review. The YAML title should be lowercased and spaces/punctuation should be replaced with - . To get started with this template: 1. Make a copy of this template. Copy this template. 1. Fill out the \"overview\" sections. This includes the Summary and Motivation sections. These should be easy and explain why the community should desire this enhancement. 1. Create a PR. Assign it to folks with expertise in that domain to help sponsor the process. The Metadata section above is intended to support the creation of tooling around the enhancement process.","title":"Neat Enhancement Idea"},{"location":"enhancements/template/#summary","text":"The Summary section is incredibly important for producing high quality user-focused documentation such as release notes or a development roadmap. It should be possible to collect this information before implementation begins in order to avoid requiring implementors to split their attention between writing release notes and implementing the feature itself. A good summary is probably at least a paragraph in length.","title":"Summary"},{"location":"enhancements/template/#motivation","text":"This section is for explicitly listing the motivation, goals and non-goals of this proposal. Describe why the change is important and the benefits to users.","title":"Motivation"},{"location":"enhancements/template/#goals","text":"List the specific goals of the proposal. How will we know that this has succeeded?","title":"Goals"},{"location":"enhancements/template/#non-goals","text":"What is out of scope for this proposal? Listing non-goals helps to focus discussion and make progress.","title":"Non-Goals"},{"location":"enhancements/template/#proposal","text":"This is where we get down to the nitty gritty of what the proposal actually is.","title":"Proposal"},{"location":"enhancements/template/#user-stories","text":"Detail the things that people will be able to do if this is implemented. Include as much detail as possible so that people can understand the \"how\" of the system. The goal here is to make this feel real for users without getting bogged down. Include a story on how this proposal will be deployed in production: lifecycle, monitoring and scale requirements or benefits.","title":"User Stories"},{"location":"enhancements/template/#story-1","text":"","title":"Story 1"},{"location":"enhancements/template/#story-2","text":"","title":"Story 2"},{"location":"enhancements/template/#implementation-detailsnotesconstraints-optional","text":"What are the caveats to the implementation? What are some important details that didn't come across above. Go in to as much detail as necessary here. This might be a good place to talk about core concepts and how they relate. This is an excellent place to call out changes that need to be made in projects other than openshift/assisted-service. For example, if a change will need to be made in the agent (openshift/assisted-installer-agent) and the installer (openshift/assisted-installer; it should be mentioned here.","title":"Implementation Details/Notes/Constraints [optional]"},{"location":"enhancements/template/#risks-and-mitigations","text":"What are the risks of this proposal and how do we mitigate. Think broadly. For example, consider both security and how this will impact the larger OKD ecosystem. Will choices made here affect adoption of assisted-installer? Will this work make it harder to integrate with other upstream projects? How will security be reviewed and by whom? How will UX be reviewed and by whom? Consider including folks that also work outside your immediate sub-project.","title":"Risks and Mitigations"},{"location":"enhancements/template/#design-details-optional","text":"If an enhancement is complex enough, design details should be included. When not included, reviewers reserve the right to ask for this section to be filled in to enable more thoughtful discussion about the enhancement and it's impact.","title":"Design Details [optional]"},{"location":"enhancements/template/#open-questions","text":"This is where to call out areas of the design that require closure before deciding to implement the design. For instance, This requires exposing previously private resources which contain sensitive information. Can we do this?","title":"Open Questions"},{"location":"enhancements/template/#ui-impact","text":"No need to go into great detail about the UI changes that will need to be made. However, this is an excellent time to mention 1) if UI changes are required if this enhancement were accepted and 2) at a high-level what those UI changes would be.","title":"UI Impact"},{"location":"enhancements/template/#test-plan","text":"Consider the following in developing a test plan for this enhancement: - Will there be e2e and integration tests, in addition to unit tests? - How will it be tested in isolation vs with other components? No need to outline all of the test cases, just the general strategy. Anything that would count as tricky in the implementation and anything particularly challenging to test should be called out.","title":"Test Plan"},{"location":"enhancements/template/#drawbacks","text":"The idea is to find the best form of an argument why this enhancement should not be implemented.","title":"Drawbacks"},{"location":"enhancements/template/#alternatives","text":"Similar to the Drawbacks section the Alternatives section is used to highlight and record other possible approaches to delivering the value proposed by an enhancement.","title":"Alternatives"},{"location":"hive-integration/","text":"Hive Integration The goal of the Hive integration is to enable Assisted Installer capabilities on-premise in users' \"Hub\" clusters by installing clusters via Multi-cluster management, such as through Hive and RHACM (Red Hat Advanced Cluster Management). A full description of the enhancement is available here . For this integration, the Assisted Installer APIs are available via CRDs CRD Types ClusterDeployment The ClusterDeployment CRD is an API provided by Hive. See Hive documentation here . The ClusterDeployment can have a reference to an AgentClusterInstall ( Spec.ClusterInstallRef ) that defines the required parameters of the Cluster. Deletion of ClusterDeployment will trigger the clusterdeployments.agent-install.openshift.io/ai-deprovision finalizer pre-deletion logic, which will delete the referenced AgentClusterInstall. AgentClusterInstall In the AgentClusterInstall, the user can specify requirements like Networking, number of Control Plane and Worker nodes and more. The installation will start automatically if the required number of hosts is available, the hosts are ready to be installed and the Agents are approved. Once the installation started, changes to the AgentClusterInstall Spec will be revoked. Selecting a specific OCP release version is done using a ClusterImageSet, see documentation here . The AgentClusterInstall reflects the Cluster/Installation status through Conditions. Deletion of AgentClusterInstall will trigger the agentclusterinstall .agent-install.openshift.io/ai-deprovision finalizer pre-deletion logic, which will deregister the backend cluster and delete all the related Agent CRs. Here an example how to print AgentClusterInstall conditions: $ kubectl get agentclusterinstalls.extensions.hive.openshift.io -n mynamespace -o=jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\"\\n\"}{range .status.conditions[*]}{.type}{\"\\t\"}{.message}{\"\\n\"}{end}' test-infra-agent-cluster-install SpecSynced The Spec has been successfully applied Validated The cluster's validations are passing RequirementsMet The cluster installation stopped Completed The installation has completed: Cluster is installed Failed The installation has not failed Stopped The installation has stopped because it completed successfully More details on conditions is available here Debug Information The DebugInfo field under Status provides additional information for debugging installation process: - EventsURL specifies an HTTP/S URL that contains events occured during cluster installation process InfraEnv The InfraEnv CRD represents the configuration needed to create the discovery ISO. The user can specify proxy settings, ignition overrides and specify NMState labels. When the ISO is ready, an URL will be available in the CR. The InfraEnv reflects the image creation status through Conditions. More details on conditions is available here The InfraEnv can be created without a Cluster Deployment reference for late binding flow. More information is available here . NMStateConfig The NMStateConfig contains network configuration that will applied on the hosts. See NMState repository here . To link between an InfraEnv to NMState (either one or more): InfraEnv CR: add a label to nmStateConfigLabelSelector with a user defined name and value. NMState CR: Specify the same label + value in Object metadata. Upon InfraEnv creation, the InfraEnv controller will search by label+value for matching NMState resources and construct a config to be sent as StaticNetworkConfig as a part of ImageCreateParams. The backend does all validations, and currently, there is no handling of configuration conflicts (e.g., two nmstate resources using the same MAC address). The InfraEnv controller will watch for NMState config creation/changes and search for corresponding InfraEnv resources to reconcile since we need to regenerate the image for those. :warning: It is advised to create all NMStateConfigs resources before their corresponding InfraEnv. The reason is that InfraEnv doesn't have a way to know how many NMStateConfigs to expect; therefore, it re-creates its ISO when new NMStateConfigs are found. The new ISO automatically propagates to any agents that haven't yet started installing. Agent The Agent CRD represents a Host that boot from an ISO and registered to a cluster. It will be created by Assisted Service when a host registers. In the Agent, the user can specify the hostname, role, installation disk and more. Also, the host hardware inventory and statuses are available. Note that if the Agent is not Approved, it will not be part of the installation. Here how to approve an Agent: $ kubectl -n mynamespace patch agents.agent-install.openshift.io 120af504-d88e-46bd-bec2-b8b261db3b01 -p '{\"spec\":{\"approved\":true}}' --type merge The Agent reflects the Host status through Conditions. More details on conditions is available here Here an example how to print Agent conditions: $ kubectl get agents.agent-install.openshift.io -n mynamespace -o=jsonpath='{range .items[*]}{\"\\n\"}{.spec.clusterDeploymentName.name}{\"\\n\"}{.status.inventory.hostname}{\"\\n\"}{range .status.conditions[*]}{.type}{\"\\t\"}{.message}{\"\\n\"}{end}' test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-2 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Configuring test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-0 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Configuring test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-1 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Waiting for control plane Once the cluster is installed, the ClusterDeployment is set to Installed and secrets for kubeconfig and credentials are created and referenced in the AgentClusterInstall. Day 2 worker In case of none SNO deployment, after that the cluster is installed, the original cluster is deleted and a Day 2 cluster is created instead in the Assisted Service database. Additional nodes can be added by booting from the new generated ISO. Each additional host will start installation once the Agent is Approved and the Host is in known state. Note that the user needs to approved the additional nodes in the installed cluster. Bare Metal Operator Integration In case that the Bare Metal Operator is installed, the Baremetal Agent Controller will sync between the Agent CR and the matching BareMetalHost CR: Find the right pairs of BMH/Agent using their MAC addresses Set the Image.URL in the BMH copying it from the InfraEnv's status. Reconcile the Agent's spec by copying the following attributes from the BMH's annotations: Role: master/worker Hostname (optional for user to set) MachineConfigPool (optional for user to set) Reconcile the BareMetalHost hardware details by copying the Agent's inventory data to the BMH's hardwaredetails annotation. See BMAC documentation here . Working with mirror registry In case all of your images are in mirror registries, the service, discovery ISO, and installed nodes must be configured with the proper registries.conf and authentication certificate. To do so, see the Mirror Registry Configuration section here . Assisted Installer Kube API CR examples docs/hive-integration/crds stores working examples of various resources we spawn via kube-api in assisted-installer, for Hive integration. Those examples are here for reference. You will likely need to adapt those for your own needs. InfraEnv InfraEnv Late Binding NMState Config Hive PullSecret Secret Hive ClusterDeployment AgentClusterInstall AgentClusterInstall SNO Creating InstallConfig overrides In order to alter the default install config yaml used when running openshift-install create commands. More information about install-config overrides is available here In case of failure to apply the overrides the agentclusterinstall conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the clusterdeployment controller will update the install config yaml with the annotation value. Note that this configuration must be applied prior to starting the installation $ kubectl annotate agentclusterinstalls.extensions.hive.openshift.io test-cluster -n mynamespace agent-install.openshift.io/install-config-overrides=\"{\\\"networking\\\":{\\\"networkType\\\": \\\"OVNKubernetes\\\"},\\\"fips\\\":true}\" agentclusterinstalls.extensions.hive.openshift.io/test-cluster annotated $ kubectl get agentclusterinstalls.extensions.hive.openshift.io test-cluster -n mynamespace -o yaml apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: annotations: agent-install.openshift.io/install-config-overrides: '{\"networking\":{\"networkType\": \"OVNKubernetes\"},\"fips\":true}' creationTimestamp: \"2021-04-01T07:04:49Z\" generation: 1 name: test-cluster namespace: mynamespace resourceVersion: \"183201\" ... Creating host installer args overrides In order to alter the default coreos-installer arguments used when running coreos-installer openshift-install create command. List of supported args can be found here In case of failure to apply the overrides the agent conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the bmac controller will update the agent spec with the annotation value. Then agent controller will forward it to host configuration. Note that this configuration must be applied prior to starting the installation $ kubectl annotate bmh openshift-worker-0 -n mynamespace bmac.agent-install.openshift.io/installer-args=\"[\\\"--append-karg\\\", \\\"ip=192.0.2.2::192.0.2.254:255.255.255.0:core0.example.com:enp1s0:none\\\", \\\"--save-partindex\\\", \\\"1\\\", \\\"-n\\\"]\" baremetalhost.metal3.io/openshift-worker-0 annotated $ oc get bmh openshift-worker-0 -n mynamespace -o yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: annotations: bmac.agent-install.openshift.io/installer-args: '[\"--append-karg\", \"ip=192.0.2.2::192.0.2.254:255.255.255.0:core0.example.com:enp1s0:none\", \"--save-partindex\", \"1\", \"-n\"]' creationTimestamp: \"2021-04-13T10:46:57Z\" generation: 1 name: openshift-worker-0 namespace: mynamespace spec: Creating host ignition config overrides In case of failure to apply the overrides, the agent conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the bmac controller will update the agent spec with the annotation value. Then agent controller will forward it to host configuration. Note that this configuration must be applied prior to starting the installation $ kubectl annotate bmh openshift-worker-0 -n mynamespace bmac.agent-install.openshift.io/ignition-config-overrides=\"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/tmp/example\\\", \\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,aGVscGltdHJhcHBlZGluYXN3YWdnZXJzcGVj\\\"}}]}}\" baremetalhost.metal3.io/openshift-worker-0 annotated $ oc get bmh openshift-worker-0 -n mynamespace -o yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: annotations: bmac.agent-install.openshift.io/ignition-config-overrides: '{\"ignition\": {\"version\": \"3.1.0\"}, \"storage\": {\"files\": [{\"path\": \"/tmp/example\", \"contents\": {\"source\": \"data:text/plain;base64,aGVscGltdHJhcHBlZGluYXN3YWdnZXJzcGVj\"}}]}}' creationTimestamp: \"2021-04-14T10:46:57Z\" generation: 1 name: openshift-worker-0 namespace: mynamespace spec: Creating Additional manifests In order to add custom manifests that will be added to the installation manifests generated by openshift-install create command, user will need to create configmap with valid manifests: kind: ConfigMap apiVersion: v1 metadata: name: my-baremetal-cluster-install-manifests namespace: mynamespace data: 99_master_kernel_arg.yaml: | apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 99-openshift-machineconfig-master-kargs spec: kernelArguments: - 'loglevel=7'` Create/update AgentClusterInstall with field manifestsConfigMapRef: apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: name: my-baremetal-cluster namespace: mynamespace spec: manifestsConfigMapRef: name: my-baremetal-cluster-install-manifests If manifests provided in configmap data section will be in bad format or configmap will not exists but will be referenced we will set error in Sync condition only if cluster will be ready for installation. Changing configmap should fix the issue. Teardown procedure Deleting the ClusterDeployment will automatically trigger the deletion of its referenced AgentClusterInstall and the deletion of all the Agents connected to it (Unless late binding was used, see here ). Note that the installed OCP cluster, if exists, will not be affected by the deletion of the ClusterDeployment. Deleting only the AgentClusterInstall will delete the Agents connected to it (Unless late binding was used), but the ClusterDeployment will remain. BareMetalHost, InfraEnv, ClusterImageSet and NMStateConfig deletion will not trigger deletion of other resources. In case that the assisted-service is not available, the deletion of ClusterDeployment, AgentClusterInstall and Agents resources will be blocked due to finalizers that are set on them. Here an example on how to remove finalizers on a resource: kubectl -n mynamespace patch agentclusterinstalls.extensions.hive.openshift.io my-aci -p '{\"metadata\":{\"finalizers\":null}}' --type=merge Development CRD update Changes in CRDs should be made in the CRDs Go files located here . After the changes are done, the yamls need to be generated by running: skipper make generate-all","title":"Hive Integration"},{"location":"hive-integration/#hive-integration","text":"The goal of the Hive integration is to enable Assisted Installer capabilities on-premise in users' \"Hub\" clusters by installing clusters via Multi-cluster management, such as through Hive and RHACM (Red Hat Advanced Cluster Management). A full description of the enhancement is available here . For this integration, the Assisted Installer APIs are available via CRDs","title":"Hive Integration"},{"location":"hive-integration/#crd-types","text":"","title":"CRD Types"},{"location":"hive-integration/#clusterdeployment","text":"The ClusterDeployment CRD is an API provided by Hive. See Hive documentation here . The ClusterDeployment can have a reference to an AgentClusterInstall ( Spec.ClusterInstallRef ) that defines the required parameters of the Cluster. Deletion of ClusterDeployment will trigger the clusterdeployments.agent-install.openshift.io/ai-deprovision finalizer pre-deletion logic, which will delete the referenced AgentClusterInstall.","title":"ClusterDeployment"},{"location":"hive-integration/#agentclusterinstall","text":"In the AgentClusterInstall, the user can specify requirements like Networking, number of Control Plane and Worker nodes and more. The installation will start automatically if the required number of hosts is available, the hosts are ready to be installed and the Agents are approved. Once the installation started, changes to the AgentClusterInstall Spec will be revoked. Selecting a specific OCP release version is done using a ClusterImageSet, see documentation here . The AgentClusterInstall reflects the Cluster/Installation status through Conditions. Deletion of AgentClusterInstall will trigger the agentclusterinstall .agent-install.openshift.io/ai-deprovision finalizer pre-deletion logic, which will deregister the backend cluster and delete all the related Agent CRs. Here an example how to print AgentClusterInstall conditions: $ kubectl get agentclusterinstalls.extensions.hive.openshift.io -n mynamespace -o=jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\"\\n\"}{range .status.conditions[*]}{.type}{\"\\t\"}{.message}{\"\\n\"}{end}' test-infra-agent-cluster-install SpecSynced The Spec has been successfully applied Validated The cluster's validations are passing RequirementsMet The cluster installation stopped Completed The installation has completed: Cluster is installed Failed The installation has not failed Stopped The installation has stopped because it completed successfully More details on conditions is available here","title":"AgentClusterInstall"},{"location":"hive-integration/#debug-information","text":"The DebugInfo field under Status provides additional information for debugging installation process: - EventsURL specifies an HTTP/S URL that contains events occured during cluster installation process","title":"Debug Information"},{"location":"hive-integration/#infraenv","text":"The InfraEnv CRD represents the configuration needed to create the discovery ISO. The user can specify proxy settings, ignition overrides and specify NMState labels. When the ISO is ready, an URL will be available in the CR. The InfraEnv reflects the image creation status through Conditions. More details on conditions is available here The InfraEnv can be created without a Cluster Deployment reference for late binding flow. More information is available here .","title":"InfraEnv"},{"location":"hive-integration/#nmstateconfig","text":"The NMStateConfig contains network configuration that will applied on the hosts. See NMState repository here . To link between an InfraEnv to NMState (either one or more): InfraEnv CR: add a label to nmStateConfigLabelSelector with a user defined name and value. NMState CR: Specify the same label + value in Object metadata. Upon InfraEnv creation, the InfraEnv controller will search by label+value for matching NMState resources and construct a config to be sent as StaticNetworkConfig as a part of ImageCreateParams. The backend does all validations, and currently, there is no handling of configuration conflicts (e.g., two nmstate resources using the same MAC address). The InfraEnv controller will watch for NMState config creation/changes and search for corresponding InfraEnv resources to reconcile since we need to regenerate the image for those. :warning: It is advised to create all NMStateConfigs resources before their corresponding InfraEnv. The reason is that InfraEnv doesn't have a way to know how many NMStateConfigs to expect; therefore, it re-creates its ISO when new NMStateConfigs are found. The new ISO automatically propagates to any agents that haven't yet started installing.","title":"NMStateConfig"},{"location":"hive-integration/#agent","text":"The Agent CRD represents a Host that boot from an ISO and registered to a cluster. It will be created by Assisted Service when a host registers. In the Agent, the user can specify the hostname, role, installation disk and more. Also, the host hardware inventory and statuses are available. Note that if the Agent is not Approved, it will not be part of the installation. Here how to approve an Agent: $ kubectl -n mynamespace patch agents.agent-install.openshift.io 120af504-d88e-46bd-bec2-b8b261db3b01 -p '{\"spec\":{\"approved\":true}}' --type merge The Agent reflects the Host status through Conditions. More details on conditions is available here Here an example how to print Agent conditions: $ kubectl get agents.agent-install.openshift.io -n mynamespace -o=jsonpath='{range .items[*]}{\"\\n\"}{.spec.clusterDeploymentName.name}{\"\\n\"}{.status.inventory.hostname}{\"\\n\"}{range .status.conditions[*]}{.type}{\"\\t\"}{.message}{\"\\n\"}{end}' test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-2 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Configuring test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-0 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Configuring test-infra-cluster-assisted-installer test-infra-cluster-assisted-installer-master-1 SpecSynced The Spec has been successfully applied Connected The agent's connection to the installation service is unimpaired RequirementsMet Installation already started and is in progress Validated The agent's validations are passing Installed The installation is in progress: Waiting for control plane Once the cluster is installed, the ClusterDeployment is set to Installed and secrets for kubeconfig and credentials are created and referenced in the AgentClusterInstall.","title":"Agent"},{"location":"hive-integration/#day-2-worker","text":"In case of none SNO deployment, after that the cluster is installed, the original cluster is deleted and a Day 2 cluster is created instead in the Assisted Service database. Additional nodes can be added by booting from the new generated ISO. Each additional host will start installation once the Agent is Approved and the Host is in known state. Note that the user needs to approved the additional nodes in the installed cluster.","title":"Day 2 worker"},{"location":"hive-integration/#bare-metal-operator-integration","text":"In case that the Bare Metal Operator is installed, the Baremetal Agent Controller will sync between the Agent CR and the matching BareMetalHost CR: Find the right pairs of BMH/Agent using their MAC addresses Set the Image.URL in the BMH copying it from the InfraEnv's status. Reconcile the Agent's spec by copying the following attributes from the BMH's annotations: Role: master/worker Hostname (optional for user to set) MachineConfigPool (optional for user to set) Reconcile the BareMetalHost hardware details by copying the Agent's inventory data to the BMH's hardwaredetails annotation. See BMAC documentation here .","title":"Bare Metal Operator Integration"},{"location":"hive-integration/#working-with-mirror-registry","text":"In case all of your images are in mirror registries, the service, discovery ISO, and installed nodes must be configured with the proper registries.conf and authentication certificate. To do so, see the Mirror Registry Configuration section here .","title":"Working with mirror registry"},{"location":"hive-integration/#assisted-installer-kube-api-cr-examples","text":"docs/hive-integration/crds stores working examples of various resources we spawn via kube-api in assisted-installer, for Hive integration. Those examples are here for reference. You will likely need to adapt those for your own needs. InfraEnv InfraEnv Late Binding NMState Config Hive PullSecret Secret Hive ClusterDeployment AgentClusterInstall AgentClusterInstall SNO","title":"Assisted Installer Kube API CR examples"},{"location":"hive-integration/#creating-installconfig-overrides","text":"In order to alter the default install config yaml used when running openshift-install create commands. More information about install-config overrides is available here In case of failure to apply the overrides the agentclusterinstall conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the clusterdeployment controller will update the install config yaml with the annotation value. Note that this configuration must be applied prior to starting the installation $ kubectl annotate agentclusterinstalls.extensions.hive.openshift.io test-cluster -n mynamespace agent-install.openshift.io/install-config-overrides=\"{\\\"networking\\\":{\\\"networkType\\\": \\\"OVNKubernetes\\\"},\\\"fips\\\":true}\" agentclusterinstalls.extensions.hive.openshift.io/test-cluster annotated $ kubectl get agentclusterinstalls.extensions.hive.openshift.io test-cluster -n mynamespace -o yaml apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: annotations: agent-install.openshift.io/install-config-overrides: '{\"networking\":{\"networkType\": \"OVNKubernetes\"},\"fips\":true}' creationTimestamp: \"2021-04-01T07:04:49Z\" generation: 1 name: test-cluster namespace: mynamespace resourceVersion: \"183201\" ...","title":"Creating InstallConfig overrides"},{"location":"hive-integration/#creating-host-installer-args-overrides","text":"In order to alter the default coreos-installer arguments used when running coreos-installer openshift-install create command. List of supported args can be found here In case of failure to apply the overrides the agent conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the bmac controller will update the agent spec with the annotation value. Then agent controller will forward it to host configuration. Note that this configuration must be applied prior to starting the installation $ kubectl annotate bmh openshift-worker-0 -n mynamespace bmac.agent-install.openshift.io/installer-args=\"[\\\"--append-karg\\\", \\\"ip=192.0.2.2::192.0.2.254:255.255.255.0:core0.example.com:enp1s0:none\\\", \\\"--save-partindex\\\", \\\"1\\\", \\\"-n\\\"]\" baremetalhost.metal3.io/openshift-worker-0 annotated $ oc get bmh openshift-worker-0 -n mynamespace -o yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: annotations: bmac.agent-install.openshift.io/installer-args: '[\"--append-karg\", \"ip=192.0.2.2::192.0.2.254:255.255.255.0:core0.example.com:enp1s0:none\", \"--save-partindex\", \"1\", \"-n\"]' creationTimestamp: \"2021-04-13T10:46:57Z\" generation: 1 name: openshift-worker-0 namespace: mynamespace spec:","title":"Creating host installer args overrides"},{"location":"hive-integration/#creating-host-ignition-config-overrides","text":"In case of failure to apply the overrides, the agent conditions will reflect the error and show the relevant error message. Add an annotation with the desired options, the bmac controller will update the agent spec with the annotation value. Then agent controller will forward it to host configuration. Note that this configuration must be applied prior to starting the installation $ kubectl annotate bmh openshift-worker-0 -n mynamespace bmac.agent-install.openshift.io/ignition-config-overrides=\"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/tmp/example\\\", \\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,aGVscGltdHJhcHBlZGluYXN3YWdnZXJzcGVj\\\"}}]}}\" baremetalhost.metal3.io/openshift-worker-0 annotated $ oc get bmh openshift-worker-0 -n mynamespace -o yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: annotations: bmac.agent-install.openshift.io/ignition-config-overrides: '{\"ignition\": {\"version\": \"3.1.0\"}, \"storage\": {\"files\": [{\"path\": \"/tmp/example\", \"contents\": {\"source\": \"data:text/plain;base64,aGVscGltdHJhcHBlZGluYXN3YWdnZXJzcGVj\"}}]}}' creationTimestamp: \"2021-04-14T10:46:57Z\" generation: 1 name: openshift-worker-0 namespace: mynamespace spec:","title":"Creating host ignition config overrides"},{"location":"hive-integration/#creating-additional-manifests","text":"In order to add custom manifests that will be added to the installation manifests generated by openshift-install create command, user will need to create configmap with valid manifests: kind: ConfigMap apiVersion: v1 metadata: name: my-baremetal-cluster-install-manifests namespace: mynamespace data: 99_master_kernel_arg.yaml: | apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 99-openshift-machineconfig-master-kargs spec: kernelArguments: - 'loglevel=7'` Create/update AgentClusterInstall with field manifestsConfigMapRef: apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: name: my-baremetal-cluster namespace: mynamespace spec: manifestsConfigMapRef: name: my-baremetal-cluster-install-manifests If manifests provided in configmap data section will be in bad format or configmap will not exists but will be referenced we will set error in Sync condition only if cluster will be ready for installation. Changing configmap should fix the issue.","title":"Creating Additional manifests"},{"location":"hive-integration/#teardown-procedure","text":"Deleting the ClusterDeployment will automatically trigger the deletion of its referenced AgentClusterInstall and the deletion of all the Agents connected to it (Unless late binding was used, see here ). Note that the installed OCP cluster, if exists, will not be affected by the deletion of the ClusterDeployment. Deleting only the AgentClusterInstall will delete the Agents connected to it (Unless late binding was used), but the ClusterDeployment will remain. BareMetalHost, InfraEnv, ClusterImageSet and NMStateConfig deletion will not trigger deletion of other resources. In case that the assisted-service is not available, the deletion of ClusterDeployment, AgentClusterInstall and Agents resources will be blocked due to finalizers that are set on them. Here an example on how to remove finalizers on a resource: kubectl -n mynamespace patch agentclusterinstalls.extensions.hive.openshift.io my-aci -p '{\"metadata\":{\"finalizers\":null}}' --type=merge","title":"Teardown procedure"},{"location":"hive-integration/#development","text":"","title":"Development"},{"location":"hive-integration/#crd-update","text":"Changes in CRDs should be made in the CRDs Go files located here . After the changes are done, the yamls need to be generated by running: skipper make generate-all","title":"CRD update"},{"location":"hive-integration/baremetal-agent-controller/","text":"Baremetal Agent Controller (a.k.a BMAC) BMAC is a Kubernetes controller responsible for reconciling BareMetalHost and Agent (defined and maintained in this repo) resources for the agent-based deployment scenario. Testing The testing environment for BMAC consists of Downstream dev-scripts deployment Baremetal Operator : It defines the BareMetalHost custom resource Assisted Installer Operator : To deploy and manage the assisted installer deployment. Read the operator docs to know more about its dependencies and installation process. Each of the components listed above provide their own documentation on how to deploy and configure them. However, you can find below a set of recommended configs that can be used for each of these components: Dev Scripts # Giving Master nodes some extra CPU since we won't be # deploying any workers export MASTER_VCPU=4 export MASTER_MEMORY=20000 # Set specs for workers export WORKER_VCPU=4 export WORKER_MEMORY=20000 export WORKER_DISK=60 # No workers are needed to test BMAC export NUM_WORKERS=0 # Add extra workers so we can use it for the deployment. # SNO requires 1 extra machine to be created. export NUM_EXTRA_WORKERS=1 # At the time of this writing, this requires the 1195 PR # mentioned below. export PROVISIONING_NETWORK_PROFILE=Disabled # Add extradisks to VMs export VM_EXTRADISKS=true export VM_EXTRADISKS_LIST=\"vda vdb\" export VM_EXTRADISKS_SIZE=\"30G\" export REDFISH_EMULATOR_IGNORE_BOOT_DEVICE=True The config above should provide you with an environment that is ready to be used for the operator, assisted installer, and BMAC tests. Here are a few tips that would help simplifying the environment and the steps required: Clone baremetal-operator somewhere and set the BAREMETAL_OPERATOR_LOCAL_IMAGE in your config. NOTE The default hardware requirements for the OCP cluster are higher than the values provided below. A guide on how to customize validator requirements can be found here . Local Baremetal Operator (optional) NOTE This section is completely optional. If you don't need to run your own clone of the baremetal-operator , just ignore it and proceed to the next step. The baremetal-operator will define the BareMetalHost custom resource required by the agent based install process. Setting the BAREMETAL_OPERATOR_LOCAL_IMAGE should build and run the BMO already. However, it's recommended to run the local-bmo script to facilitate the deployment and monitoring of the BMO. Here's what using local-bmo looks like: It's possible to disable inspection for the master (and workers) nodes before running the local-bmo script. This will make the script less noisy which will make debugging easier. ./metal3-dev/pause-control-plane.sh The pause-control-plane.sh script only pauses the control plane. You can do the same for the worker nodes with the following command for host in $(oc get baremetalhost -n openshift-machine-api -o name | grep -e '-worker-'); do oc annotate --overwrite -n openshift-machine-api \"$host\" \\ 'baremetalhost.metal3.io/paused=\"\"' done The steps mentioned above are optional, and only recommended for debugging purposes. Let's now run local-bmo and move on. This script will tail the logs so do it in a separate buffer so that it can be kept running. # Note variable is different from the one in your dev-script # config file. You can set it to the same path, though. export BAREMETAL_OPERATOR_PATH=/path/to/your/local/clone ./metal3-dev/local-bmo.sh Assisted Installer Operator Once the dev-script environment is up-and-running, and the bmo has been deployed, you can proceed to deploying the Assisted Installer Operator. There's a script in the dev-scripts repo that facilitates this step: [dev@edge-10 dev-scripts]$ ./assisted_deployment.sh install_assisted_service Take a look at the script itself to know what variables can be customized for the Assisted Installer Operator deployment. Creating AgentClusterInstall, ClusterDeployment and InfraEnv resources A number of resources has to be created in order to have the deployment fully ready for deploying OCP clusters. A typical workflow is as follows create the PullSecret in order to create it directly from file you can use the following kubectl create secret -n assisted-installer generic pull-secret --from-file=.dockerconfigjson=pull_secret.json create the ClusterImageSet optionally create a custom ConfigMap overriding default Assisted Service configuration create the AgentClusterInstall or AgentClusterInstall for SNO more manifests (e.g. IPv6 deployments) can be found here create the ClusterDeployment create the InfraEnv patch BareMetalOperator to watch namespaces other than openshift-machine-api $ oc patch provisioning provisioning-configuration --type merge -p '{\"spec\":{\"watchAllNamespaces\": true}}' NOTE When deploying AgentClusterInstall for SNO it is important to make sure that machineNetwork subnet matches the subnet used by libvirt VMs (configured by passing EXTERNAL_SUBNET_V4 to the dev-scripts config ). It defaults to 192.168.111.0/24 therefore the sample manifest linked above needs to be adapted. At this moment it's good to check logs and verify that there are no conflicting parameters, the ISO has been created correctly and that the installation can be started once a suitable node is provided. To check if the ISO has been created correctly, do oc get infraenv myinfraenv -o jsonpath='{.status.isoDownloadURL}' -n assisted-installer Creating BareMetalHost resources The baremetal operator creates the BareMetalHost resources for the existing nodes automatically. For scenarios using extra worker nodes (like SNO), it will be necessary to create BareMetalHost resources manually. Luckily enough, assisted_deployment.sh is one step ahead and it has prepared the manifest for us already. less ocp/ostest/saved-assets/assisted-installer-manifests/06-extra-host-manifests.yaml The created BareMetalHost manifest contains already a correct namespace as well as annotations to disable the inspection and cleaning. Below is an example on what it could look like. Please remember to change the value of the infraenvs.agent-install.openshift.io label in case you are using different than the default one ( myinfraenv ). apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: name: ostest-worker-0 namespace: assisted-installer annotations: inspect.metal3.io: disabled labels: infraenvs.agent-install.openshift.io: \"myinfraenv\" spec: online: true bootMACAddress: 00:ec:ee:f8:5a:ba automatedCleaningMode: disabled bmc: address: .... credentialsName: bmc-secret Setting automatedCleaningMode field and the inspect.metal3.io is optional as BMAC will add them automatically. Without those the BareMetalHost will boot IPA and spend some time in the inspecting phase when the manifest is applied. Setting the infraenvs.agent-install.openshift.io is required and it must be set to the name of the InfraEnv to use. Without it, BMAC won't be able to set the ISO Url in the BareMetalHost resource. It is possible to specify RootDeviceHints for the BareMetalHost resource. Root device hints are used to tell the installer what disk to use as the installation disk. Refer to the baremetal-operator documentation to know more. NOTE BMAC is always setting automatedCleaningMode: disabled even if the BareMetalHost manifest specifies another value (e.g. automatedCleaningMode: metadata ). This may be changed in the future releases, but currently we do not support using Ironic to clean the node. Installation flow After all the resources described above are created the installation starts automatically. A detailed flow is out of scope of this document and can be found here . An Agent resource will be created that can be monitored during the installation proces as in the example below $ oc get agent -A $ oc get agentclusterinstalls test-agent-cluster-install -o json | jq '.status.conditions[] |select(.type | contains(\"Completed\"))' After the installation succeeds there are two new secrets created in the assisted-installer namespace assisted-installer single-node-admin-kubeconfig Opaque 1 12h assisted-installer single-node-admin-password Opaque 2 12h Kubeconfig can be exported to the file with $ oc get secret single-node-admin-kubeconfig -o json -n assisted-installer | jq '.data' | cut -d '\"' -f 4 | tr -d '{}' | base64 --decode > /tmp/kubeconfig-sno.yml NOTE ClusterDeployment resource defines baseDomain for the installed OCP cluster. This one will be used in the generated kubeconfig file so it may happen (depending on the domain chosen) that there is no connectivity caused by name not being resolved. In such a scenario a manual intervention may be needed (e.g. manual entry in /etc/hosts ). Troubleshooting I have created the BMH, the ClusterDeployment, and the InfraEnv resources. Why doesn't the node start? The first thing to do is to verify that an ISO has been created and that it is associated with the BMH. Here are a few commands that can be run to achieve this: $ oc describe infraenv $YOUR_INFRAENV | grep ISO $ oc describe bmh $YOUR_BMH | grep Image InfraEnv's ISO Url doesn't have an URL set This means something may have gone wrong during the ISO generation. Check the assisted-service logs (and docs) to know what happened. InfraEnv has an URL associated but the BMH Image URL field is not set: Check that the infraenvs.agent-install.openshift.io label is set in your BareMetalHost resource and that the value matches the name of the InfraEnv's. Remember that both resources must be in the same namespace. Check that resources in the openshift-machine-api are up and running. cluster-baremetal-operator is responsible for handling the state of the BMH so if that one is not running, your BMH will never move forward. Check that cluster-baremetal-operator is not configured to ignore any namespaces or CRDs. You can do it by checking the overrides section in $ oc describe clusterversion version --namespace openshift-cluster-version URL is set everywhere, node still doesn't start Double check that the BareMetalHost definition has online set to true. BMAC should take care of this during the reconcile but, you know, software, computer gnomes, and dark magic. Node boots but it loooks like it is booting something else Check that the inspect.metal3.io and automatedCleaningMode are both set to disabled . This will prevent Ironic from doing inspection and any cleaning, which will speed up the deployment process and prevent it from running IPA before running the ISO. This should be set automatically by BMAC in the part linked here but if that is not the case, start from checking the assisted-service logs as there may be more errors related to the BMH. Node boots, but nothing else seems to be happening Check that an agent has been registered for this cluster and BMH. You can verify this by chekcing the existing agents and find the one that has an interface with a MacAddress that matches the BMH BootMACAddress . Remember that in between the node booting from the Discovery ISO and the Agent CR being created you may need to wait a few minutes. If there is an agent, the next thing to check is that all validations have passed. This can be done by inspecting the ClusterDeployment and verify that the validation phase has succeeded.","title":"Baremetal agent controller"},{"location":"hive-integration/baremetal-agent-controller/#baremetal-agent-controller-aka-bmac","text":"BMAC is a Kubernetes controller responsible for reconciling BareMetalHost and Agent (defined and maintained in this repo) resources for the agent-based deployment scenario.","title":"Baremetal Agent Controller (a.k.a BMAC)"},{"location":"hive-integration/baremetal-agent-controller/#testing","text":"The testing environment for BMAC consists of Downstream dev-scripts deployment Baremetal Operator : It defines the BareMetalHost custom resource Assisted Installer Operator : To deploy and manage the assisted installer deployment. Read the operator docs to know more about its dependencies and installation process. Each of the components listed above provide their own documentation on how to deploy and configure them. However, you can find below a set of recommended configs that can be used for each of these components:","title":"Testing"},{"location":"hive-integration/baremetal-agent-controller/#dev-scripts","text":"# Giving Master nodes some extra CPU since we won't be # deploying any workers export MASTER_VCPU=4 export MASTER_MEMORY=20000 # Set specs for workers export WORKER_VCPU=4 export WORKER_MEMORY=20000 export WORKER_DISK=60 # No workers are needed to test BMAC export NUM_WORKERS=0 # Add extra workers so we can use it for the deployment. # SNO requires 1 extra machine to be created. export NUM_EXTRA_WORKERS=1 # At the time of this writing, this requires the 1195 PR # mentioned below. export PROVISIONING_NETWORK_PROFILE=Disabled # Add extradisks to VMs export VM_EXTRADISKS=true export VM_EXTRADISKS_LIST=\"vda vdb\" export VM_EXTRADISKS_SIZE=\"30G\" export REDFISH_EMULATOR_IGNORE_BOOT_DEVICE=True The config above should provide you with an environment that is ready to be used for the operator, assisted installer, and BMAC tests. Here are a few tips that would help simplifying the environment and the steps required: Clone baremetal-operator somewhere and set the BAREMETAL_OPERATOR_LOCAL_IMAGE in your config. NOTE The default hardware requirements for the OCP cluster are higher than the values provided below. A guide on how to customize validator requirements can be found here .","title":"Dev Scripts"},{"location":"hive-integration/baremetal-agent-controller/#local-baremetal-operator-optional","text":"NOTE This section is completely optional. If you don't need to run your own clone of the baremetal-operator , just ignore it and proceed to the next step. The baremetal-operator will define the BareMetalHost custom resource required by the agent based install process. Setting the BAREMETAL_OPERATOR_LOCAL_IMAGE should build and run the BMO already. However, it's recommended to run the local-bmo script to facilitate the deployment and monitoring of the BMO. Here's what using local-bmo looks like: It's possible to disable inspection for the master (and workers) nodes before running the local-bmo script. This will make the script less noisy which will make debugging easier. ./metal3-dev/pause-control-plane.sh The pause-control-plane.sh script only pauses the control plane. You can do the same for the worker nodes with the following command for host in $(oc get baremetalhost -n openshift-machine-api -o name | grep -e '-worker-'); do oc annotate --overwrite -n openshift-machine-api \"$host\" \\ 'baremetalhost.metal3.io/paused=\"\"' done The steps mentioned above are optional, and only recommended for debugging purposes. Let's now run local-bmo and move on. This script will tail the logs so do it in a separate buffer so that it can be kept running. # Note variable is different from the one in your dev-script # config file. You can set it to the same path, though. export BAREMETAL_OPERATOR_PATH=/path/to/your/local/clone ./metal3-dev/local-bmo.sh","title":"Local Baremetal Operator (optional)"},{"location":"hive-integration/baremetal-agent-controller/#assisted-installer-operator","text":"Once the dev-script environment is up-and-running, and the bmo has been deployed, you can proceed to deploying the Assisted Installer Operator. There's a script in the dev-scripts repo that facilitates this step: [dev@edge-10 dev-scripts]$ ./assisted_deployment.sh install_assisted_service Take a look at the script itself to know what variables can be customized for the Assisted Installer Operator deployment.","title":"Assisted Installer Operator"},{"location":"hive-integration/baremetal-agent-controller/#creating-agentclusterinstall-clusterdeployment-and-infraenv-resources","text":"A number of resources has to be created in order to have the deployment fully ready for deploying OCP clusters. A typical workflow is as follows create the PullSecret in order to create it directly from file you can use the following kubectl create secret -n assisted-installer generic pull-secret --from-file=.dockerconfigjson=pull_secret.json create the ClusterImageSet optionally create a custom ConfigMap overriding default Assisted Service configuration create the AgentClusterInstall or AgentClusterInstall for SNO more manifests (e.g. IPv6 deployments) can be found here create the ClusterDeployment create the InfraEnv patch BareMetalOperator to watch namespaces other than openshift-machine-api $ oc patch provisioning provisioning-configuration --type merge -p '{\"spec\":{\"watchAllNamespaces\": true}}' NOTE When deploying AgentClusterInstall for SNO it is important to make sure that machineNetwork subnet matches the subnet used by libvirt VMs (configured by passing EXTERNAL_SUBNET_V4 to the dev-scripts config ). It defaults to 192.168.111.0/24 therefore the sample manifest linked above needs to be adapted. At this moment it's good to check logs and verify that there are no conflicting parameters, the ISO has been created correctly and that the installation can be started once a suitable node is provided. To check if the ISO has been created correctly, do oc get infraenv myinfraenv -o jsonpath='{.status.isoDownloadURL}' -n assisted-installer","title":"Creating AgentClusterInstall, ClusterDeployment and InfraEnv resources"},{"location":"hive-integration/baremetal-agent-controller/#creating-baremetalhost-resources","text":"The baremetal operator creates the BareMetalHost resources for the existing nodes automatically. For scenarios using extra worker nodes (like SNO), it will be necessary to create BareMetalHost resources manually. Luckily enough, assisted_deployment.sh is one step ahead and it has prepared the manifest for us already. less ocp/ostest/saved-assets/assisted-installer-manifests/06-extra-host-manifests.yaml The created BareMetalHost manifest contains already a correct namespace as well as annotations to disable the inspection and cleaning. Below is an example on what it could look like. Please remember to change the value of the infraenvs.agent-install.openshift.io label in case you are using different than the default one ( myinfraenv ). apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: name: ostest-worker-0 namespace: assisted-installer annotations: inspect.metal3.io: disabled labels: infraenvs.agent-install.openshift.io: \"myinfraenv\" spec: online: true bootMACAddress: 00:ec:ee:f8:5a:ba automatedCleaningMode: disabled bmc: address: .... credentialsName: bmc-secret Setting automatedCleaningMode field and the inspect.metal3.io is optional as BMAC will add them automatically. Without those the BareMetalHost will boot IPA and spend some time in the inspecting phase when the manifest is applied. Setting the infraenvs.agent-install.openshift.io is required and it must be set to the name of the InfraEnv to use. Without it, BMAC won't be able to set the ISO Url in the BareMetalHost resource. It is possible to specify RootDeviceHints for the BareMetalHost resource. Root device hints are used to tell the installer what disk to use as the installation disk. Refer to the baremetal-operator documentation to know more. NOTE BMAC is always setting automatedCleaningMode: disabled even if the BareMetalHost manifest specifies another value (e.g. automatedCleaningMode: metadata ). This may be changed in the future releases, but currently we do not support using Ironic to clean the node.","title":"Creating BareMetalHost resources"},{"location":"hive-integration/baremetal-agent-controller/#installation-flow","text":"After all the resources described above are created the installation starts automatically. A detailed flow is out of scope of this document and can be found here . An Agent resource will be created that can be monitored during the installation proces as in the example below $ oc get agent -A $ oc get agentclusterinstalls test-agent-cluster-install -o json | jq '.status.conditions[] |select(.type | contains(\"Completed\"))' After the installation succeeds there are two new secrets created in the assisted-installer namespace assisted-installer single-node-admin-kubeconfig Opaque 1 12h assisted-installer single-node-admin-password Opaque 2 12h Kubeconfig can be exported to the file with $ oc get secret single-node-admin-kubeconfig -o json -n assisted-installer | jq '.data' | cut -d '\"' -f 4 | tr -d '{}' | base64 --decode > /tmp/kubeconfig-sno.yml NOTE ClusterDeployment resource defines baseDomain for the installed OCP cluster. This one will be used in the generated kubeconfig file so it may happen (depending on the domain chosen) that there is no connectivity caused by name not being resolved. In such a scenario a manual intervention may be needed (e.g. manual entry in /etc/hosts ).","title":"Installation flow"},{"location":"hive-integration/baremetal-agent-controller/#troubleshooting","text":"I have created the BMH, the ClusterDeployment, and the InfraEnv resources. Why doesn't the node start? The first thing to do is to verify that an ISO has been created and that it is associated with the BMH. Here are a few commands that can be run to achieve this: $ oc describe infraenv $YOUR_INFRAENV | grep ISO $ oc describe bmh $YOUR_BMH | grep Image InfraEnv's ISO Url doesn't have an URL set This means something may have gone wrong during the ISO generation. Check the assisted-service logs (and docs) to know what happened. InfraEnv has an URL associated but the BMH Image URL field is not set: Check that the infraenvs.agent-install.openshift.io label is set in your BareMetalHost resource and that the value matches the name of the InfraEnv's. Remember that both resources must be in the same namespace. Check that resources in the openshift-machine-api are up and running. cluster-baremetal-operator is responsible for handling the state of the BMH so if that one is not running, your BMH will never move forward. Check that cluster-baremetal-operator is not configured to ignore any namespaces or CRDs. You can do it by checking the overrides section in $ oc describe clusterversion version --namespace openshift-cluster-version URL is set everywhere, node still doesn't start Double check that the BareMetalHost definition has online set to true. BMAC should take care of this during the reconcile but, you know, software, computer gnomes, and dark magic. Node boots but it loooks like it is booting something else Check that the inspect.metal3.io and automatedCleaningMode are both set to disabled . This will prevent Ironic from doing inspection and any cleaning, which will speed up the deployment process and prevent it from running IPA before running the ISO. This should be set automatically by BMAC in the part linked here but if that is not the case, start from checking the assisted-service logs as there may be more errors related to the BMH. Node boots, but nothing else seems to be happening Check that an agent has been registered for this cluster and BMH. You can verify this by chekcing the existing agents and find the one that has an interface with a MacAddress that matches the BMH BootMACAddress . Remember that in between the node booting from the Discovery ISO and the Agent CR being created you may need to wait a few minutes. If there is an agent, the next thing to check is that all validations have passed. This can be done by inspecting the ClusterDeployment and verify that the validation phase has succeeded.","title":"Troubleshooting"},{"location":"hive-integration/kube-api-conditions/","text":"Hive Integration - Conditions Conditions provide a standard mechanism for higher-level status reporting from a controller. Read more about conditions here AgentClusterInstall Conditions AgentClusterInstall supported condition types are: SpecSynced , RequirementsMet , Completed , Failed , Stopped and Validated . Type Status Reason Message Description SpecSynced True SyncOK The Spec has been successfully applied If the Spec was successfully applied SpecSynced False BackendError The Spec could not be synced due to backend error: If the Spec was not applied due to 500 error SpecSynced False InputError The Spec could not be synced due to an input error: If the Spec was not applied due to 40X error Validated True ValidationsPassing The cluster's validations are passing Otherwise than other conditions Validated False ValidationsFailing The cluster's validations are failing: \"summary of not-succeeded validations\" If the cluster status is \"insufficient\" Validated False ValidationsUserPending The cluster's validations are pending for user: \"summary of not-succeeded validations\" If the cluster status is \"pending-for-input\" Validated Unknown ValidationsUnknown The cluster's validations have not yet been calculated If the validations have not yet been calculated RequirementsMet True ClusterIsReady The cluster is ready to begin the installation if the cluster status is \"ready\" RequirementsMet False ClusterNotReady The cluster is not ready to begin the installation If the cluster is before installation (\"insufficient\"/\"pending-for-input\") RequirementsMet True ClusterAlreadyInstalling The cluster requirements are met If the cluster has begun installing (\"preparing-for-installation\", \"installing\", \"finalizing\", \"installing-pending-user-action\", \"adding-hosts\") RequirementsMet True ClusterInstallationStopped The cluster installation stopped If the cluster has stopped installing (\"installed\", \"error\") RequirementsMet False InsufficientAgents The cluster currently requires X agents but only Y are ready If the cluster is ready but we don't have the expected number of ready agents RequirementsMet False UnapprovedAgents The installation is pending on the approval of X agents If the cluster is ready with the expected number of ready agents, but not all have been approved RequirementsMet False AdditionalAgents The cluster currently requires exactly X agents but have Y registered If the cluster is ready but more agents are registered than the number or required Completed True InstallationCompleted The installation has completed: \"status_info\" If the cluster status is \"installed\" Completed False InstallationFailed The installation has failed: \"status_info\" If the cluster status is \"error\" Completed False InstallationNotStarted The installation has not yet started If the cluster is before installation (\"insufficient\"/\"pending-for-input\"/\"ready\") Completed False InstallationOnHold The installation is on hold, to unhold set holdInstallation to false If the cluster is before installation and holdInstallation is set to true in the spec (\"ready\") Completed False InstallationInProgress The installation is in progress: \"status_info\" If the cluster is installing (\"preparing-for-installation\", \"installing\", \"finalizing\", \"installing-pending-user-action\") Failed True InstallationFailed The installation failed: \"status_info\" if the cluster status is \"error\" Failed False InstallationNotFailed The installation has not failed If the cluster status is not \"error\" Stopped True InstallationFailed The installation has stopped due to error if the cluster status is \"error\" Stopped True InstallationCancelled The installation has stopped because it was cancelled if the cluster status is \"cancelled\" Stopped True InstallationCompleted The installation has stopped because it completed successfully if the cluster status is \"installed\" Stopped False InstallationNotStopped The installation is waiting to start or in progress If the cluster status is not \"error\", \"cancelled\" or \"installed Here an example of AgentClusterInstall conditions: Status: Conditions: Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The Spec has been successfully applied Reason: SyncOK Status: True Type: SpecSynced Last Probe Time: 2021-05-12T09:07:39Z Last Transition Time: 2021-05-12T09:07:39Z Message: The cluster requirements are met Reason: ClusterAlreadyInstalling Status: True Type: RequirementsMet Last Probe Time: 2021-05-12T09:07:39Z Last Transition Time: 2021-05-12T09:07:39Z Message: The cluster's validations are passing Reason: ValidationsPassing Status: True Type: Validated Last Probe Time: 2021-05-12T09:20:09Z Last Transition Time: 2021-05-12T09:20:09Z Message: The installation is in progress: Finalizing cluster installation Reason: InstallationInProgress Status: False Type: Completed Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The installation has not failed Reason: InstallationNotFailed Status: False Type: Failed Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The installation is waiting to start or in progress Reason: InstallationNotStopped Status: False Type: Stopped Agent Conditions The Agent condition types supported are: SpecSynced , Connected , RequirementsMet , Validated , Installed and Bound . Type Status Reason Message Description SpecSynced True SyncOK The Spec has been successfully applied If the Spec was successfully applied SpecSynced False BackendError The Spec could not be synced due to backend error: If the Spec was not applied due to 500 error SpecSynced False InputError The Spec could not be synced due to an input error: If the Spec was not applied due to 40X error Validated True ValidationsPassing The agent's validations are passing Otherwise than other conditions Validated False ValidationsFailing The agent's validations are failing: \"summary of not-succeeded validations\" If the host status is \"insufficient\" Validated False ValidationsUserPending The agent's validations are pending for user: \"summary of not-succeeded validations\" If the host status is \"pending-for-input\" Validated Unknown ValidationsUnknown The agent's validations have not yet been calculated If the validations have not yet been calculated Validated False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Validated False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" RequirementsMet True AgentIsReady The agent is ready to begin the installation If the host is approved and in status \"known\" RequirementsMet False AgentNotReady The agent is not ready to begin the installation If the host is before installation (\"discovering\"/\"insufficient\"/\"disconnected\"/\"pending-input\") RequirementsMet False AgentIsNotApproved The agent is not approved If the host is not approved and in status \"known\" RequirementsMet True AgentAlreadyInstalling Installation already started and is in progress If the agent has begun installing (\"preparing-successful\",\"preparing-for-installation\", \"installing\") RequirementsMet True AgentInstallationStopped The agent installation stopped If the agent has stopped installing (\"installed\", \"error\") RequirementsMet False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" RequirementsMet False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" Installed True InstallationCompleted The installation has completed: \"status_info\" If the host status is \"installed\" Installed False InstallationFailed The installation has failed: \"status_info\" If the host status is \"error\" Installed False InstallationNotStarted The installation has not yet started If the cluster is before installation (\"discovering\"/\"insufficient\"/\"disconnected\"/\"pending-input/known\") Installed False InstallationInProgress The installation is in progress: \"status_info\" If the host is installing (\"preparing-for-installation\", \"preparing-successful\", \"installing\") Installed False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Installed False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" Connected True AgentIsConnected The agent has not contacted the installation service in some time, user action should be taken If the host status is not \"disconnected\" Connected False AgentIsDisconnected The agent's connection to the installation service is unimpaired If the host status is \"error\" Bound True Bound The agent is bound to a cluster deployment If the host status is \"known\", \"disconnected\", \"disconnected\", \"insufficient\", \"disabled\" or \"discovering\" Bound False Bound The agent is not bound to any cluster deployment If the host status is \"known-unbound\", \"disconnected-unbound\", \"disconnected-unbound\", \"insufficient-unbound\", \"disabled-unbound\" or \"discovering-unbound\" Bound False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Bound False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" Bound False UnbindingPendingUserAction The agent is currently unbinding; Pending host reboot from infraenv image If the host status is \"unbinding-pending-user-action\" Here an example of Agent conditions: Status: Conditions: Last Transition Time: 2021-04-22T15:50:24Z Message: The Spec has been successfully applied Reason: SyncOK Status: True Type: SpecSynced Last Transition Time: 2021-04-22T15:50:24Z Message: The agent's connection to the installation service is unimpaired Reason: AgentIsConnected Status: True Type: Connected Last Transition Time: 2021-04-22T15:50:33Z Message: Installation already started and is in progress Reason: AgentAlreadyInstalling Status: True Type: RequirementsMet Last Transition Time: 2021-04-22T15:50:26Z Message: The agent's validations are passing Reason: ValidationsPassing Status: True Type: Validated Last Transition Time: 2021-04-22T15:50:24Z Message: The installation is in progress: Host is preparing for installation Reason: InstallationInProgress Status: False Type: Installed InfraEnv Conditions The InfraEnv condition type supported is: ImageCreated Type Status Reason Message Description ImageCreated True ImageCreated Image has been created If the ISO image was successfully created ImageCreated False ImageCreationError Failed to create image: \"error message\" If the ISO image was not successfully created Here an example of InfraEnv conditions: Status: Conditions: Last Transition Time: 2021-04-22T15:49:35Z Message: Image has been created Reason: ImageCreated Status: True Type: ImageCreated","title":"Hive Integration - Conditions"},{"location":"hive-integration/kube-api-conditions/#hive-integration-conditions","text":"Conditions provide a standard mechanism for higher-level status reporting from a controller. Read more about conditions here","title":"Hive Integration - Conditions"},{"location":"hive-integration/kube-api-conditions/#agentclusterinstall-conditions","text":"AgentClusterInstall supported condition types are: SpecSynced , RequirementsMet , Completed , Failed , Stopped and Validated . Type Status Reason Message Description SpecSynced True SyncOK The Spec has been successfully applied If the Spec was successfully applied SpecSynced False BackendError The Spec could not be synced due to backend error: If the Spec was not applied due to 500 error SpecSynced False InputError The Spec could not be synced due to an input error: If the Spec was not applied due to 40X error Validated True ValidationsPassing The cluster's validations are passing Otherwise than other conditions Validated False ValidationsFailing The cluster's validations are failing: \"summary of not-succeeded validations\" If the cluster status is \"insufficient\" Validated False ValidationsUserPending The cluster's validations are pending for user: \"summary of not-succeeded validations\" If the cluster status is \"pending-for-input\" Validated Unknown ValidationsUnknown The cluster's validations have not yet been calculated If the validations have not yet been calculated RequirementsMet True ClusterIsReady The cluster is ready to begin the installation if the cluster status is \"ready\" RequirementsMet False ClusterNotReady The cluster is not ready to begin the installation If the cluster is before installation (\"insufficient\"/\"pending-for-input\") RequirementsMet True ClusterAlreadyInstalling The cluster requirements are met If the cluster has begun installing (\"preparing-for-installation\", \"installing\", \"finalizing\", \"installing-pending-user-action\", \"adding-hosts\") RequirementsMet True ClusterInstallationStopped The cluster installation stopped If the cluster has stopped installing (\"installed\", \"error\") RequirementsMet False InsufficientAgents The cluster currently requires X agents but only Y are ready If the cluster is ready but we don't have the expected number of ready agents RequirementsMet False UnapprovedAgents The installation is pending on the approval of X agents If the cluster is ready with the expected number of ready agents, but not all have been approved RequirementsMet False AdditionalAgents The cluster currently requires exactly X agents but have Y registered If the cluster is ready but more agents are registered than the number or required Completed True InstallationCompleted The installation has completed: \"status_info\" If the cluster status is \"installed\" Completed False InstallationFailed The installation has failed: \"status_info\" If the cluster status is \"error\" Completed False InstallationNotStarted The installation has not yet started If the cluster is before installation (\"insufficient\"/\"pending-for-input\"/\"ready\") Completed False InstallationOnHold The installation is on hold, to unhold set holdInstallation to false If the cluster is before installation and holdInstallation is set to true in the spec (\"ready\") Completed False InstallationInProgress The installation is in progress: \"status_info\" If the cluster is installing (\"preparing-for-installation\", \"installing\", \"finalizing\", \"installing-pending-user-action\") Failed True InstallationFailed The installation failed: \"status_info\" if the cluster status is \"error\" Failed False InstallationNotFailed The installation has not failed If the cluster status is not \"error\" Stopped True InstallationFailed The installation has stopped due to error if the cluster status is \"error\" Stopped True InstallationCancelled The installation has stopped because it was cancelled if the cluster status is \"cancelled\" Stopped True InstallationCompleted The installation has stopped because it completed successfully if the cluster status is \"installed\" Stopped False InstallationNotStopped The installation is waiting to start or in progress If the cluster status is not \"error\", \"cancelled\" or \"installed Here an example of AgentClusterInstall conditions: Status: Conditions: Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The Spec has been successfully applied Reason: SyncOK Status: True Type: SpecSynced Last Probe Time: 2021-05-12T09:07:39Z Last Transition Time: 2021-05-12T09:07:39Z Message: The cluster requirements are met Reason: ClusterAlreadyInstalling Status: True Type: RequirementsMet Last Probe Time: 2021-05-12T09:07:39Z Last Transition Time: 2021-05-12T09:07:39Z Message: The cluster's validations are passing Reason: ValidationsPassing Status: True Type: Validated Last Probe Time: 2021-05-12T09:20:09Z Last Transition Time: 2021-05-12T09:20:09Z Message: The installation is in progress: Finalizing cluster installation Reason: InstallationInProgress Status: False Type: Completed Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The installation has not failed Reason: InstallationNotFailed Status: False Type: Failed Last Probe Time: 2021-05-12T09:06:30Z Last Transition Time: 2021-05-12T09:06:30Z Message: The installation is waiting to start or in progress Reason: InstallationNotStopped Status: False Type: Stopped","title":"AgentClusterInstall Conditions"},{"location":"hive-integration/kube-api-conditions/#agent-conditions","text":"The Agent condition types supported are: SpecSynced , Connected , RequirementsMet , Validated , Installed and Bound . Type Status Reason Message Description SpecSynced True SyncOK The Spec has been successfully applied If the Spec was successfully applied SpecSynced False BackendError The Spec could not be synced due to backend error: If the Spec was not applied due to 500 error SpecSynced False InputError The Spec could not be synced due to an input error: If the Spec was not applied due to 40X error Validated True ValidationsPassing The agent's validations are passing Otherwise than other conditions Validated False ValidationsFailing The agent's validations are failing: \"summary of not-succeeded validations\" If the host status is \"insufficient\" Validated False ValidationsUserPending The agent's validations are pending for user: \"summary of not-succeeded validations\" If the host status is \"pending-for-input\" Validated Unknown ValidationsUnknown The agent's validations have not yet been calculated If the validations have not yet been calculated Validated False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Validated False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" RequirementsMet True AgentIsReady The agent is ready to begin the installation If the host is approved and in status \"known\" RequirementsMet False AgentNotReady The agent is not ready to begin the installation If the host is before installation (\"discovering\"/\"insufficient\"/\"disconnected\"/\"pending-input\") RequirementsMet False AgentIsNotApproved The agent is not approved If the host is not approved and in status \"known\" RequirementsMet True AgentAlreadyInstalling Installation already started and is in progress If the agent has begun installing (\"preparing-successful\",\"preparing-for-installation\", \"installing\") RequirementsMet True AgentInstallationStopped The agent installation stopped If the agent has stopped installing (\"installed\", \"error\") RequirementsMet False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" RequirementsMet False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" Installed True InstallationCompleted The installation has completed: \"status_info\" If the host status is \"installed\" Installed False InstallationFailed The installation has failed: \"status_info\" If the host status is \"error\" Installed False InstallationNotStarted The installation has not yet started If the cluster is before installation (\"discovering\"/\"insufficient\"/\"disconnected\"/\"pending-input/known\") Installed False InstallationInProgress The installation is in progress: \"status_info\" If the host is installing (\"preparing-for-installation\", \"preparing-successful\", \"installing\") Installed False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Installed False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" or \"unbinding-pending-user-action\" Connected True AgentIsConnected The agent has not contacted the installation service in some time, user action should be taken If the host status is not \"disconnected\" Connected False AgentIsDisconnected The agent's connection to the installation service is unimpaired If the host status is \"error\" Bound True Bound The agent is bound to a cluster deployment If the host status is \"known\", \"disconnected\", \"disconnected\", \"insufficient\", \"disabled\" or \"discovering\" Bound False Bound The agent is not bound to any cluster deployment If the host status is \"known-unbound\", \"disconnected-unbound\", \"disconnected-unbound\", \"insufficient-unbound\", \"disabled-unbound\" or \"discovering-unbound\" Bound False Binding The agent is currently binding to a cluster deployment If the host status is \"binding\" Bound False Unbinding The agent is currently unbinding from a cluster deployment If the host status is \"unbinding\" Bound False UnbindingPendingUserAction The agent is currently unbinding; Pending host reboot from infraenv image If the host status is \"unbinding-pending-user-action\" Here an example of Agent conditions: Status: Conditions: Last Transition Time: 2021-04-22T15:50:24Z Message: The Spec has been successfully applied Reason: SyncOK Status: True Type: SpecSynced Last Transition Time: 2021-04-22T15:50:24Z Message: The agent's connection to the installation service is unimpaired Reason: AgentIsConnected Status: True Type: Connected Last Transition Time: 2021-04-22T15:50:33Z Message: Installation already started and is in progress Reason: AgentAlreadyInstalling Status: True Type: RequirementsMet Last Transition Time: 2021-04-22T15:50:26Z Message: The agent's validations are passing Reason: ValidationsPassing Status: True Type: Validated Last Transition Time: 2021-04-22T15:50:24Z Message: The installation is in progress: Host is preparing for installation Reason: InstallationInProgress Status: False Type: Installed","title":"Agent Conditions"},{"location":"hive-integration/kube-api-conditions/#infraenv-conditions","text":"The InfraEnv condition type supported is: ImageCreated Type Status Reason Message Description ImageCreated True ImageCreated Image has been created If the ISO image was successfully created ImageCreated False ImageCreationError Failed to create image: \"error message\" If the ISO image was not successfully created Here an example of InfraEnv conditions: Status: Conditions: Last Transition Time: 2021-04-22T15:49:35Z Message: Image has been created Reason: ImageCreated Status: True Type: ImageCreated","title":"InfraEnv Conditions"},{"location":"hive-integration/kube-api-getting-started/","text":"Kube API - Getting Started Guide This document is a step-by-step guide that demonstrates how to deploy a single-node cluster end to end. Note : * This document is not meant to expand on each and every resource in detail; other documents already do that . Instead, expect the details needed to understand the context and what is currently happening. The order in which actions are performed here is optional , and you may choose to go by a different one. The one order restriction the API does have is in regards to creating NMstateConfigs (which are optional) before InfraEnv , for more details, check the warning mentioned here . What To Expect A walk through of custom resource definitions (CRDs) and understand how they relate to each other. A clear understanding of what happened, both in OpenShift / Kubernetes and assisted-service backend per each action described below. How To Use This guide Make sure that for each step you follow, you understand how and why it is done. You are encouraged to copy and paste the below-mentioned commands to your terminal and see things in action. Note the inline comments in the yaml examples. There is no need to remove those in case you which to copy and create the resources directly in your environment. Assumptions You have deployed both the operator and assisted-service by, for example, using these instructions . Alternatively, you may choose to deploy via OLM or as a part of ACM, which is also applicable. Using the discovery image: Boot it yourself : You are able to use the generated discovery image to boot your host. Zero Touch Provisioning (ZTP) : Detailed in the advanced section . Configure BMH / BareMetalHost to automate the host discovery procedure. This document uses dev-scripts for that. read about it here . Resource Types and Relationships Cluster Prerequisites 1. Create a namespace for your resources cat <<EOF | kubectl create -f - apiVersion: v1 kind: Namespace metadata: name: demo-worker4 # Use any name, but note that using 'assisted-installer' is not recommended EOF 2. Create a Pull Secret Use the secret obtained from console.redhat.com cat <<EOF | kubectl create -f - apiVersion: v1 kind: Secret type: kubernetes.io/dockerconfigjson metadata: name: pull-secret namespace: demo-worker4 stringData: .dockerconfigjson: '{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"your secret here\",\"email\":\"user@example.com\"}}}' EOF 3. Generate ClusterImageSet , to specify the OpenShift version to be used. cat <<EOF | kubectl create -f - apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.8.0-rc.0-x86_64 # That version may change over time EOF Creating Cluster Resources Define a cluster with assisted-service For that, you'll to need create two resources: 1. Create ClusterDepoyment cat <<EOF | kubectl create -f - apiVersion: hive.openshift.io/v1 kind: ClusterDeployment metadata: name: single-node namespace: demo-worker4 spec: baseDomain: hive.example.com clusterInstallRef: group: extensions.hive.openshift.io kind: AgentClusterInstall name: test-agent-cluster-install # Use the same name for AgentClusterInstall version: v1beta1 clusterName: test-cluster controlPlaneConfig: servingCertificates: {} platform: agentBareMetal: agentSelector: matchLabels: bla: aaa pullSecretRef: name: pull-secret # Use the pull secret name mentioned in Cluster Prerequisites step 2 EOF Result At this point, there is no AgentClusterInstall , so it won't do much and won't even register the cluster. ClusterDeployment resource conditions: kubectl -n demo-worker4 get clusterdeployments.hive.openshift.io single-node -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" single-node RelocationFailed no ClusterRelocates match Hibernating Condition Initialized AuthenticationFailure Platform credentials passed authentication check assisted-service log: time=\"2021-06-28T20:45:07Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789 time=\"2021-06-28T20:45:07Z\" level=info msg=\"AgentClusterInstall does not exist for ClusterDeployment single-node\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:147\" AgentClusterInstall=test-agent-cluster-install cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789 time=\"2021-06-28T20:45:07Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789 2. Create AgentClusterInstal cat <<EOF | kubectl create -f - apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: name: test-agent-cluster-install namespace: demo-worker4 spec: clusterDeploymentRef: name: single-node # Use the clusterDeployment name from the previous step imageSetRef: name: openshift-v4.8.0 # Use the ClusterImageSet name mentioned in Cluster Prerequisites step 3 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.111.0/24 serviceNetwork: - 172.30.0.0/16 provisionRequirements: controlPlaneAgents: 1 #sshPublicKey: ssh-rsa your-public-key-here (optional) EOF Result The cluster is fully defined in assisted-service backend (PostgreSQL DB). No discovery image has been created at this point. AgentClusterInstall resource conditions: kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied RequirementsMet The cluster is not ready to begin the installation Validated The cluster's validations are failing: Single-node clusters must have a single master node and no workers. Completed The installation has not yet started Failed The installation has not failed Stopped The installation is waiting to start or in progress assisted-service log: time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Creating a new cluster single-node demo-worker4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).createNewCluster\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:843\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Stored OCP version: 4.8.0-rc.0\" func=\"github.com/openshift/assisted-service/internal/versions.(*handler).AddOpenshiftVersion\" file=\"/go/src/github.com/openshift/origin/internal/versions/versions.go:240\" pkg=versions time=\"2021-06-28T21:01:31Z\" level=info msg=\"Register cluster: test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:344\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"HA mode is None, setting UserManagedNetworking to true and VipDhcpAllocation to false\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:387\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Successfully registered cluster test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal.func1\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:351\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"no infraEnv for the clusterDeployment single-node in namespace demo-worker4\" func=github.com/openshift/assisted-service/internal/controller/controllers.getInfraEnvByClusterDeployment file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/common.go:67\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"update cluster 02a89bb9-6141-4d14-a82e-42f254217502 with params: &{AdditionalNtpSource:<nil> APIVip:<nil> APIVipDNSName:<nil> BaseDNSDomain:<nil> ClusterNetworkCidr:<nil> ClusterNetworkHostPrefix:<nil> DisksSelectedConfig:[] HostsMachineConfigPoolNames:[] HostsNames:[] HostsRoles:[] HTTPProxy:<nil> HTTPSProxy:<nil> Hyperthreading:<nil> IngressVip:<nil> MachineNetworkCidr:0xc003c5e760 Name:<nil> NoProxy:<nil> OlmOperators:[] PullSecret:<nil> ServiceNetworkCidr:<nil> SSHPublicKey:<nil> UserManagedNetworking:<nil> VipDhcpAllocation:<nil>}\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).updateClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1715\" go-id=684 pkg=Inventory request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"no infraEnv for the clusterDeployment single-node in namespace demo-worker4\" func=github.com/openshift/assisted-service/internal/controller/controllers.getInfraEnvByClusterDeployment file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/common.go:67\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"Updated clusterDeployment demo-worker4/single-node\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).updateIfNeeded\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:714\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b Generate Cluster Discovery Image 1. Optional - Create NMStateConfig Needed for optional host-level network configuration, such as static IP addresses and more. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: NMStateConfig metadata: name: mynmstateconfig namespace: demo-worker4 labels: demo-nmstate-label: some-value # both the label name and value must be included in InfraEnv nmStateConfigLabelSelector matchLabels spec: config: interfaces: - name: eth0 type: ethernet state: up mac-address: 02:00:00:80:12:14 ipv4: enabled: true address: - ip: 192.168.111.30 prefix-length: 24 dhcp: false - name: eth1 type: ethernet state: up mac-address: 02:00:00:80:12:15 ipv4: enabled: true address: - ip: 192.168.140.30 prefix-length: 24 dhcp: false dns-resolver: config: server: - 192.168.126.1 routes: config: - destination: 0.0.0.0/0 next-hop-address: 192.168.111.1 next-hop-interface: eth1 table-id: 254 - destination: 0.0.0.0/0 next-hop-address: 192.168.140.1 next-hop-interface: eth1 table-id: 254 interfaces: - name: \"eth0\" macAddress: \"02:00:00:80:12:14\" - name: \"eth1\" macAddress: \"02:00:00:80:12:15\" EOF 2. Create InfraEnv The InfraEnv resource describes everything about the infrastructure environment that is needed in order to create a discovery ISO. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: InfraEnv metadata: name: myinfraenv namespace: demo-worker4 spec: clusterRef: name: single-node # Use the above created clusterDeployment resource name and namespace namespace: demo-worker4 pullSecretRef: # currently ignored by InfraEnv controller name: pull-secret #sshAuthorizedKey: 'your_pub_key_here' (optional) , this key will allow to connect to machines booted from the discovery ISO. nmStateConfigLabelSelector: matchLabels: demo-nmstate-label: some-value # both the label name and value must match the NMStateConfig label section EOF Result Discovery Image Created. Note that the log does specify that nmStateConfigs were included. InfraEnv resource conditions: kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" myinfraenv ImageCreated Image has been created custom resource isoDownloadURL: kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-28T21:42:19Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A assisted-service log: time=\"2021-06-28T21:42:19Z\" level=info msg=\"InfraEnv Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:81\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"the amount of nmStateConfigs included in the image is: 1\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).ensureISO\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:288\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"prepare image for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).GenerateClusterISOInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:865\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"Successfully uploaded file 02a89bb9-6141-4d14-a82e-42f254217502/discovery.ign\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClient).Upload\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:76\" go-id=654 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"Filesystem '/data/' usage is 3.6%\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClientDecorator).conditionalLog\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:426\" time=\"2021-06-28T21:42:19Z\" level=info msg=\"Creating minimal ISO for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).generateClusterMinimalISO.func1\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1102\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Start configuring static network for 1 hosts\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).GenerateStaticNetworkConfigData\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:45\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Adding NMConnection file <eth0.nmconnection>\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).createNMConnectionFiles\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:128\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Adding NMConnection file <eth1.nmconnection>\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).createNMConnectionFiles\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:128\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Uploading minimal ISO for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).generateClusterMinimalISO\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1118\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Successfully uploaded file discovery-image-02a89bb9-6141-4d14-a82e-42f254217502.iso\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClient).UploadStream\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:159\" go-id=654 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Generated cluster <02a89bb9-6141-4d14-a82e-42f254217502> image with ignition config {\\n \\\"ignition\\\": {\\n \\\"version\\\": \\\"3.1.0\\\"\\n },\\n \\\"passwd\\\": {\\n \\\"users\\\": [\\n *****\\n ]\\n },\\n \\\"systemd\\\": {\\n \\\"units\\\": [{\\n \\\"name\\\": \\\"agent.service\\\",\\n \\\"enabled\\\": true,\\n \\\"contents\\\": \\\"[Service]\\\\nType=simple\\\\nRestart=always\\\\nRestartSec=3\\\\nStartLimitInterval=0\\\\nEnvironment=HTTP_PROXY=\\\\nEnvironment=http_proxy=\\\\nEnvironment=HTTPS_PROXY=\\\\nEnvironment=https_proxy=\\\\nEnvironment=NO_PROXY=\\\\nEnvironment=no_proxy=\\\\nEnvironment=PULL_SECRET_TOKEN=*****\\\\nTimeoutStartSec=180\\\\nExecStartPre=/usr/local/bin/agent-fix-bz1964591 quay.io/ocpmetal/assisted-installer-agent:latest\\\\nExecStartPre=podman run --privileged --rm -v /usr/local/bin:/hostbin quay.io/ocpmetal/assisted-installer-agent:latest cp /usr/bin/agent /hostbin\\\\nExecStart=/usr/local/bin/agent --url https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org --cluster-id 02a89bb9-6141-4d14-a82e-42f254217502 --agent-version quay.io/ocpmetal/assisted-installer-agent:latest --insecure=false --cacert /etc/assisted-service/service-ca-cert.crt\\\\n\\\\n[Unit]\\\\nWants=network-online.target\\\\nAfter=network-online.target\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\\\"\\n },\\n {\\n \\\"name\\\": \\\"selinux.service\\\",\\n \\\"enabled\\\": true,\\n \\\"contents\\\": \\\"[Service]\\\\nType=oneshot\\\\nExecStartPre=checkmodule -M -m -o /root/assisted.mod /root/assisted.te\\\\nExecStartPre=semodule_package -o /root/assisted.pp -m /root/assisted.mod\\\\nExecStart=semodule -i /root/assisted.pp\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\\\"\\n }\\n ]\\n },\\n \\\"storage\\\": {\\n \\\"files\\\": [{\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/usr/local/bin/agent-fix-bz1964591\\\",\\n \\\"mode\\\": 755,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%23%21%2Fusr%2Fbin%2Fsh%0A%0A%23%20This%20script%20is%20a%20workaround%20for%20bugzilla%201964591%20where%20symlinks%20inside%20%2Fvar%2Flib%2Fcontainers%2F%20get%0A%23%20corrupted%20under%20some%20circumstances.%0A%23%0A%23%20In%20order%20to%20let%20agent.service%20start%20correctly%20we%20are%20checking%20here%20whether%20the%20requested%0A%23%20container%20image%20exists%20and%20in%20case%20%22podman%20images%22%20returns%20an%20error%20we%20try%20removing%20the%20faulty%0A%23%20image.%0A%23%0A%23%20In%20such%20a%20scenario%20agent.service%20will%20detect%20the%20image%20is%20not%20present%20and%20pull%20it%20again.%20In%20case%0A%23%20the%20image%20is%20present%20and%20can%20be%20detected%20correctly%2C%20no%20any%20action%20is%20required.%0A%0AIMAGE=$%28echo%20$1%20%7C%20sed%20%27s%2F:.%2A%2F%2F%27%29%0Apodman%20images%20%7C%20grep%20$IMAGE%20%7C%7C%20podman%20rmi%20--force%20$1%20%7C%7C%20true%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/motd\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%0A%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%0AThis%20is%20a%20host%20being%20installed%20by%20the%20OpenShift%20Assisted%20Installer.%0AIt%20will%20be%20installed%20from%20scratch%20during%20the%20installation.%0AThe%20primary%20service%20is%20agent.service.%20%20To%20watch%20its%20status%20run%20e.g%0Asudo%20journalctl%20-u%20agent.service%0A%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/NetworkManager/conf.d/01-ipv6.conf\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%0A%5Bconnection%5D%0Aipv6.dhcp-iaid=mac%0Aipv6.dhcp-duid=ll%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/root/.docker/config.json\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,*****\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/root/assisted.te\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:text/plain;base64,Cm1vZHVsZSBhc3Npc3RlZCAxLjA7CnJlcXVpcmUgewogICAgICAgIHR5cGUgY2hyb255ZF90OwogICAgICAgIHR5cGUgY29udGFpbmVyX2ZpbGVfdDsKICAgICAgICB0eXBlIHNwY190OwogICAgICAgIGNsYXNzIHVuaXhfZGdyYW1fc29ja2V0IHNlbmR0bzsKICAgICAgICBjbGFzcyBkaXIgc2VhcmNoOwogICAgICAgIGNsYXNzIHNvY2tfZmlsZSB3cml0ZTsKfQojPT09PT09PT09PT09PSBjaHJvbnlkX3QgPT09PT09PT09PT09PT0KYWxsb3cgY2hyb255ZF90IGNvbnRhaW5lcl9maWxlX3Q6ZGlyIHNlYXJjaDsKYWxsb3cgY2hyb255ZF90IGNvbnRhaW5lcl9maWxlX3Q6c29ja19maWxlIHdyaXRlOwphbGxvdyBjaHJvbnlkX3Qgc3BjX3Q6dW5peF9kZ3JhbV9zb2NrZXQgc2VuZHRvOwo=\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/pki/ca-trust/source/anchors/rh-it-root-ca.crt\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,*****\\\" }\\n },\\n {\\n \\\"path\\\": \\\"/etc/assisted-service/service-ca-cert.crt\\\",\\n \\\"mode\\\": 420,\\n \\\"overwrite\\\": true,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:text/plain;charset=utf-8;base64,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\\\" }\\n }]\\n }\\n}\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).getIgnitionConfigForLogging\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1064\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=f417920f-897c-49af-ba3e-b467026c94b0 time=\"2021-06-28T21:42:20Z\" level=info msg=\"ISODownloadURL changed from to https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).updateEnsureISOSuccess\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:318\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"InfraEnv Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:78\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"InfraEnv Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:81\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=0d9c0da4-beab-4a41-85a3-4444728dd6f0 time=\"2021-06-28T21:42:20Z\" level=info msg=\"the amount of nmStateConfigs included in the image is: 1\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).ensureISO\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:288\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=0d9c0da4-beab-4a41-85a3-4444728dd6f0 Inspect Assisted Service Cluster Events Get the full event list here: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" At this point, you should expect to see events such as: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:01:31.938Z\", \"message\": \"Registered cluster \\\"test-cluster\\\"\", \"severity\": \"info\" }, { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:01:31.941Z\", \"message\": \"Successfully registered cluster test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\", \"severity\": \"info\" }, { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:42:20.768Z\", \"message\": \"Generated image (Image type is \\\"minimal-iso\\\", SSH public key is not set)\", \"severity\": \"info\" }, Boot The Host From The Discovery Image The procedure described below is called: Boot It Yourself . There is an alternative automated procedure using BMH . For details, check the Zero Touch Provisioning (ZTP) advanced section . 1. Download the image Minimal-ISO is the default image type here; therefore, expect an image at the size of ~100MB. curl --insecure -o image.iso $(kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.status.isoDownloadURL}\") 2. Boot the host from image Use virtual media / USB drive / other methods to boot your host using the discovery image. 3. Host discovery and approval The Agent CRD represents a Host that boot from a discovery image and registered to a cluster, and created automatically by assisted-service. Find more details about the agent CRD here . At first, it will look as follows: kubectl -n demo-worker4 get agents NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster false To trigger the installation, you will need to approve the agent kubectl -n demo-worker4 patch agents.agent-install.openshift.io 07e80ea9-200c-4f82-aff4-4932acb773d4 -p '{\"spec\":{\"approved\":true}}' --type merge You should now see it got approved NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster true 4. Cluster Installation Installation has started, to monitor the progress you may inspect cluster events: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" Another option is to inspect AgentClusterInstall conditions, as mentioned above . 5. End Result Lastly, when installed successfully: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T16:45:21.643Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" } Advanced: Zero Touch Provisioning The procedure described below is called: Zero Touch Provisioning (ZTP) . There is an alternative manual procedure, not involving BMH . For details, check the Boot The Host From The Discovery Image . Utilize BareMetalHost Examples below generated by dev-scripts and were modified manually. Control the host via BMC, which allows a host remote connection and ability to power cycle independently from the host operating system. 1. Create BMC Secret cat <<EOF | kubectl create -f - apiVersion: v1 kind: Secret metadata: name: ostest-extraworker-4-bmc-secret namespace: demo-worker4 type: Opaque data: username: foo # Change me password: bar # Change me EOF 2. Create BareMetalHost cat <<EOF | kubectl create -f - apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: name: ostest-extraworker-4 namespace: demo-worker4 annotations: inspect.metal3.io: disabled labels: infraenvs.agent-install.openshift.io: \"myinfraenv\" # Must match the InfraEnv name and exist in the same namespace spec: online: true automatedCleaningMode: disabled bootMACAddress: 00:cd:16:5c:68:47 # Change to your mac address bmc: address: redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c # Should be changed to match your machine credentialsName: ostest-extraworker-4-bmc-secret # Must match the bmc secret you have just created EOF NOTE We are always setting automatedCleaningMode: disabled even if the BareMetalHost manifest specifies another value (e.g. automatedCleaningMode: metadata ). This may be changed in the future releases, but currently we do not support using Ironic to clean the node. Result Host turned on. Image download started. This might take a while. Host discovery happened. An Agent CR got created automatically. Having issues? try this troubleshooting section virsh status example: Relevant only for deployments that use virtual machines (managed by libvirt) for simulating bare metal hosts. Id Name State --------------------------------------- 5 ostest_master_1 running 6 ostest_master_2 running 7 ostest_master_0 running 10 ostest_worker_1 running 11 ostest_worker_0 running 15 ostest_extraworker_4 running # Our host is now up and running custom resource events: kubectl -n demo-worker4 describe baremetalhosts.metal3.io ostest-extraworker-4 | grep Events\\: -A30 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Registered 19m metal3-baremetal-controller Registered new host Normal BMCAccessValidated 19m metal3-baremetal-controller Verified access to BMC Normal InspectionSkipped 19m metal3-baremetal-controller disabled by annotation Normal ProfileSet 19m metal3-baremetal-controller Hardware profile set: unknown Normal ProvisioningStarted 19m metal3-baremetal-controller Image provisioning started for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A Normal ProvisioningComplete 19m metal3-baremetal-controller Image provisioning completed for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A assisted-service log: time=\"2021-06-29T15:45:19Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580659 1 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil <nil> nil nil nil false disabled} { <nil> <nil> { { <nil>} <nil> <nil>} {nil } {nil } false {{0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Updating dirty BMH &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580659 1 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil <nil> nil nil nil false disabled} { <nil> <nil> { { <nil>} <nil> <nil>} {nil } {nil } false {{0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:152\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a ... time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580764 2 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [baremetalhost.metal3.io] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}} {assisted-service Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:spec\\\":{\\\"f:image\\\":{\\\".\\\":{},\\\"f:format\\\":{},\\\"f:url\\\":{}}}}} {baremetal-operator Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:finalizers\\\":{\\\".\\\":{},\\\"v:\\\\\\\"baremetalhost.metal3.io\\\\\\\"\\\":{}}},\\\"f:status\\\":{\\\".\\\":{},\\\"f:errorCount\\\":{},\\\"f:errorMessage\\\":{},\\\"f:goodCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}},\\\"f:hardwareProfile\\\":{},\\\"f:lastUpdated\\\":{},\\\"f:operationHistory\\\":{\\\".\\\":{},\\\"f:deprovision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:inspect\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:provision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:register\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}}},\\\"f:operationalStatus\\\":{},\\\"f:poweredOn\\\":{},\\\"f:provisioning\\\":{\\\".\\\":{},\\\"f:ID\\\":{},\\\"f:bootMode\\\":{},\\\"f:image\\\":{\\\".\\\":{},\\\"f:url\\\":{}},\\\"f:rootDeviceHints\\\":{\\\".\\\":{},\\\"f:deviceName\\\":{}},\\\"f:state\\\":{}},\\\"f:triedCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil 0xc0013bec40 nil nil nil false disabled} {OK 2021-06-29 15:45:30 +0000 UTC unknown <nil> {ready 5600a851-4a0f-4b9b-b9f7-c4131b63180d { <nil>} 0xc00427fef0 UEFI <nil>} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} false {{2021-06-29 15:45:19 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"Stopping reconcileBMH: Either the InfraEnv image is not ready or there is nothing to update.\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:550\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580765 2 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [baremetalhost.metal3.io] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}} {assisted-service Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:spec\\\":{\\\"f:image\\\":{\\\".\\\":{},\\\"f:format\\\":{},\\\"f:url\\\":{}}}}} {baremetal-operator Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:finalizers\\\":{\\\".\\\":{},\\\"v:\\\\\\\"baremetalhost.metal3.io\\\\\\\"\\\":{}}},\\\"f:status\\\":{\\\".\\\":{},\\\"f:errorCount\\\":{},\\\"f:errorMessage\\\":{},\\\"f:goodCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}},\\\"f:hardwareProfile\\\":{},\\\"f:lastUpdated\\\":{},\\\"f:operationHistory\\\":{\\\".\\\":{},\\\"f:deprovision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:inspect\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:provision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:register\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}}},\\\"f:operationalStatus\\\":{},\\\"f:poweredOn\\\":{},\\\"f:provisioning\\\":{\\\".\\\":{},\\\"f:ID\\\":{},\\\"f:bootMode\\\":{},\\\"f:image\\\":{\\\".\\\":{},\\\"f:url\\\":{}},\\\"f:rootDeviceHints\\\":{\\\".\\\":{},\\\"f:deviceName\\\":{}},\\\"f:state\\\":{}},\\\"f:triedCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil 0xc001068900 nil nil nil false disabled} {OK 2021-06-29 15:45:30 +0000 UTC unknown <nil> {provisioning 5600a851-4a0f-4b9b-b9f7-c4131b63180d { <nil>} 0xc00104acf0 UEFI <nil>} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} false {{2021-06-29 15:45:19 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=info msg=\"Stopping reconcileBMH: Either the InfraEnv image is not ready or there is nothing to update.\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:550\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Request: {GET /api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip, deflate] Forwarded:[for=192.168.111.21;host=assisted-service-assisted-installer.apps.ostest.test.metalkube.org;proto=https] User-Agent:[python-requests/2.20.0] X-Forwarded-For:[192.168.111.21] X-Forwarded-Host:[assisted-service-assisted-installer.apps.ostest.test.metalkube.org] X-Forwarded-Port:[443] X-Forwarded-Proto:[https]] {} <nil> 0 [] false assisted-service-assisted-installer.apps.ostest.test.metalkube.org map[] map[] <nil> map[] 10.131.0.1:42932 /api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A 0xc002f438c0 <nil> <nil> 0xc00170eff0}\" func=github.com/openshift/assisted-service/internal/metrics.Handler.func1 file=\"/go/src/github.com/openshift/origin/internal/metrics/reporter.go:20\" pkg=matched-h time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Authenticating cluster 02a89bb9-6141-4d14-a82e-42f254217502 JWT\" func=\"github.com/openshift/assisted-service/pkg/auth.(*LocalAuthenticator).AuthAgentAuth\" file=\"/go/src/github.com/openshift/origin/pkg/auth/local_authenticator.go:82\" pkg=auth time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Pushing cluster event single-node demo-worker4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*controllerEventsWrapper).AddEvent\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/controller_event_wrapper.go:36\" time=\"2021-06-29T15:45:33Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=9d4b6bae-b3a2-420f-ad4b-12bb4fa3d5da time=\"2021-06-29T15:45:33Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=9d4b6bae-b3a2-420f-ad4b-12bb4fa3d5da Inspect Assisted Service Cluster Events Get the full event list here: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" Expect to see an event that indicate the image download started: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T15:45:33.489Z\", \"message\": \"Started image download (image type is \\\"minimal-iso\\\")\", \"severity\": \"info\" } Agent Discovery and Installation When the host boots from the discovery image, it gets registered to assisted-service. At first it will look like this: kubectl -n demo-worker4 get agents NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster false BMAC will automatically approve the agent, which will start the installation. NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster true Lastly, when installed successfully: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T16:45:21.643Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" }","title":"Kube API - Getting Started Guide"},{"location":"hive-integration/kube-api-getting-started/#kube-api-getting-started-guide","text":"This document is a step-by-step guide that demonstrates how to deploy a single-node cluster end to end. Note : * This document is not meant to expand on each and every resource in detail; other documents already do that . Instead, expect the details needed to understand the context and what is currently happening. The order in which actions are performed here is optional , and you may choose to go by a different one. The one order restriction the API does have is in regards to creating NMstateConfigs (which are optional) before InfraEnv , for more details, check the warning mentioned here .","title":"Kube API - Getting Started Guide"},{"location":"hive-integration/kube-api-getting-started/#what-to-expect","text":"A walk through of custom resource definitions (CRDs) and understand how they relate to each other. A clear understanding of what happened, both in OpenShift / Kubernetes and assisted-service backend per each action described below.","title":"What To Expect"},{"location":"hive-integration/kube-api-getting-started/#how-to-use-this-guide","text":"Make sure that for each step you follow, you understand how and why it is done. You are encouraged to copy and paste the below-mentioned commands to your terminal and see things in action. Note the inline comments in the yaml examples. There is no need to remove those in case you which to copy and create the resources directly in your environment.","title":"How To Use This guide"},{"location":"hive-integration/kube-api-getting-started/#assumptions","text":"You have deployed both the operator and assisted-service by, for example, using these instructions . Alternatively, you may choose to deploy via OLM or as a part of ACM, which is also applicable. Using the discovery image: Boot it yourself : You are able to use the generated discovery image to boot your host. Zero Touch Provisioning (ZTP) : Detailed in the advanced section . Configure BMH / BareMetalHost to automate the host discovery procedure. This document uses dev-scripts for that. read about it here .","title":"Assumptions"},{"location":"hive-integration/kube-api-getting-started/#resource-types-and-relationships","text":"","title":"Resource Types and Relationships"},{"location":"hive-integration/kube-api-getting-started/#cluster-prerequisites","text":"","title":"Cluster Prerequisites"},{"location":"hive-integration/kube-api-getting-started/#1-create-a-namespace-for-your-resources","text":"cat <<EOF | kubectl create -f - apiVersion: v1 kind: Namespace metadata: name: demo-worker4 # Use any name, but note that using 'assisted-installer' is not recommended EOF","title":"1. Create a namespace for your resources"},{"location":"hive-integration/kube-api-getting-started/#2-create-a-pull-secret","text":"Use the secret obtained from console.redhat.com cat <<EOF | kubectl create -f - apiVersion: v1 kind: Secret type: kubernetes.io/dockerconfigjson metadata: name: pull-secret namespace: demo-worker4 stringData: .dockerconfigjson: '{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"your secret here\",\"email\":\"user@example.com\"}}}' EOF","title":"2. Create a Pull Secret"},{"location":"hive-integration/kube-api-getting-started/#3-generate-clusterimageset-to-specify-the-openshift-version-to-be-used","text":"cat <<EOF | kubectl create -f - apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.8.0-rc.0-x86_64 # That version may change over time EOF","title":"3. Generate ClusterImageSet, to specify the OpenShift version to be used."},{"location":"hive-integration/kube-api-getting-started/#creating-cluster-resources","text":"","title":"Creating Cluster Resources"},{"location":"hive-integration/kube-api-getting-started/#define-a-cluster-with-assisted-service","text":"For that, you'll to need create two resources:","title":"Define a cluster with assisted-service"},{"location":"hive-integration/kube-api-getting-started/#1-create-clusterdepoyment","text":"cat <<EOF | kubectl create -f - apiVersion: hive.openshift.io/v1 kind: ClusterDeployment metadata: name: single-node namespace: demo-worker4 spec: baseDomain: hive.example.com clusterInstallRef: group: extensions.hive.openshift.io kind: AgentClusterInstall name: test-agent-cluster-install # Use the same name for AgentClusterInstall version: v1beta1 clusterName: test-cluster controlPlaneConfig: servingCertificates: {} platform: agentBareMetal: agentSelector: matchLabels: bla: aaa pullSecretRef: name: pull-secret # Use the pull secret name mentioned in Cluster Prerequisites step 2 EOF","title":"1. Create ClusterDepoyment"},{"location":"hive-integration/kube-api-getting-started/#result","text":"At this point, there is no AgentClusterInstall , so it won't do much and won't even register the cluster.","title":"Result"},{"location":"hive-integration/kube-api-getting-started/#clusterdeployment-resource-conditions","text":"kubectl -n demo-worker4 get clusterdeployments.hive.openshift.io single-node -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" single-node RelocationFailed no ClusterRelocates match Hibernating Condition Initialized AuthenticationFailure Platform credentials passed authentication check","title":"ClusterDeployment resource conditions:"},{"location":"hive-integration/kube-api-getting-started/#assisted-service-log","text":"time=\"2021-06-28T20:45:07Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789 time=\"2021-06-28T20:45:07Z\" level=info msg=\"AgentClusterInstall does not exist for ClusterDeployment single-node\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:147\" AgentClusterInstall=test-agent-cluster-install cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789 time=\"2021-06-28T20:45:07Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=b5022e40-4a1e-4658-ba0a-5159938b7789","title":"assisted-service log:"},{"location":"hive-integration/kube-api-getting-started/#2-create-agentclusterinstal","text":"cat <<EOF | kubectl create -f - apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: name: test-agent-cluster-install namespace: demo-worker4 spec: clusterDeploymentRef: name: single-node # Use the clusterDeployment name from the previous step imageSetRef: name: openshift-v4.8.0 # Use the ClusterImageSet name mentioned in Cluster Prerequisites step 3 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.111.0/24 serviceNetwork: - 172.30.0.0/16 provisionRequirements: controlPlaneAgents: 1 #sshPublicKey: ssh-rsa your-public-key-here (optional) EOF","title":"2. Create AgentClusterInstal"},{"location":"hive-integration/kube-api-getting-started/#result_1","text":"The cluster is fully defined in assisted-service backend (PostgreSQL DB). No discovery image has been created at this point.","title":"Result"},{"location":"hive-integration/kube-api-getting-started/#agentclusterinstall-resource-conditions","text":"kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied RequirementsMet The cluster is not ready to begin the installation Validated The cluster's validations are failing: Single-node clusters must have a single master node and no workers. Completed The installation has not yet started Failed The installation has not failed Stopped The installation is waiting to start or in progress","title":"AgentClusterInstall resource conditions:"},{"location":"hive-integration/kube-api-getting-started/#assisted-service-log_1","text":"time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Creating a new cluster single-node demo-worker4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).createNewCluster\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:843\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Stored OCP version: 4.8.0-rc.0\" func=\"github.com/openshift/assisted-service/internal/versions.(*handler).AddOpenshiftVersion\" file=\"/go/src/github.com/openshift/origin/internal/versions/versions.go:240\" pkg=versions time=\"2021-06-28T21:01:31Z\" level=info msg=\"Register cluster: test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:344\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"HA mode is None, setting UserManagedNetworking to true and VipDhcpAllocation to false\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:387\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"Successfully registered cluster test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).RegisterClusterInternal.func1\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:351\" cluster_id=02a89bb9-6141-4d14-a82e-42f254217502 go-id=684 pkg=Inventory request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"no infraEnv for the clusterDeployment single-node in namespace demo-worker4\" func=github.com/openshift/assisted-service/internal/controller/controllers.getInfraEnvByClusterDeployment file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/common.go:67\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=562c4762-532a-4a2a-b99f-53fa57d0b966 time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"update cluster 02a89bb9-6141-4d14-a82e-42f254217502 with params: &{AdditionalNtpSource:<nil> APIVip:<nil> APIVipDNSName:<nil> BaseDNSDomain:<nil> ClusterNetworkCidr:<nil> ClusterNetworkHostPrefix:<nil> DisksSelectedConfig:[] HostsMachineConfigPoolNames:[] HostsNames:[] HostsRoles:[] HTTPProxy:<nil> HTTPSProxy:<nil> Hyperthreading:<nil> IngressVip:<nil> MachineNetworkCidr:0xc003c5e760 Name:<nil> NoProxy:<nil> OlmOperators:[] PullSecret:<nil> ServiceNetworkCidr:<nil> SSHPublicKey:<nil> UserManagedNetworking:<nil> VipDhcpAllocation:<nil>}\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).updateClusterInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1715\" go-id=684 pkg=Inventory request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"no infraEnv for the clusterDeployment single-node in namespace demo-worker4\" func=github.com/openshift/assisted-service/internal/controller/controllers.getInfraEnvByClusterDeployment file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/common.go:67\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"Updated clusterDeployment demo-worker4/single-node\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).updateIfNeeded\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:714\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b time=\"2021-06-28T21:01:31Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=98cac786-f3c2-4237-ae7c-f5c1a9e6e85b","title":"assisted-service log:"},{"location":"hive-integration/kube-api-getting-started/#generate-cluster-discovery-image","text":"","title":"Generate Cluster Discovery Image"},{"location":"hive-integration/kube-api-getting-started/#1-optional-create-nmstateconfig","text":"Needed for optional host-level network configuration, such as static IP addresses and more. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: NMStateConfig metadata: name: mynmstateconfig namespace: demo-worker4 labels: demo-nmstate-label: some-value # both the label name and value must be included in InfraEnv nmStateConfigLabelSelector matchLabels spec: config: interfaces: - name: eth0 type: ethernet state: up mac-address: 02:00:00:80:12:14 ipv4: enabled: true address: - ip: 192.168.111.30 prefix-length: 24 dhcp: false - name: eth1 type: ethernet state: up mac-address: 02:00:00:80:12:15 ipv4: enabled: true address: - ip: 192.168.140.30 prefix-length: 24 dhcp: false dns-resolver: config: server: - 192.168.126.1 routes: config: - destination: 0.0.0.0/0 next-hop-address: 192.168.111.1 next-hop-interface: eth1 table-id: 254 - destination: 0.0.0.0/0 next-hop-address: 192.168.140.1 next-hop-interface: eth1 table-id: 254 interfaces: - name: \"eth0\" macAddress: \"02:00:00:80:12:14\" - name: \"eth1\" macAddress: \"02:00:00:80:12:15\" EOF","title":"1. Optional - Create NMStateConfig"},{"location":"hive-integration/kube-api-getting-started/#2-create-infraenv","text":"The InfraEnv resource describes everything about the infrastructure environment that is needed in order to create a discovery ISO. cat <<EOF | kubectl create -f - apiVersion: agent-install.openshift.io/v1beta1 kind: InfraEnv metadata: name: myinfraenv namespace: demo-worker4 spec: clusterRef: name: single-node # Use the above created clusterDeployment resource name and namespace namespace: demo-worker4 pullSecretRef: # currently ignored by InfraEnv controller name: pull-secret #sshAuthorizedKey: 'your_pub_key_here' (optional) , this key will allow to connect to machines booted from the discovery ISO. nmStateConfigLabelSelector: matchLabels: demo-nmstate-label: some-value # both the label name and value must match the NMStateConfig label section EOF","title":"2. Create InfraEnv"},{"location":"hive-integration/kube-api-getting-started/#result_2","text":"Discovery Image Created. Note that the log does specify that nmStateConfigs were included.","title":"Result"},{"location":"hive-integration/kube-api-getting-started/#infraenv-resource-conditions","text":"kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" myinfraenv ImageCreated Image has been created","title":"InfraEnv resource conditions:"},{"location":"hive-integration/kube-api-getting-started/#custom-resource-isodownloadurl","text":"kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-28T21:42:19Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A","title":"custom resource isoDownloadURL:"},{"location":"hive-integration/kube-api-getting-started/#assisted-service-log_2","text":"time=\"2021-06-28T21:42:19Z\" level=info msg=\"InfraEnv Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:81\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"the amount of nmStateConfigs included in the image is: 1\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).ensureISO\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:288\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"prepare image for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).GenerateClusterISOInternal\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:865\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"Successfully uploaded file 02a89bb9-6141-4d14-a82e-42f254217502/discovery.ign\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClient).Upload\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:76\" go-id=654 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:19Z\" level=info msg=\"Filesystem '/data/' usage is 3.6%\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClientDecorator).conditionalLog\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:426\" time=\"2021-06-28T21:42:19Z\" level=info msg=\"Creating minimal ISO for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).generateClusterMinimalISO.func1\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1102\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Start configuring static network for 1 hosts\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).GenerateStaticNetworkConfigData\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:45\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Adding NMConnection file <eth0.nmconnection>\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).createNMConnectionFiles\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:128\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Adding NMConnection file <eth1.nmconnection>\" func=\"github.com/openshift/assisted-service/pkg/staticnetworkconfig.(*StaticNetworkConfigGenerator).createNMConnectionFiles\" file=\"/go/src/github.com/openshift/origin/pkg/staticnetworkconfig/generator.go:128\" pkg=static_network_config time=\"2021-06-28T21:42:20Z\" level=info msg=\"Uploading minimal ISO for cluster 02a89bb9-6141-4d14-a82e-42f254217502\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).generateClusterMinimalISO\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1118\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Successfully uploaded file discovery-image-02a89bb9-6141-4d14-a82e-42f254217502.iso\" func=\"github.com/openshift/assisted-service/pkg/s3wrapper.(*FSClient).UploadStream\" file=\"/go/src/github.com/openshift/origin/pkg/s3wrapper/filesystem.go:159\" go-id=654 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"Generated cluster <02a89bb9-6141-4d14-a82e-42f254217502> image with ignition config {\\n \\\"ignition\\\": {\\n \\\"version\\\": \\\"3.1.0\\\"\\n },\\n \\\"passwd\\\": {\\n \\\"users\\\": [\\n *****\\n ]\\n },\\n \\\"systemd\\\": {\\n \\\"units\\\": [{\\n \\\"name\\\": \\\"agent.service\\\",\\n \\\"enabled\\\": true,\\n \\\"contents\\\": \\\"[Service]\\\\nType=simple\\\\nRestart=always\\\\nRestartSec=3\\\\nStartLimitInterval=0\\\\nEnvironment=HTTP_PROXY=\\\\nEnvironment=http_proxy=\\\\nEnvironment=HTTPS_PROXY=\\\\nEnvironment=https_proxy=\\\\nEnvironment=NO_PROXY=\\\\nEnvironment=no_proxy=\\\\nEnvironment=PULL_SECRET_TOKEN=*****\\\\nTimeoutStartSec=180\\\\nExecStartPre=/usr/local/bin/agent-fix-bz1964591 quay.io/ocpmetal/assisted-installer-agent:latest\\\\nExecStartPre=podman run --privileged --rm -v /usr/local/bin:/hostbin quay.io/ocpmetal/assisted-installer-agent:latest cp /usr/bin/agent /hostbin\\\\nExecStart=/usr/local/bin/agent --url https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org --cluster-id 02a89bb9-6141-4d14-a82e-42f254217502 --agent-version quay.io/ocpmetal/assisted-installer-agent:latest --insecure=false --cacert /etc/assisted-service/service-ca-cert.crt\\\\n\\\\n[Unit]\\\\nWants=network-online.target\\\\nAfter=network-online.target\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\\\"\\n },\\n {\\n \\\"name\\\": \\\"selinux.service\\\",\\n \\\"enabled\\\": true,\\n \\\"contents\\\": \\\"[Service]\\\\nType=oneshot\\\\nExecStartPre=checkmodule -M -m -o /root/assisted.mod /root/assisted.te\\\\nExecStartPre=semodule_package -o /root/assisted.pp -m /root/assisted.mod\\\\nExecStart=semodule -i /root/assisted.pp\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\\\"\\n }\\n ]\\n },\\n \\\"storage\\\": {\\n \\\"files\\\": [{\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/usr/local/bin/agent-fix-bz1964591\\\",\\n \\\"mode\\\": 755,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%23%21%2Fusr%2Fbin%2Fsh%0A%0A%23%20This%20script%20is%20a%20workaround%20for%20bugzilla%201964591%20where%20symlinks%20inside%20%2Fvar%2Flib%2Fcontainers%2F%20get%0A%23%20corrupted%20under%20some%20circumstances.%0A%23%0A%23%20In%20order%20to%20let%20agent.service%20start%20correctly%20we%20are%20checking%20here%20whether%20the%20requested%0A%23%20container%20image%20exists%20and%20in%20case%20%22podman%20images%22%20returns%20an%20error%20we%20try%20removing%20the%20faulty%0A%23%20image.%0A%23%0A%23%20In%20such%20a%20scenario%20agent.service%20will%20detect%20the%20image%20is%20not%20present%20and%20pull%20it%20again.%20In%20case%0A%23%20the%20image%20is%20present%20and%20can%20be%20detected%20correctly%2C%20no%20any%20action%20is%20required.%0A%0AIMAGE=$%28echo%20$1%20%7C%20sed%20%27s%2F:.%2A%2F%2F%27%29%0Apodman%20images%20%7C%20grep%20$IMAGE%20%7C%7C%20podman%20rmi%20--force%20$1%20%7C%7C%20true%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/motd\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%0A%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%0AThis%20is%20a%20host%20being%20installed%20by%20the%20OpenShift%20Assisted%20Installer.%0AIt%20will%20be%20installed%20from%20scratch%20during%20the%20installation.%0AThe%20primary%20service%20is%20agent.service.%20%20To%20watch%20its%20status%20run%20e.g%0Asudo%20journalctl%20-u%20agent.service%0A%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%20%20%2A%2A%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/NetworkManager/conf.d/01-ipv6.conf\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,%0A%5Bconnection%5D%0Aipv6.dhcp-iaid=mac%0Aipv6.dhcp-duid=ll%0A\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/root/.docker/config.json\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,*****\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/root/assisted.te\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:text/plain;base64,Cm1vZHVsZSBhc3Npc3RlZCAxLjA7CnJlcXVpcmUgewogICAgICAgIHR5cGUgY2hyb255ZF90OwogICAgICAgIHR5cGUgY29udGFpbmVyX2ZpbGVfdDsKICAgICAgICB0eXBlIHNwY190OwogICAgICAgIGNsYXNzIHVuaXhfZGdyYW1fc29ja2V0IHNlbmR0bzsKICAgICAgICBjbGFzcyBkaXIgc2VhcmNoOwogICAgICAgIGNsYXNzIHNvY2tfZmlsZSB3cml0ZTsKfQojPT09PT09PT09PT09PSBjaHJvbnlkX3QgPT09PT09PT09PT09PT0KYWxsb3cgY2hyb255ZF90IGNvbnRhaW5lcl9maWxlX3Q6ZGlyIHNlYXJjaDsKYWxsb3cgY2hyb255ZF90IGNvbnRhaW5lcl9maWxlX3Q6c29ja19maWxlIHdyaXRlOwphbGxvdyBjaHJvbnlkX3Qgc3BjX3Q6dW5peF9kZ3JhbV9zb2NrZXQgc2VuZHRvOwo=\\\" }\\n },\\n {\\n \\\"overwrite\\\": true,\\n \\\"path\\\": \\\"/etc/pki/ca-trust/source/anchors/rh-it-root-ca.crt\\\",\\n \\\"mode\\\": 420,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:,*****\\\" }\\n },\\n {\\n \\\"path\\\": \\\"/etc/assisted-service/service-ca-cert.crt\\\",\\n \\\"mode\\\": 420,\\n \\\"overwrite\\\": true,\\n \\\"user\\\": {\\n \\\"name\\\": \\\"root\\\"\\n },\\n \\\"contents\\\": { \\\"source\\\": \\\"data:text/plain;charset=utf-8;base64,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\\\" }\\n }]\\n }\\n}\" func=\"github.com/openshift/assisted-service/internal/bminventory.(*bareMetalInventory).getIgnitionConfigForLogging\" file=\"/go/src/github.com/openshift/origin/internal/bminventory/inventory.go:1064\" go-id=654 pkg=Inventory request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=f417920f-897c-49af-ba3e-b467026c94b0 time=\"2021-06-28T21:42:20Z\" level=info msg=\"ISODownloadURL changed from to https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).updateEnsureISOSuccess\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:318\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"InfraEnv Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:78\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=5ad649fd-bf49-4b41-aa33-c35a601c1555 time=\"2021-06-28T21:42:20Z\" level=info msg=\"InfraEnv Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:81\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=0d9c0da4-beab-4a41-85a3-4444728dd6f0 time=\"2021-06-28T21:42:20Z\" level=info msg=\"the amount of nmStateConfigs included in the image is: 1\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*InfraEnvReconciler).ensureISO\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/infraenv_controller.go:288\" go-id=654 infra_env=myinfraenv infra_env_namespace=demo-worker4 request_id=0d9c0da4-beab-4a41-85a3-4444728dd6f0","title":"assisted-service log:"},{"location":"hive-integration/kube-api-getting-started/#inspect-assisted-service-cluster-events","text":"Get the full event list here: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" At this point, you should expect to see events such as: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:01:31.938Z\", \"message\": \"Registered cluster \\\"test-cluster\\\"\", \"severity\": \"info\" }, { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:01:31.941Z\", \"message\": \"Successfully registered cluster test-cluster with id 02a89bb9-6141-4d14-a82e-42f254217502\", \"severity\": \"info\" }, { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-28T21:42:20.768Z\", \"message\": \"Generated image (Image type is \\\"minimal-iso\\\", SSH public key is not set)\", \"severity\": \"info\" },","title":"Inspect Assisted Service Cluster Events"},{"location":"hive-integration/kube-api-getting-started/#boot-the-host-from-the-discovery-image","text":"The procedure described below is called: Boot It Yourself . There is an alternative automated procedure using BMH . For details, check the Zero Touch Provisioning (ZTP) advanced section .","title":"Boot The Host From The Discovery Image"},{"location":"hive-integration/kube-api-getting-started/#1-download-the-image","text":"Minimal-ISO is the default image type here; therefore, expect an image at the size of ~100MB. curl --insecure -o image.iso $(kubectl -n demo-worker4 get infraenvs.agent-install.openshift.io myinfraenv -o=jsonpath=\"{.status.isoDownloadURL}\")","title":"1. Download the image"},{"location":"hive-integration/kube-api-getting-started/#2-boot-the-host-from-image","text":"Use virtual media / USB drive / other methods to boot your host using the discovery image.","title":"2. Boot the host from image"},{"location":"hive-integration/kube-api-getting-started/#3-host-discovery-and-approval","text":"The Agent CRD represents a Host that boot from a discovery image and registered to a cluster, and created automatically by assisted-service. Find more details about the agent CRD here . At first, it will look as follows: kubectl -n demo-worker4 get agents NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster false To trigger the installation, you will need to approve the agent kubectl -n demo-worker4 patch agents.agent-install.openshift.io 07e80ea9-200c-4f82-aff4-4932acb773d4 -p '{\"spec\":{\"approved\":true}}' --type merge You should now see it got approved NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster true","title":"3. Host discovery and approval"},{"location":"hive-integration/kube-api-getting-started/#4-cluster-installation","text":"Installation has started, to monitor the progress you may inspect cluster events: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" Another option is to inspect AgentClusterInstall conditions, as mentioned above .","title":"4. Cluster Installation"},{"location":"hive-integration/kube-api-getting-started/#5-end-result","text":"Lastly, when installed successfully: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T16:45:21.643Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" }","title":"5. End Result"},{"location":"hive-integration/kube-api-getting-started/#advanced-zero-touch-provisioning","text":"The procedure described below is called: Zero Touch Provisioning (ZTP) . There is an alternative manual procedure, not involving BMH . For details, check the Boot The Host From The Discovery Image .","title":"Advanced: Zero Touch Provisioning"},{"location":"hive-integration/kube-api-getting-started/#utilize-baremetalhost","text":"Examples below generated by dev-scripts and were modified manually. Control the host via BMC, which allows a host remote connection and ability to power cycle independently from the host operating system.","title":"Utilize BareMetalHost"},{"location":"hive-integration/kube-api-getting-started/#1-create-bmc-secret","text":"cat <<EOF | kubectl create -f - apiVersion: v1 kind: Secret metadata: name: ostest-extraworker-4-bmc-secret namespace: demo-worker4 type: Opaque data: username: foo # Change me password: bar # Change me EOF","title":"1. Create BMC Secret"},{"location":"hive-integration/kube-api-getting-started/#2-create-baremetalhost","text":"cat <<EOF | kubectl create -f - apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: name: ostest-extraworker-4 namespace: demo-worker4 annotations: inspect.metal3.io: disabled labels: infraenvs.agent-install.openshift.io: \"myinfraenv\" # Must match the InfraEnv name and exist in the same namespace spec: online: true automatedCleaningMode: disabled bootMACAddress: 00:cd:16:5c:68:47 # Change to your mac address bmc: address: redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c # Should be changed to match your machine credentialsName: ostest-extraworker-4-bmc-secret # Must match the bmc secret you have just created EOF NOTE We are always setting automatedCleaningMode: disabled even if the BareMetalHost manifest specifies another value (e.g. automatedCleaningMode: metadata ). This may be changed in the future releases, but currently we do not support using Ironic to clean the node.","title":"2. Create BareMetalHost"},{"location":"hive-integration/kube-api-getting-started/#result_3","text":"Host turned on. Image download started. This might take a while. Host discovery happened. An Agent CR got created automatically. Having issues? try this troubleshooting section","title":"Result"},{"location":"hive-integration/kube-api-getting-started/#virsh-status-example","text":"Relevant only for deployments that use virtual machines (managed by libvirt) for simulating bare metal hosts. Id Name State --------------------------------------- 5 ostest_master_1 running 6 ostest_master_2 running 7 ostest_master_0 running 10 ostest_worker_1 running 11 ostest_worker_0 running 15 ostest_extraworker_4 running # Our host is now up and running","title":"virsh status example:"},{"location":"hive-integration/kube-api-getting-started/#custom-resource-events","text":"kubectl -n demo-worker4 describe baremetalhosts.metal3.io ostest-extraworker-4 | grep Events\\: -A30 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Registered 19m metal3-baremetal-controller Registered new host Normal BMCAccessValidated 19m metal3-baremetal-controller Verified access to BMC Normal InspectionSkipped 19m metal3-baremetal-controller disabled by annotation Normal ProfileSet 19m metal3-baremetal-controller Hardware profile set: unknown Normal ProvisioningStarted 19m metal3-baremetal-controller Image provisioning started for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A Normal ProvisioningComplete 19m metal3-baremetal-controller Image provisioning completed for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A","title":"custom resource events:"},{"location":"hive-integration/kube-api-getting-started/#assisted-service-log_3","text":"time=\"2021-06-29T15:45:19Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580659 1 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil <nil> nil nil nil false disabled} { <nil> <nil> { { <nil>} <nil> <nil>} {nil } {nil } false {{0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=debug msg=\"Updating dirty BMH &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580659 1 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil <nil> nil nil nil false disabled} { <nil> <nil> { { <nil>} <nil> <nil>} {nil } {nil } false {{0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:152\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a time=\"2021-06-29T15:45:19Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=e526f6a1-77a1-4aea-8331-4872dbfe3b4a ... time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580764 2 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [baremetalhost.metal3.io] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}} {assisted-service Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:spec\\\":{\\\"f:image\\\":{\\\".\\\":{},\\\"f:format\\\":{},\\\"f:url\\\":{}}}}} {baremetal-operator Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:finalizers\\\":{\\\".\\\":{},\\\"v:\\\\\\\"baremetalhost.metal3.io\\\\\\\"\\\":{}}},\\\"f:status\\\":{\\\".\\\":{},\\\"f:errorCount\\\":{},\\\"f:errorMessage\\\":{},\\\"f:goodCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}},\\\"f:hardwareProfile\\\":{},\\\"f:lastUpdated\\\":{},\\\"f:operationHistory\\\":{\\\".\\\":{},\\\"f:deprovision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:inspect\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:provision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:register\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}}},\\\"f:operationalStatus\\\":{},\\\"f:poweredOn\\\":{},\\\"f:provisioning\\\":{\\\".\\\":{},\\\"f:ID\\\":{},\\\"f:bootMode\\\":{},\\\"f:image\\\":{\\\".\\\":{},\\\"f:url\\\":{}},\\\"f:rootDeviceHints\\\":{\\\".\\\":{},\\\"f:deviceName\\\":{}},\\\"f:state\\\":{}},\\\"f:triedCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil 0xc0013bec40 nil nil nil false disabled} {OK 2021-06-29 15:45:30 +0000 UTC unknown <nil> {ready 5600a851-4a0f-4b9b-b9f7-c4131b63180d { <nil>} 0xc00427fef0 UEFI <nil>} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} false {{2021-06-29 15:45:19 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"Stopping reconcileBMH: Either the InfraEnv image is not ready or there is nothing to update.\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:550\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=aae0e368-3e1b-44d4-a4df-047080e7cd18 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:139\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Started BMH reconcile for demo-worker4/ostest-extraworker-4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:503\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH value &{{BareMetalHost metal3.io/v1alpha1} {ostest-extraworker-4 demo-worker4 261f7856-e01c-48cb-9b6c-cd6873bf38be 4580765 2 2021-06-29 15:45:19 +0000 UTC <nil> <nil> map[infraenvs.agent-install.openshift.io:myinfraenv] map[inspect.metal3.io:disabled] [] [baremetalhost.metal3.io] [{kubectl-create Update metal3.io/v1alpha1 2021-06-29 15:45:19 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:annotations\\\":{\\\".\\\":{},\\\"f:inspect.metal3.io\\\":{}},\\\"f:labels\\\":{\\\".\\\":{},\\\"f:infraenvs.agent-install.openshift.io\\\":{}}},\\\"f:spec\\\":{\\\".\\\":{},\\\"f:automatedCleaningMode\\\":{},\\\"f:bmc\\\":{\\\".\\\":{},\\\"f:address\\\":{},\\\"f:credentialsName\\\":{}},\\\"f:bootMACAddress\\\":{},\\\"f:online\\\":{}}}} {assisted-service Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:spec\\\":{\\\"f:image\\\":{\\\".\\\":{},\\\"f:format\\\":{},\\\"f:url\\\":{}}}}} {baremetal-operator Update metal3.io/v1alpha1 2021-06-29 15:45:30 +0000 UTC FieldsV1 {\\\"f:metadata\\\":{\\\"f:finalizers\\\":{\\\".\\\":{},\\\"v:\\\\\\\"baremetalhost.metal3.io\\\\\\\"\\\":{}}},\\\"f:status\\\":{\\\".\\\":{},\\\"f:errorCount\\\":{},\\\"f:errorMessage\\\":{},\\\"f:goodCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}},\\\"f:hardwareProfile\\\":{},\\\"f:lastUpdated\\\":{},\\\"f:operationHistory\\\":{\\\".\\\":{},\\\"f:deprovision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:inspect\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:provision\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}},\\\"f:register\\\":{\\\".\\\":{},\\\"f:end\\\":{},\\\"f:start\\\":{}}},\\\"f:operationalStatus\\\":{},\\\"f:poweredOn\\\":{},\\\"f:provisioning\\\":{\\\".\\\":{},\\\"f:ID\\\":{},\\\"f:bootMode\\\":{},\\\"f:image\\\":{\\\".\\\":{},\\\"f:url\\\":{}},\\\"f:rootDeviceHints\\\":{\\\".\\\":{},\\\"f:deviceName\\\":{}},\\\"f:state\\\":{}},\\\"f:triedCredentials\\\":{\\\".\\\":{},\\\"f:credentials\\\":{\\\".\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{}},\\\"f:credentialsVersion\\\":{}}}}}]} {[] {redfish-virtualmedia+http://192.168.111.1:8000/redfish/v1/Systems/dd8071d3-4ab5-47c1-aa1d-b2f1e9db0d0c ostest-extraworker-4-bmc-secret false} <nil> <nil> 00:cd:16:5c:68:47 true nil 0xc001068900 nil nil nil false disabled} {OK 2021-06-29 15:45:30 +0000 UTC unknown <nil> {provisioning 5600a851-4a0f-4b9b-b9f7-c4131b63180d { <nil>} 0xc00104acf0 UEFI <nil>} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} {&SecretReference{Name:ostest-extraworker-4-bmc-secret,Namespace:demo-worker4,} 4580662} false {{2021-06-29 15:45:19 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 2021-06-29 15:45:30 +0000 UTC} {2021-06-29 15:45:30 +0000 UTC 0001-01-01 00:00:00 +0000 UTC} {0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC}} 0}}\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:504\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"BMH label infraenvs.agent-install.openshift.io value myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:472\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=debug msg=\"Loading InfraEnv myinfraenv\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).findInfraEnvForBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:479\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=info msg=\"Stopping reconcileBMH: Either the InfraEnv image is not ready or there is nothing to update.\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).reconcileBMH\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:550\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:30Z\" level=info msg=\"BareMetalHost Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*BMACReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/bmh_agent_controller.go:136\" bare_metal_host=ostest-extraworker-4 bare_metal_host_namespace=demo-worker4 go-id=655 request_id=6eb8ccc7-80a6-49ea-8fa7-280489a233a7 time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Request: {GET /api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip, deflate] Forwarded:[for=192.168.111.21;host=assisted-service-assisted-installer.apps.ostest.test.metalkube.org;proto=https] User-Agent:[python-requests/2.20.0] X-Forwarded-For:[192.168.111.21] X-Forwarded-Host:[assisted-service-assisted-installer.apps.ostest.test.metalkube.org] X-Forwarded-Port:[443] X-Forwarded-Proto:[https]] {} <nil> 0 [] false assisted-service-assisted-installer.apps.ostest.test.metalkube.org map[] map[] <nil> map[] 10.131.0.1:42932 /api/assisted-install/v1/clusters/02a89bb9-6141-4d14-a82e-42f254217502/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMDJhODliYjktNjE0MS00ZDE0LWE4MmUtNDJmMjU0MjE3NTAyIn0.ciL2w3g89ftu-R-Z3-KwrDtlll2kd4EMhbrE3YLZkQNQBEdCJ1gg5Sjgjfj4Ekbi7C3XsDEsIRSYTGJPL-Pu8A 0xc002f438c0 <nil> <nil> 0xc00170eff0}\" func=github.com/openshift/assisted-service/internal/metrics.Handler.func1 file=\"/go/src/github.com/openshift/origin/internal/metrics/reporter.go:20\" pkg=matched-h time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Authenticating cluster 02a89bb9-6141-4d14-a82e-42f254217502 JWT\" func=\"github.com/openshift/assisted-service/pkg/auth.(*LocalAuthenticator).AuthAgentAuth\" file=\"/go/src/github.com/openshift/origin/pkg/auth/local_authenticator.go:82\" pkg=auth time=\"2021-06-29T15:45:33Z\" level=debug msg=\"Pushing cluster event single-node demo-worker4\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*controllerEventsWrapper).AddEvent\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/controller_event_wrapper.go:36\" time=\"2021-06-29T15:45:33Z\" level=info msg=\"ClusterDeployment Reconcile started\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:115\" cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=9d4b6bae-b3a2-420f-ad4b-12bb4fa3d5da time=\"2021-06-29T15:45:33Z\" level=info msg=\"ClusterDeployment Reconcile ended\" func=\"github.com/openshift/assisted-service/internal/controller/controllers.(*ClusterDeploymentsReconciler).Reconcile.func1\" file=\"/go/src/github.com/openshift/origin/internal/controller/controllers/clusterdeployments_controller.go:112\" agent_cluster_install=test-agent-cluster-install agent_cluster_install_namespace=demo-worker4 cluster_deployment=single-node cluster_deployment_namespace=demo-worker4 go-id=684 request_id=9d4b6bae-b3a2-420f-ad4b-12bb4fa3d5da","title":"assisted-service log:"},{"location":"hive-integration/kube-api-getting-started/#inspect-assisted-service-cluster-events_1","text":"Get the full event list here: curl -s -k $(kubectl -n demo-worker4 get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" Expect to see an event that indicate the image download started: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T15:45:33.489Z\", \"message\": \"Started image download (image type is \\\"minimal-iso\\\")\", \"severity\": \"info\" }","title":"Inspect Assisted Service Cluster Events"},{"location":"hive-integration/kube-api-getting-started/#agent-discovery-and-installation","text":"When the host boots from the discovery image, it gets registered to assisted-service. At first it will look like this: kubectl -n demo-worker4 get agents NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster false BMAC will automatically approve the agent, which will start the installation. NAME CLUSTER APPROVED 07e80ea9-200c-4f82-aff4-4932acb773d4 test-cluster true Lastly, when installed successfully: { \"cluster_id\": \"02a89bb9-6141-4d14-a82e-42f254217502\", \"event_time\": \"2021-06-29T16:45:21.643Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" }","title":"Agent Discovery and Installation"},{"location":"hive-integration/kube-api-install-retry/","text":"Hive Integration - Retry to Install a cluster In a case where the cluster installation has failed, the user may want to restart the installation. A failed install may happen for multiple reasons; make sure you understand the root cause before trying again. To restart the installation, the user will need to: 1. Delete the cluster AgentClusterInstall resource. 2. Delete all the cluster BareMetalHost resources (a single resource in the case of SNO). 3. Recreate the AgentClusterInstall resource that was deleted in step 1 4. Recreate the BareMetalHost resources that were deleted in step 2 :warning: Note: Recreating AgentClusterInstall will de-register and re-register the backend cluster, which will trigger a discovery image generation for InfraEnv . Baremetal Agent Controller (a.k.a BMAC ) is inspecting InfraEnv for any changes to status.isoDownloadURL , and will pick up the newly generated discovery image. If you boot your machines in other methods (boot it yourself), make sure you use the new image for that. This document will capture the changes before (failed install) and after (resources recreated) to demonstrate this change to the image, but you won't have to do that when you reattempt the installation. Baseline How may a failed installation look? For that look at the AgentClusterInstall conditions: $ kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied Validated The cluster's validations are passing RequirementsMet The cluster installation stopped Completed The installation has failed: cluster has hosts in error Failed The installation failed: cluster has hosts in error Stopped The installation has stopped due to error Capture current discovery image URL Expect URLs to match. InfraEnv $ kubectl -n test-namespace get infraenvs.agent-install.openshift.io test-infraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-23T14:24:57Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/2748ddac-0ac9-489b-a38c-ce0d29d22b02/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjc0OGRkYWMtMGFjOS00ODliLWEzOGMtY2UwZDI5ZDIyYjAyIn0.L67oWuxClinXtCiqRcieOS4vAJCFNVztAE_A2TYnBYJawhAox6NfiuxUih2TKwZxbNVCOwLdQXt_5rjYL6Xn5g BareMetalHost $ kubectl -n test-namespace get baremetalhosts.metal3.io ostest-extraworker-3 -o=jsonpath=\"{.spec.image.url}{'\\n'}\" https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/2748ddac-0ac9-489b-a38c-ce0d29d22b02/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjc0OGRkYWMtMGFjOS00ODliLWEzOGMtY2UwZDI5ZDIyYjAyIn0.L67oWuxClinXtCiqRcieOS4vAJCFNVztAE_A2TYnBYJawhAox6NfiuxUih2TKwZxbNVCOwLdQXt_5rjYL6Xn5g Delete and Recreate As mentioned above, Delete both AgentClusterInstall and all the cluster BareMetalHost resources. Create AgentClusterInstall The InfraEnv controller: 3.1. Gets notified for a successful backend cluster registration. 3.2. Reconcile and send a request for the backend to generate a discovery image. Inspect InfraEnv for: 4.1. Changes to status.isoDownloadURL , cluster-id and token. 4.2. Notice that status.createdTime was updated. $ kubectl -n test-namespace get infraenvs.agent-install.openshift.io test-infraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-24T10:31:16Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Inspect AgentClusterInstall conditions $ kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied RequirementsMet The cluster is not ready to begin the installation Validated The cluster's validations are failing: Single-node clusters must have a single master node and no workers. Completed The installation has not yet started Failed The installation has not failed Stopped The installation is waiting to start or in progress Create BareMetalHost resource(s). Note that BMAC will wait for the InfraEnv image to be at least 1 minute old. Check BareMetalHost events: $ kubectl -n test-namespace describe baremetalhosts.metal3.io ostest-extraworker-3 <...> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Registered 61s metal3-baremetal-controller Registered new host Normal BMCAccessValidated 50s metal3-baremetal-controller Verified access to BMC Normal InspectionSkipped 50s metal3-baremetal-controller disabled by annotation Normal ProfileSet 50s metal3-baremetal-controller Hardware profile set: unknown Normal ProvisioningStarted 49s metal3-baremetal-controller Image provisioning started for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Normal ProvisioningComplete 39s metal3-baremetal-controller Image provisioning completed for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Check backend cluster events: $ curl -s -k $(kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" First, expect: { \"cluster_id\": \"21ade42e-1c78-48b0-bde7-e875632527c1\", \"event_time\": \"2021-06-24T10:49:19.232Z\", \"message\": \"Started image download (image type is \\\"minimal-iso\\\")\", \"severity\": \"info\" } Lastly, when installed: { \"cluster_id\": \"21ade42e-1c78-48b0-bde7-e875632527c1\", \"event_time\": \"2021-06-24T11:34:08.644Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" }","title":"Hive Integration - Retry to Install a cluster"},{"location":"hive-integration/kube-api-install-retry/#hive-integration-retry-to-install-a-cluster","text":"In a case where the cluster installation has failed, the user may want to restart the installation. A failed install may happen for multiple reasons; make sure you understand the root cause before trying again. To restart the installation, the user will need to: 1. Delete the cluster AgentClusterInstall resource. 2. Delete all the cluster BareMetalHost resources (a single resource in the case of SNO). 3. Recreate the AgentClusterInstall resource that was deleted in step 1 4. Recreate the BareMetalHost resources that were deleted in step 2 :warning: Note: Recreating AgentClusterInstall will de-register and re-register the backend cluster, which will trigger a discovery image generation for InfraEnv . Baremetal Agent Controller (a.k.a BMAC ) is inspecting InfraEnv for any changes to status.isoDownloadURL , and will pick up the newly generated discovery image. If you boot your machines in other methods (boot it yourself), make sure you use the new image for that. This document will capture the changes before (failed install) and after (resources recreated) to demonstrate this change to the image, but you won't have to do that when you reattempt the installation.","title":"Hive Integration - Retry to Install a cluster"},{"location":"hive-integration/kube-api-install-retry/#baseline","text":"How may a failed installation look? For that look at the AgentClusterInstall conditions: $ kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied Validated The cluster's validations are passing RequirementsMet The cluster installation stopped Completed The installation has failed: cluster has hosts in error Failed The installation failed: cluster has hosts in error Stopped The installation has stopped due to error","title":"Baseline"},{"location":"hive-integration/kube-api-install-retry/#capture-current-discovery-image-url","text":"Expect URLs to match.","title":"Capture current discovery image URL"},{"location":"hive-integration/kube-api-install-retry/#infraenv","text":"$ kubectl -n test-namespace get infraenvs.agent-install.openshift.io test-infraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-23T14:24:57Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/2748ddac-0ac9-489b-a38c-ce0d29d22b02/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjc0OGRkYWMtMGFjOS00ODliLWEzOGMtY2UwZDI5ZDIyYjAyIn0.L67oWuxClinXtCiqRcieOS4vAJCFNVztAE_A2TYnBYJawhAox6NfiuxUih2TKwZxbNVCOwLdQXt_5rjYL6Xn5g","title":"InfraEnv"},{"location":"hive-integration/kube-api-install-retry/#baremetalhost","text":"$ kubectl -n test-namespace get baremetalhosts.metal3.io ostest-extraworker-3 -o=jsonpath=\"{.spec.image.url}{'\\n'}\" https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/2748ddac-0ac9-489b-a38c-ce0d29d22b02/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjc0OGRkYWMtMGFjOS00ODliLWEzOGMtY2UwZDI5ZDIyYjAyIn0.L67oWuxClinXtCiqRcieOS4vAJCFNVztAE_A2TYnBYJawhAox6NfiuxUih2TKwZxbNVCOwLdQXt_5rjYL6Xn5g","title":"BareMetalHost"},{"location":"hive-integration/kube-api-install-retry/#delete-and-recreate","text":"As mentioned above, Delete both AgentClusterInstall and all the cluster BareMetalHost resources. Create AgentClusterInstall The InfraEnv controller: 3.1. Gets notified for a successful backend cluster registration. 3.2. Reconcile and send a request for the backend to generate a discovery image. Inspect InfraEnv for: 4.1. Changes to status.isoDownloadURL , cluster-id and token. 4.2. Notice that status.createdTime was updated. $ kubectl -n test-namespace get infraenvs.agent-install.openshift.io test-infraenv -o=jsonpath=\"{.status.createdTime}{'\\n'}{.status.isoDownloadURL}{'\\n'}\" 2021-06-24T10:31:16Z https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Inspect AgentClusterInstall conditions $ kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.metadata.name}{'\\n'}{range .status.conditions[*]}{.type}{'\\t'}{.message}{'\\n'}\" test-agent-cluster-install SpecSynced The Spec has been successfully applied RequirementsMet The cluster is not ready to begin the installation Validated The cluster's validations are failing: Single-node clusters must have a single master node and no workers. Completed The installation has not yet started Failed The installation has not failed Stopped The installation is waiting to start or in progress Create BareMetalHost resource(s). Note that BMAC will wait for the InfraEnv image to be at least 1 minute old. Check BareMetalHost events: $ kubectl -n test-namespace describe baremetalhosts.metal3.io ostest-extraworker-3 <...> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Registered 61s metal3-baremetal-controller Registered new host Normal BMCAccessValidated 50s metal3-baremetal-controller Verified access to BMC Normal InspectionSkipped 50s metal3-baremetal-controller disabled by annotation Normal ProfileSet 50s metal3-baremetal-controller Hardware profile set: unknown Normal ProvisioningStarted 49s metal3-baremetal-controller Image provisioning started for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Normal ProvisioningComplete 39s metal3-baremetal-controller Image provisioning completed for https://assisted-service-assisted-installer.apps.ostest.test.metalkube.org/api/assisted-install/v1/clusters/21ade42e-1c78-48b0-bde7-e875632527c1/downloads/image?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbHVzdGVyX2lkIjoiMjFhZGU0MmUtMWM3OC00OGIwLWJkZTctZTg3NTYzMjUyN2MxIn0.IbVClRJQm8nihs7N2B9hiJ523qioKKqymaxGWkQPCIdnMspx_pfWRUeieYyEVDUExLeBPuFlwb84mLPCuZCLzg Check backend cluster events: $ curl -s -k $(kubectl -n test-namespace get agentclusterinstalls.extensions.hive.openshift.io test-agent-cluster-install -o=jsonpath=\"{.status.debugInfo.eventsURL}\") | jq \".\" First, expect: { \"cluster_id\": \"21ade42e-1c78-48b0-bde7-e875632527c1\", \"event_time\": \"2021-06-24T10:49:19.232Z\", \"message\": \"Started image download (image type is \\\"minimal-iso\\\")\", \"severity\": \"info\" } Lastly, when installed: { \"cluster_id\": \"21ade42e-1c78-48b0-bde7-e875632527c1\", \"event_time\": \"2021-06-24T11:34:08.644Z\", \"message\": \"Successfully finished installing cluster test-cluster\", \"severity\": \"info\" }","title":"Delete and Recreate"},{"location":"hive-integration/kube-api-select-ocp-versions/","text":"Hive Integration - Selecting OpenShift Versions As part of Hive Integration , a means to add and select an OpenShift release version is required. In order to facilitate this functionality, the ClusterImageSet CRD is utilized for specifying a release image. A useful use-case is an environment with mirrored releases, in which the release image is mirrored to a local registry. To set a different RHCOS image for an OpenShift version: URL and version should be specified in AgentServiceConfig CRD. ClusterImageSet The ClusterImageSet is used for referencing to a OpenShift release image. So available versions are represented in an Hive cluster by defined ClusterImageSet resources. To use a specific release image, it should be defined in the ClusterDeployment CRD in either of the following manners: * As a reference to the ClusterImageSet in spec.provisioning.imageSetRef property. * Explicitly as a URL in spec.provisioning.releaseImage property. An example of a ClusterImageSet: apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.7.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.7.0-x86_64 Usage Set OS images in AgentServiceConfig A collection of RHCOS images can be defined within the AgentServiceConfig CRD as a mapping between a minor OpenShift version and image URL/version. E.g. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig spec: osImages: - openshiftVersion: 4.7 url: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.0/rhcos-4.7.0-x86_64-live.x86_64.iso version: 47.83.202102090044-0, cpuArchitecture: \"x86_64\" Deploy ClusterImageSet Deploy a ClusterImageSet with the requested release image. E.g. apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.8.0-fc.0-x86_64 Define imageSetRef in the ClusterDeployment The deployed ClusterImageSet should be referenced in the ClusterDeployment under spec.provisioning.imageSetRef property. E.g. apiVersion: hive.openshift.io/v1 kind: ClusterDeployment spec: provisioning: imageSetRef: name: openshift-v4.8.0 Flow The flow of adding a new version is a follows: * If a new RHCOS image is required: * Set OSImage in AgentServiceConfig under spec.osImages * OSImage should include: * openshiftVersion the OCP version in major.minor format. * url the RHCOS image (optionally a mirror). * version the RHOCS version. * Upon starting the service, the relevant host boot-files are uploaded to S3/File storage. * Deploy a ClusterImageSet with a new releaseImage URL. * The URL can be a mirror to a local registry. * Deploy a ClusterDeployment, referencing to the ClusterImageSet under spec.provisioning.imageSetRef . * Finally, a new cluster is created with the newly added openshift_version .","title":"Hive Integration - Selecting OpenShift Versions"},{"location":"hive-integration/kube-api-select-ocp-versions/#hive-integration-selecting-openshift-versions","text":"As part of Hive Integration , a means to add and select an OpenShift release version is required. In order to facilitate this functionality, the ClusterImageSet CRD is utilized for specifying a release image. A useful use-case is an environment with mirrored releases, in which the release image is mirrored to a local registry. To set a different RHCOS image for an OpenShift version: URL and version should be specified in AgentServiceConfig CRD.","title":"Hive Integration - Selecting OpenShift Versions"},{"location":"hive-integration/kube-api-select-ocp-versions/#clusterimageset","text":"The ClusterImageSet is used for referencing to a OpenShift release image. So available versions are represented in an Hive cluster by defined ClusterImageSet resources. To use a specific release image, it should be defined in the ClusterDeployment CRD in either of the following manners: * As a reference to the ClusterImageSet in spec.provisioning.imageSetRef property. * Explicitly as a URL in spec.provisioning.releaseImage property. An example of a ClusterImageSet: apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.7.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.7.0-x86_64","title":"ClusterImageSet"},{"location":"hive-integration/kube-api-select-ocp-versions/#usage","text":"","title":"Usage"},{"location":"hive-integration/kube-api-select-ocp-versions/#set-os-images-in-agentserviceconfig","text":"A collection of RHCOS images can be defined within the AgentServiceConfig CRD as a mapping between a minor OpenShift version and image URL/version. E.g. apiVersion: agent-install.openshift.io/v1beta1 kind: AgentServiceConfig spec: osImages: - openshiftVersion: 4.7 url: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/4.7.0/rhcos-4.7.0-x86_64-live.x86_64.iso version: 47.83.202102090044-0, cpuArchitecture: \"x86_64\"","title":"Set OS images in AgentServiceConfig"},{"location":"hive-integration/kube-api-select-ocp-versions/#deploy-clusterimageset","text":"Deploy a ClusterImageSet with the requested release image. E.g. apiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 spec: releaseImage: quay.io/openshift-release-dev/ocp-release:4.8.0-fc.0-x86_64","title":"Deploy ClusterImageSet"},{"location":"hive-integration/kube-api-select-ocp-versions/#define-imagesetref-in-the-clusterdeployment","text":"The deployed ClusterImageSet should be referenced in the ClusterDeployment under spec.provisioning.imageSetRef property. E.g. apiVersion: hive.openshift.io/v1 kind: ClusterDeployment spec: provisioning: imageSetRef: name: openshift-v4.8.0","title":"Define imageSetRef in the ClusterDeployment"},{"location":"hive-integration/kube-api-select-ocp-versions/#flow","text":"The flow of adding a new version is a follows: * If a new RHCOS image is required: * Set OSImage in AgentServiceConfig under spec.osImages * OSImage should include: * openshiftVersion the OCP version in major.minor format. * url the RHCOS image (optionally a mirror). * version the RHOCS version. * Upon starting the service, the relevant host boot-files are uploaded to S3/File storage. * Deploy a ClusterImageSet with a new releaseImage URL. * The URL can be a mirror to a local registry. * Deploy a ClusterDeployment, referencing to the ClusterImageSet under spec.provisioning.imageSetRef . * Finally, a new cluster is created with the newly added openshift_version .","title":"Flow"},{"location":"hive-integration/late-binding/","text":"Late Binding With late binding, a discovery ISO can be created without the need of a reference to a Cluster Deployment. Hosts booted with such an ISO can be bound to different clusters at a later stage. See full enhancement documents here: late binding and returning agents to InfraEnv . High Level Flow The user creates an InfraEnv CR without a Cluster Reference. See example here The version of the base RHCOS live ISO is selected automatically to the latest available. The Agent CR created from an host booted from this ISO will not have a Cluster Deployment reference set in it. ( spec.clusterDeploymentName ) The user creates a Cluster Deployment and Agent Cluster Install CR. The user updates the Agent's Cluster Deployment reference to the CD name: kubectl -n my_namespace patch agents.agent-install.openshift.io my_agent -p '{\"spec\":{\"clusterDeploymentName\":{\"name\":\"my_cd\",\"namespace\":\"my_cd_ns\"}}}' --type merge The process of the agent binding can be followed on the Bound condition available on the Agent CR. See [here] (kube-api-conditions.md#agent-conditions) Once the agent is bound, the flow for installation is as before. An agent can be unbound from a Cluster Deployment as long as the installation did not start. If an agent is unbound after it was installed or if it is in error / canceled state, the Agent's Bound condition will be False with UnbindingPendingUserAction reason. In this state, it is the responsibility of the user to reboot the host with the discovery ISO. With BareMetalOperator integration, the host will be rebooted automatically. Note that the Pull Secret of the InfraEnv can be different from the one specified in the Cluster Deployment. Teardown When a Cluster/Deployment is deleted, the Agents created with late binding will be returned to the InfraEnv. When an InfraEnv CR is deleted, the hosts related to it will be deleted if they are Unbound or Installed. If no more hosts are connected, the InfraEnv will be deleted. If there are still hosts connected, the InfraEnv CR will not be deleted until all the related hosts are deleted or Unbound. Unsupported flows The following operations are not supported: Update/Add/Delete a Cluster Deployment reference of an InfraEnv. Update a Cluster Deployment reference of an Agent after installation started.","title":"Late Binding"},{"location":"hive-integration/late-binding/#late-binding","text":"With late binding, a discovery ISO can be created without the need of a reference to a Cluster Deployment. Hosts booted with such an ISO can be bound to different clusters at a later stage. See full enhancement documents here: late binding and returning agents to InfraEnv .","title":"Late Binding"},{"location":"hive-integration/late-binding/#high-level-flow","text":"The user creates an InfraEnv CR without a Cluster Reference. See example here The version of the base RHCOS live ISO is selected automatically to the latest available. The Agent CR created from an host booted from this ISO will not have a Cluster Deployment reference set in it. ( spec.clusterDeploymentName ) The user creates a Cluster Deployment and Agent Cluster Install CR. The user updates the Agent's Cluster Deployment reference to the CD name: kubectl -n my_namespace patch agents.agent-install.openshift.io my_agent -p '{\"spec\":{\"clusterDeploymentName\":{\"name\":\"my_cd\",\"namespace\":\"my_cd_ns\"}}}' --type merge The process of the agent binding can be followed on the Bound condition available on the Agent CR. See [here] (kube-api-conditions.md#agent-conditions) Once the agent is bound, the flow for installation is as before. An agent can be unbound from a Cluster Deployment as long as the installation did not start. If an agent is unbound after it was installed or if it is in error / canceled state, the Agent's Bound condition will be False with UnbindingPendingUserAction reason. In this state, it is the responsibility of the user to reboot the host with the discovery ISO. With BareMetalOperator integration, the host will be rebooted automatically. Note that the Pull Secret of the InfraEnv can be different from the one specified in the Cluster Deployment.","title":"High Level Flow"},{"location":"hive-integration/late-binding/#teardown","text":"When a Cluster/Deployment is deleted, the Agents created with late binding will be returned to the InfraEnv. When an InfraEnv CR is deleted, the hosts related to it will be deleted if they are Unbound or Installed. If no more hosts are connected, the InfraEnv will be deleted. If there are still hosts connected, the InfraEnv CR will not be deleted until all the related hosts are deleted or Unbound.","title":"Teardown"},{"location":"hive-integration/late-binding/#unsupported-flows","text":"The following operations are not supported: Update/Add/Delete a Cluster Deployment reference of an InfraEnv. Update a Cluster Deployment reference of an Agent after installation started.","title":"Unsupported flows"},{"location":"user-guide/","text":"User Guide Welcome to the Openshift Assisted Service User Guide. Here we will look for the best way to help you deploying Openshift 4 in the provider you desire. OCP Deployment on Local OCP Deployment on Bare Metal OCP Deployment on vSphere OCP Deployment on RHEV OCP Deployment on Openstack Using the RESTFul API The assisted-service exposes a RESTFul API which is described in swagger.yaml . A guide of using the RESTFul API is available on restful-api-guide.yaml . Using Assisted Service On-Premises Please refer to the Hive Integration readme to learn how to install OCP cluster using Assisted Service on-premises with Hive and RHACM (Red Hat Advanced Cluster Management). Network Configuration Please refer to the Network Configuration introduction for more information about advanced network configuration with the Assisted Service.","title":"User Guide"},{"location":"user-guide/#user-guide","text":"Welcome to the Openshift Assisted Service User Guide. Here we will look for the best way to help you deploying Openshift 4 in the provider you desire. OCP Deployment on Local OCP Deployment on Bare Metal OCP Deployment on vSphere OCP Deployment on RHEV OCP Deployment on Openstack","title":"User Guide"},{"location":"user-guide/#using-the-restful-api","text":"The assisted-service exposes a RESTFul API which is described in swagger.yaml . A guide of using the RESTFul API is available on restful-api-guide.yaml .","title":"Using the RESTFul API"},{"location":"user-guide/#using-assisted-service-on-premises","text":"Please refer to the Hive Integration readme to learn how to install OCP cluster using Assisted Service on-premises with Hive and RHACM (Red Hat Advanced Cluster Management).","title":"Using Assisted Service On-Premises"},{"location":"user-guide/#network-configuration","text":"Please refer to the Network Configuration introduction for more information about advanced network configuration with the Assisted Service.","title":"Network Configuration"},{"location":"user-guide/assisted-service-on-local/","text":"Running OpenShift Assisted Service (OAS) on local machine For that we have some ways we could follow, let's take a look on some of them. Running OAS on Minikube This is the easiest way to deploy OAS, there are some Make targets to do that, but firstly we need Minikube: minikube start --driver=kvm2 skipper make deploy-all skipper make deploy-ui You could use kubectl proxy command to expose the Assisted Service UI for external access purpose Running OAS on Podman The assisted service can also be deployed without using a Kubernetes cluster. In this scenario the service and associated components are deployed onto your local host as a pod using Podman. This type of deployment requires a different container image that combines components that are used to generate the installer ISO and configuration files. First build the image: export SERVICE_ONPREM=quay.io/<your-org>/assisted-service:latest make build-onprem To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090. Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem Running OAS on vanilla K8s work in progress... Running OAS on K3s work in progress... Running OAS on KinD work in progress... Deploying Monitoring service for OAS This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus. Deploy by tag This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"OAS Running on Local"},{"location":"user-guide/assisted-service-on-local/#running-openshift-assisted-service-oas-on-local-machine","text":"For that we have some ways we could follow, let's take a look on some of them.","title":"Running OpenShift Assisted Service (OAS) on local machine"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-minikube","text":"This is the easiest way to deploy OAS, there are some Make targets to do that, but firstly we need Minikube: minikube start --driver=kvm2 skipper make deploy-all skipper make deploy-ui You could use kubectl proxy command to expose the Assisted Service UI for external access purpose","title":"Running OAS on Minikube"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-podman","text":"The assisted service can also be deployed without using a Kubernetes cluster. In this scenario the service and associated components are deployed onto your local host as a pod using Podman. This type of deployment requires a different container image that combines components that are used to generate the installer ISO and configuration files. First build the image: export SERVICE_ONPREM=quay.io/<your-org>/assisted-service:latest make build-onprem To deploy, update SERVICE_BASE_URL in the onprem-environment file to match the hostname or IP address of your host. For example if your IP address is 192.168.122.2, then the SERVICE_BASE_URL would be set to http://192.168.122.2:8090. Port 8090 is the assisted-service API. Then deploy the containers: make deploy-onprem Check all containers are up and running: podman ps -a The UI will available at: http://<host-ip-address>:8080 To remove the containers: make clean-onprem","title":"Running OAS on Podman"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-vanilla-k8s","text":"work in progress...","title":"Running OAS on vanilla K8s"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-k3s","text":"work in progress...","title":"Running OAS on K3s"},{"location":"user-guide/assisted-service-on-local/#running-oas-on-kind","text":"work in progress...","title":"Running OAS on KinD"},{"location":"user-guide/assisted-service-on-local/#deploying-monitoring-service-for-oas","text":"This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-olm make deploy-prometheus make deploy-grafana # Or just all-in make deploy-monitoring NOTE: To expose the monitoring UI's on your local environment you could follow these steps kubectl config set-context $(kubectl config current-context) --namespace assisted-installer # To expose Prometheus kubectl port-forward svc/prometheus-k8s 9090:9090 # To expose Grafana kubectl port-forward svc/grafana 3000:3000 Now you just need to access http://127.0.0.1:3000 to access to your Grafana deployment or http://127.0.0.1:9090 for Prometheus.","title":"Deploying Monitoring service for OAS"},{"location":"user-guide/assisted-service-on-local/#deploy-by-tag","text":"This feature is for internal usage and not recommended to use by external users. This option will select the required tag that will be used for each dependency. If deploy-all use a new tag the update will be done automatically and there is no need to reboot/rollout any deployment. Deploy images according to the manifest: skipper make deploy-all DEPLOY_MANIFEST_PATH=./assisted-installer.yaml Deploy images according to the manifest in the assisted-installer-deployment repo (require git tag/branch/hash): skipper make deploy-all DEPLOY_MANIFEST_TAG=master Deploy all the images with the same tag. The tag is not validated, so you need to make sure it actually exists. skipper make deploy-all DEPLOY_TAG=<tag> Default tag is latest","title":"Deploy by tag"},{"location":"user-guide/assisted-service-on-openshift/","text":"How to deploy OAS on OpenShift Deploy with Makefile Assisted Service Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: 1. APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. 1. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note : All deployment configurations are under the deploy directory in case more detailed configuration is required. UI This service support optional UI deployment. make deploy-ui TARGET=oc-ingress Monitoring This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false Configure Bare Metal Operator When OpenShift is deployed with the baremetal platform, it includes the baremetal-operator. If you want to use the baremetal-operator and its BareMetalHost CRD to automatically boot hosts with the discovery ISO, you'll need to configure baremetal-operator to watch all namespaces. oc patch provisioning provisioning-configuration --type merge -p '{\"spec\":{\"watchAllNamespaces\": true}}'","title":"OAS Running on Openshift"},{"location":"user-guide/assisted-service-on-openshift/#how-to-deploy-oas-on-openshift","text":"","title":"How to deploy OAS on OpenShift"},{"location":"user-guide/assisted-service-on-openshift/#deploy-with-makefile","text":"","title":"Deploy with Makefile"},{"location":"user-guide/assisted-service-on-openshift/#assisted-service","text":"Besides default minikube deployment, the service support deployment to OpenShift cluster using ingress as the access point to the service. make deploy-all TARGET=oc-ingress This deployment option have multiple optional parameters that should be used in case you are not the Admin of the cluster: 1. APPLY_NAMESPACE - True by default. Will try to deploy \"assisted-installer\" namespace, if you are not the Admin of the cluster or maybe you don't have permissions for this operation you may skip namespace deployment. 1. INGRESS_DOMAIN - By default deployment script will try to get the domain prefix from OpenShift ingress controller. If you don't have access to it then you may specify the domain yourself. For example: apps.ocp.prod.psi.redhat.com To set the parameters simply add them in the end of the command, for example make deploy-all TARGET=oc-ingress APPLY_NAMESPACE=False INGRESS_DOMAIN=apps.ocp.prod.psi.redhat.com Note : All deployment configurations are under the deploy directory in case more detailed configuration is required.","title":"Assisted Service"},{"location":"user-guide/assisted-service-on-openshift/#ui","text":"This service support optional UI deployment. make deploy-ui TARGET=oc-ingress","title":"UI"},{"location":"user-guide/assisted-service-on-openshift/#monitoring","text":"This will allow you to deploy Prometheus and Grafana already integrated with Assisted installer: # Step by step make deploy-prometheus TARGET=oc-ingress APPLY_NAMESPACE=false make deploy-grafana TARGET=oc-ingress APPLY_NAMESPACE=false # Or just all-in make deploy-monitoring TARGET=oc-ingress APPLY_NAMESPACE=false","title":"Monitoring"},{"location":"user-guide/assisted-service-on-openshift/#configure-bare-metal-operator","text":"When OpenShift is deployed with the baremetal platform, it includes the baremetal-operator. If you want to use the baremetal-operator and its BareMetalHost CRD to automatically boot hosts with the discovery ISO, you'll need to configure baremetal-operator to watch all namespaces. oc patch provisioning provisioning-configuration --type merge -p '{\"spec\":{\"watchAllNamespaces\": true}}'","title":"Configure Bare Metal Operator"},{"location":"user-guide/deploy-on-OSP/","text":"Openshift Deployment on OpenStack by OpenShift Assisted Service This guide explains how to deploy OpenShift by the OpenShift Assisted Service on OpenStack. NOTE Currently, the deployment of an OpenShift Cluster on Red Hat OpenStack Platform is blocked by OpenShift Assisted Service, while RDO is working fine. The related check can be disabled by including valid-platform the environment variable DISABLED_HOST_VALIDATIONS in the context of the OpenShift Assisted Service, e.g. like this: DISABLED_HOST_VALIDATIONS=valid-platform,container-images-available Requirements Two floating IP addresses like other ways of installing OpenShift , it is recommended that DNS resolves api.CLUSTERNAME.DOMAIN to the first floating IP address, and *.apps.CLUSTERNAME.DOMAIN to the second one The resources required by the VMs A dedicated OpenStack network and subnet are recommended Steps Generate the discovery iso in OpenShift Assisted Service. Upload the discovery iso as a new image to OpenStack. Create two ports and associate each one a floating IP address. Create the VMs to run the OpenShift cluster, with a bootable fresh volume and the image of the discovery iso as the second boot index. Please find an example below. Add the addresses of both floating IPs as the allowed_address_pairs of all VMs which might run a master role in the OpenShift cluster. This will enable the virtual IPs to be usable on the VMs. Upon adding an IP address to the \"allowed_address_pairs\" field in the Neutron's port the ML2/OVN driver will check if that IP matches with the IP of another existing port in the same network (Logical_Switch in OVN) and, if they do match, ML2/OVN will update the type of the matching port to \"virtual\". Please the details in Deploying highly available instances with keepalived and Highly available VIPs on OpenStack VMs with VRRP . Assign an appropriate security group to the networking ports of the VMs and to the ports of the floating IPs. A security group that allows all IP traffic works. Install the OpenShift cluster via OpenShift Assisted Service as it would be on bare metal. Example Block Device Mapping \"block_device_mapping_v2\": [ { \"uuid\": ID_OF_FRESH_VOLUME, \"boot_index\": \"0\", \"source_type\": \"volume\", \"destination_type\": \"volume\", \"delete_on_termination\": True }, { \"uuid\": ID_OF_DISCOVERY_ISO, \"source_type\": \"image\", \"volume_size\": \"1\", \"device_type\": \"cdrom\", \"boot_index\": \"1\", \"destination_type\": \"volume\", \"delete_on_termination\": True } ]","title":"OCP Deployment on Openstack"},{"location":"user-guide/deploy-on-OSP/#openshift-deployment-on-openstack-by-openshift-assisted-service","text":"This guide explains how to deploy OpenShift by the OpenShift Assisted Service on OpenStack. NOTE Currently, the deployment of an OpenShift Cluster on Red Hat OpenStack Platform is blocked by OpenShift Assisted Service, while RDO is working fine. The related check can be disabled by including valid-platform the environment variable DISABLED_HOST_VALIDATIONS in the context of the OpenShift Assisted Service, e.g. like this: DISABLED_HOST_VALIDATIONS=valid-platform,container-images-available","title":"Openshift Deployment on OpenStack by OpenShift Assisted Service"},{"location":"user-guide/deploy-on-OSP/#requirements","text":"Two floating IP addresses like other ways of installing OpenShift , it is recommended that DNS resolves api.CLUSTERNAME.DOMAIN to the first floating IP address, and *.apps.CLUSTERNAME.DOMAIN to the second one The resources required by the VMs A dedicated OpenStack network and subnet are recommended","title":"Requirements"},{"location":"user-guide/deploy-on-OSP/#steps","text":"Generate the discovery iso in OpenShift Assisted Service. Upload the discovery iso as a new image to OpenStack. Create two ports and associate each one a floating IP address. Create the VMs to run the OpenShift cluster, with a bootable fresh volume and the image of the discovery iso as the second boot index. Please find an example below. Add the addresses of both floating IPs as the allowed_address_pairs of all VMs which might run a master role in the OpenShift cluster. This will enable the virtual IPs to be usable on the VMs. Upon adding an IP address to the \"allowed_address_pairs\" field in the Neutron's port the ML2/OVN driver will check if that IP matches with the IP of another existing port in the same network (Logical_Switch in OVN) and, if they do match, ML2/OVN will update the type of the matching port to \"virtual\". Please the details in Deploying highly available instances with keepalived and Highly available VIPs on OpenStack VMs with VRRP . Assign an appropriate security group to the networking ports of the VMs and to the ports of the floating IPs. A security group that allows all IP traffic works. Install the OpenShift cluster via OpenShift Assisted Service as it would be on bare metal.","title":"Steps"},{"location":"user-guide/deploy-on-OSP/#example-block-device-mapping","text":"\"block_device_mapping_v2\": [ { \"uuid\": ID_OF_FRESH_VOLUME, \"boot_index\": \"0\", \"source_type\": \"volume\", \"destination_type\": \"volume\", \"delete_on_termination\": True }, { \"uuid\": ID_OF_DISCOVERY_ISO, \"source_type\": \"image\", \"volume_size\": \"1\", \"device_type\": \"cdrom\", \"boot_index\": \"1\", \"destination_type\": \"volume\", \"delete_on_termination\": True } ]","title":"Example Block Device Mapping"},{"location":"user-guide/deploy-on-RHEV/","text":"Openshift deployment with OAS - On RHEV/oVirt work in progress...","title":"OCP Deployment on RHEV"},{"location":"user-guide/deploy-on-RHEV/#openshift-deployment-with-oas-on-rhevovirt","text":"work in progress...","title":"Openshift deployment with OAS - On RHEV/oVirt"},{"location":"user-guide/deploy-on-bare-metal/","text":"Openshift deployment with OAS - On Bare Metal This guide contains all the sections regarding Bare Metal deployment method, like iPXE/PXE, VirtualMedia, etc... let's get started General This section is generic for the most of the cases: DHCP/DNS running on the network you wanna deploy the OCP cluster. Assisted Installer up & running (It's ok if you're working with cloud version). Typical DNS entries for API VIP and Ingress VIP. Pull Secret to reach the OCP Container Images. SSH Key pair. Note : This method could be used also in Virtual environment With that we could start, first step is create the cluster Fill the Cluster name and Pull Secret fields, also select the version you wanna deploy: Now fill the Base Domain field and the SSH Host Public Key Click on Download Discovery ISO Fill again the SSH public key and click on Generate Discovery ISO Wait for ISO generation to finish and you will reach this checkpoint iPXE iPXE deployment method NOTE1 : We use a sample URL, please change to fit your use case accordingly NOTE2 : We've set the live_url as the node hostname on 8080 port , please change to fit your use case accordingly Automatic The automatic way is done using podman, just follow this steps: export IPXE_DIR=/tmp/ipxe/ai mkdir -p ${IPXE_DIR} # This command will download the ISO, extract the Images and create the ignition config files podman run -e BASE_URL=http://devscripts2ipv6.e2e.bos.redhat.com:8080 -e ISO_URL=http://devscripts2ipv6.e2e.bos.redhat.com:6008/api/assisted-install/v1/clusters/33ffb056-ee65-4fee-91c9-f60e5ebea4a3/downloads/image -v /tmp/ipxe/ai:/data:Z --net=host -it --rm quay.io/ohadlevy/ai-ipxe # This command will host the iPXE files on an podman container podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe Manual The manual way is explained here. You need at least to have the Discovery ISO already generated Now let's download that ISO in the provisioning machine, where the iPXE files will be hosted (use the Command to download the ISO button from the Assisted Service website export IPXE_DIR=/tmp/ipxe/ai export IMAGE_PATH=/tmp/discovery_image_ocp.iso wget -O ${IMAGE_PATH} 'http://console.redhat.com/api/assisted-install/v1/clusters/<cluster_id>/downloads/image' Now we need to create the folder and the ipxe file definition mkdir -p ${IPXE_DIR} cat > $IPXE_DIR/ipxe << EOF #!ipxe set live_url $(hostname):8080 kernel \\${live_url}/vmlinuz ignition.config.url=\\${live_url}/config.ign coreos.live.rootfs_url=\\${live_url}/rootfs.img ${KERNEL_OPTS} initrd \\${live_url}/initrd.img boot EOF We also need to extract the images from the ISO export PXE_IMAGES=`isoinfo -i $IMAGE_PATH -f | grep -i images/pxeboot` for img in $PXE_IMAGES; do export name=`basename ${img,,} | sed 's/\\;1//' | sed 's/\\.$//'` echo extracting $name isoinfo -i $IMAGE_PATH -x $img > $IPXE_DIR/$name done And as a last step, write the Ignition files for the deployment echo writing custom user ignition echo '{' > $IPXE_DIR/config.ign isoinfo -i $IMAGE_PATH -x '/IMAGES/IGNITION.IMG;1' | xz -dc - | sed '1d; $d' >> $IPXE_DIR/config.ign echo '}' >> $IPXE_DIR/config.ign After the Ignition files creation we need to host the files, for that we will use a podman contianer based on nginx podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe Booting the nodes from iPXE First step, we need to set up the boot mode on the iDrac's as boot once for iPXE, this will depend on the steps on every Bare Metal Manufacturer/Version/Hardware. When you are booting the nodes, stay tuned to press crtl-b when the prompt say that: Now we need to get a correct IP and point to the right iPXE file And we just need to wait until the boot was finished, and the nodes start appearing on the Assisted Service interface Then we will modify the nodename to use a right name for Openshift Create another 2 more nodes and repeat this step Now fill the API Virtual IP and Ingress Virtual IP fields Now you just need to click on Install Cluster button and wait for the installation to finish.","title":"OCP Deployment on Bare Metal"},{"location":"user-guide/deploy-on-bare-metal/#openshift-deployment-with-oas-on-bare-metal","text":"This guide contains all the sections regarding Bare Metal deployment method, like iPXE/PXE, VirtualMedia, etc... let's get started","title":"Openshift deployment with OAS - On Bare Metal"},{"location":"user-guide/deploy-on-bare-metal/#general","text":"This section is generic for the most of the cases: DHCP/DNS running on the network you wanna deploy the OCP cluster. Assisted Installer up & running (It's ok if you're working with cloud version). Typical DNS entries for API VIP and Ingress VIP. Pull Secret to reach the OCP Container Images. SSH Key pair. Note : This method could be used also in Virtual environment With that we could start, first step is create the cluster Fill the Cluster name and Pull Secret fields, also select the version you wanna deploy: Now fill the Base Domain field and the SSH Host Public Key Click on Download Discovery ISO Fill again the SSH public key and click on Generate Discovery ISO Wait for ISO generation to finish and you will reach this checkpoint","title":"General"},{"location":"user-guide/deploy-on-bare-metal/#ipxe","text":"iPXE deployment method NOTE1 : We use a sample URL, please change to fit your use case accordingly NOTE2 : We've set the live_url as the node hostname on 8080 port , please change to fit your use case accordingly","title":"iPXE"},{"location":"user-guide/deploy-on-bare-metal/#automatic","text":"The automatic way is done using podman, just follow this steps: export IPXE_DIR=/tmp/ipxe/ai mkdir -p ${IPXE_DIR} # This command will download the ISO, extract the Images and create the ignition config files podman run -e BASE_URL=http://devscripts2ipv6.e2e.bos.redhat.com:8080 -e ISO_URL=http://devscripts2ipv6.e2e.bos.redhat.com:6008/api/assisted-install/v1/clusters/33ffb056-ee65-4fee-91c9-f60e5ebea4a3/downloads/image -v /tmp/ipxe/ai:/data:Z --net=host -it --rm quay.io/ohadlevy/ai-ipxe # This command will host the iPXE files on an podman container podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe","title":"Automatic"},{"location":"user-guide/deploy-on-bare-metal/#manual","text":"The manual way is explained here. You need at least to have the Discovery ISO already generated Now let's download that ISO in the provisioning machine, where the iPXE files will be hosted (use the Command to download the ISO button from the Assisted Service website export IPXE_DIR=/tmp/ipxe/ai export IMAGE_PATH=/tmp/discovery_image_ocp.iso wget -O ${IMAGE_PATH} 'http://console.redhat.com/api/assisted-install/v1/clusters/<cluster_id>/downloads/image' Now we need to create the folder and the ipxe file definition mkdir -p ${IPXE_DIR} cat > $IPXE_DIR/ipxe << EOF #!ipxe set live_url $(hostname):8080 kernel \\${live_url}/vmlinuz ignition.config.url=\\${live_url}/config.ign coreos.live.rootfs_url=\\${live_url}/rootfs.img ${KERNEL_OPTS} initrd \\${live_url}/initrd.img boot EOF We also need to extract the images from the ISO export PXE_IMAGES=`isoinfo -i $IMAGE_PATH -f | grep -i images/pxeboot` for img in $PXE_IMAGES; do export name=`basename ${img,,} | sed 's/\\;1//' | sed 's/\\.$//'` echo extracting $name isoinfo -i $IMAGE_PATH -x $img > $IPXE_DIR/$name done And as a last step, write the Ignition files for the deployment echo writing custom user ignition echo '{' > $IPXE_DIR/config.ign isoinfo -i $IMAGE_PATH -x '/IMAGES/IGNITION.IMG;1' | xz -dc - | sed '1d; $d' >> $IPXE_DIR/config.ign echo '}' >> $IPXE_DIR/config.ign After the Ignition files creation we need to host the files, for that we will use a podman contianer based on nginx podman run -v ${IPXE_DIR}:/app:ro -p 8080:8080 -d --rm bitnami/nginx:latest To ensure if your container is working fine, check the url with a curl command curl http://$(hostname):8080/ipxe/ipxe","title":"Manual"},{"location":"user-guide/deploy-on-bare-metal/#booting-the-nodes-from-ipxe","text":"First step, we need to set up the boot mode on the iDrac's as boot once for iPXE, this will depend on the steps on every Bare Metal Manufacturer/Version/Hardware. When you are booting the nodes, stay tuned to press crtl-b when the prompt say that: Now we need to get a correct IP and point to the right iPXE file And we just need to wait until the boot was finished, and the nodes start appearing on the Assisted Service interface Then we will modify the nodename to use a right name for Openshift Create another 2 more nodes and repeat this step Now fill the API Virtual IP and Ingress Virtual IP fields Now you just need to click on Install Cluster button and wait for the installation to finish.","title":"Booting the nodes from iPXE"},{"location":"user-guide/deploy-on-local/","text":"Openshift deployment with OAS - On Local work in progress...","title":"OCP Deployment on Local"},{"location":"user-guide/deploy-on-local/#openshift-deployment-with-oas-on-local","text":"work in progress...","title":"Openshift deployment with OAS - On Local"},{"location":"user-guide/deploy-on-vsphere/","text":"Openshift deployment with OAS - On vSphere work in progress...","title":"OCP Deployment on vSphere"},{"location":"user-guide/deploy-on-vsphere/#openshift-deployment-with-oas-on-vsphere","text":"work in progress...","title":"Openshift deployment with OAS - On vSphere"},{"location":"user-guide/install-customization/","text":"Assisted Installer Customization Manifests These APIs allows for adding arbitrary manifests to the set generated by openshift-install create manifests . A typical use case would be to create a MachineConfig which would make some persistent customization to a group of nodes. These will only take effect after the machine config operator is up and running so if a change is needed before or during the installation process, another API will be required. Create a cluster manifest # base64 encoding of the example from https://docs.openshift.com/container-platform/4.6/installing/install_config/installing-customizing.html content=YXBpVmVyc2lvbjogbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3YxCmtpbmQ6IE1hY2hpbmVDb25maWcKbWV0YWRhdGE6CiAgbGFiZWxzOgogICAgbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3JvbGU6IG1hc3RlcgogIG5hbWU6IDk5LW9wZW5zaGlmdC1tYWNoaW5lY29uZmlnLW1hc3Rlci1rYXJncwpzcGVjOgogIGtlcm5lbEFyZ3VtZW50czoKICAgIC0gJ2xvZ2xldmVsPTcnCg== file=99-openshift-machineconfig-master-kargs.yaml folder=openshift curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request POST \\ --data \"{\\\"file_name\\\":\\\"$file\\\", \\\"folder\\\":\\\"$folder\\\", \\\"content\\\":\\\"$content\\\"}\" \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/manifests\" View a manifest\u2019s contents file=openshift/99-openshift-machineconfig-master-kargs.yaml curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/manifests/files?file_name=$file\" Discovery Ignition The discovery ignition is used to make changes to the CoreOS live iso image which runs before we actually write anything to the target disk. An example use case would be configuring a separate container registry to pull the assisted-installer-agent image from. The discovery ignition must use version 3.1.0 regardless of the version of the cluster that will eventually be created. Patch the discovery ignition # ignition patch file { \"config\": \"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/etc/containers/registries.conf\\\", \\\"mode\\\": 420, \\\"overwrite\\\": true, \\\"user\\\": { \\\"name\\\": \\\"root\\\"},\\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,dW5xdWFsaWZpZWQtc2VhcmNoLXJlZ2lzdHJpZXMgPSBbInJlZ2lzdHJ5LmFjY2Vzcy5yZWRoYXQuY29tIiwgImRvY2tlci5pbyJdCltbcmVnaXN0cnldXQogICBwcmVmaXggPSAiIgogICBsb2NhdGlvbiA9ICJxdWF5LmlvL29jcG1ldGFsIgogICBtaXJyb3ItYnktZGlnZXN0LW9ubHkgPSBmYWxzZQogICBbW3JlZ2lzdHJ5Lm1pcnJvcl1dCiAgIGxvY2F0aW9uID0gImxvY2FsLnJlZ2lzdHJ5OjUwMDAvb2NwbWV0YWwiCg==\\\"}}]}}\" } curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data @discovery-ign.json \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition\" View the discovery ignition curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition\" Install Config These endpoints alter the default install config yaml used when running openshift-install create commands. Install config customization reference is available here Some of this content will be pinned to particular values, but most can be edited. Note, some of this content will be pinned to particular values by the assisted-installer (can't be overwritten). You can compose the install-config overrides by creating a json string with the options you wish to set. An example install config with disabled hyperthreading for the control plane: apiVersion: v1 baseDomain: example.com controlPlane: name: master hyperthreading: Disabled compute: - name: worker replicas: 5 metadata: name: test-cluster platform: ... pullSecret: '{\"auths\": ...}' sshKey: ssh-ed25519 AAAA... should look like this: \"{\\\"controlPlane\\\":{\\\"hyperthreading\\\":\\\"Disabled\\\"}}\" Patch the install config curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '\"{\\\"controlPlane\\\":{\\\"hyperthreading\\\":\\\"Disabled\\\"}}\"' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/install-config\" View the install config curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/install-config\" Pointer Ignition The pointer ignition is used to customize the particular host when it reboots into the installed system. These changes will be made to the host, but will not be tracked as a machine config. Generally a machine config will be a better fit than editing the pointer ignition, but if some change is specific to an individual host it would be made here. The pointer ignition override version must match the version of the ignition generated by openshift-installer. This means that the version required for the override will change depending on the cluster version being installed. OCP Version(s) Ignition version 4.6 3.1.0 4.7, 4.8, 4.9, 4.10 3.2.0 Patch the pointer ignition curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '{\"config\": \"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/etc/example\\\", \\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,SGVsbG8gZnJvbSBob3N0IDg0Njk2NzdiLThlZGEtNDQzOS1iNDQwLTc3ZGM5M2FkZmNlZgo=\\\"}}]}}\"}' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/ignition\" View the pointer ignition curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/ignition\" Installer Params This endpoint sets parameters to be passed to the coreos-installer command line in addition to the ones we provide by default. The only parameters that can be set with this endpoint are: [\"--append-karg\", \"--delete-karg\", \"-n\", \"--copy-network\", \"--network-dir\", \"--save-partlabel\", \"--save-partindex\", \"--image-url\"] Set the installer params curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '{\"args\": [\"--append-karg\", \"nameserver=8.8.8.8\", \"-n\"]}' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/installer-args\"","title":"Assisted Installer Customization"},{"location":"user-guide/install-customization/#assisted-installer-customization","text":"","title":"Assisted Installer Customization"},{"location":"user-guide/install-customization/#manifests","text":"These APIs allows for adding arbitrary manifests to the set generated by openshift-install create manifests . A typical use case would be to create a MachineConfig which would make some persistent customization to a group of nodes. These will only take effect after the machine config operator is up and running so if a change is needed before or during the installation process, another API will be required.","title":"Manifests"},{"location":"user-guide/install-customization/#create-a-cluster-manifest","text":"# base64 encoding of the example from https://docs.openshift.com/container-platform/4.6/installing/install_config/installing-customizing.html content=YXBpVmVyc2lvbjogbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3YxCmtpbmQ6IE1hY2hpbmVDb25maWcKbWV0YWRhdGE6CiAgbGFiZWxzOgogICAgbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3JvbGU6IG1hc3RlcgogIG5hbWU6IDk5LW9wZW5zaGlmdC1tYWNoaW5lY29uZmlnLW1hc3Rlci1rYXJncwpzcGVjOgogIGtlcm5lbEFyZ3VtZW50czoKICAgIC0gJ2xvZ2xldmVsPTcnCg== file=99-openshift-machineconfig-master-kargs.yaml folder=openshift curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request POST \\ --data \"{\\\"file_name\\\":\\\"$file\\\", \\\"folder\\\":\\\"$folder\\\", \\\"content\\\":\\\"$content\\\"}\" \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/manifests\"","title":"Create a cluster manifest"},{"location":"user-guide/install-customization/#view-a-manifests-contents","text":"file=openshift/99-openshift-machineconfig-master-kargs.yaml curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/manifests/files?file_name=$file\"","title":"View a manifest\u2019s contents"},{"location":"user-guide/install-customization/#discovery-ignition","text":"The discovery ignition is used to make changes to the CoreOS live iso image which runs before we actually write anything to the target disk. An example use case would be configuring a separate container registry to pull the assisted-installer-agent image from. The discovery ignition must use version 3.1.0 regardless of the version of the cluster that will eventually be created.","title":"Discovery Ignition"},{"location":"user-guide/install-customization/#patch-the-discovery-ignition","text":"# ignition patch file { \"config\": \"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/etc/containers/registries.conf\\\", \\\"mode\\\": 420, \\\"overwrite\\\": true, \\\"user\\\": { \\\"name\\\": \\\"root\\\"},\\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,dW5xdWFsaWZpZWQtc2VhcmNoLXJlZ2lzdHJpZXMgPSBbInJlZ2lzdHJ5LmFjY2Vzcy5yZWRoYXQuY29tIiwgImRvY2tlci5pbyJdCltbcmVnaXN0cnldXQogICBwcmVmaXggPSAiIgogICBsb2NhdGlvbiA9ICJxdWF5LmlvL29jcG1ldGFsIgogICBtaXJyb3ItYnktZGlnZXN0LW9ubHkgPSBmYWxzZQogICBbW3JlZ2lzdHJ5Lm1pcnJvcl1dCiAgIGxvY2F0aW9uID0gImxvY2FsLnJlZ2lzdHJ5OjUwMDAvb2NwbWV0YWwiCg==\\\"}}]}}\" } curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data @discovery-ign.json \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition\"","title":"Patch the discovery ignition"},{"location":"user-guide/install-customization/#view-the-discovery-ignition","text":"curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/discovery-ignition\"","title":"View the discovery ignition"},{"location":"user-guide/install-customization/#install-config","text":"These endpoints alter the default install config yaml used when running openshift-install create commands. Install config customization reference is available here Some of this content will be pinned to particular values, but most can be edited. Note, some of this content will be pinned to particular values by the assisted-installer (can't be overwritten). You can compose the install-config overrides by creating a json string with the options you wish to set. An example install config with disabled hyperthreading for the control plane: apiVersion: v1 baseDomain: example.com controlPlane: name: master hyperthreading: Disabled compute: - name: worker replicas: 5 metadata: name: test-cluster platform: ... pullSecret: '{\"auths\": ...}' sshKey: ssh-ed25519 AAAA... should look like this: \"{\\\"controlPlane\\\":{\\\"hyperthreading\\\":\\\"Disabled\\\"}}\"","title":"Install Config"},{"location":"user-guide/install-customization/#patch-the-install-config","text":"curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '\"{\\\"controlPlane\\\":{\\\"hyperthreading\\\":\\\"Disabled\\\"}}\"' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/install-config\"","title":"Patch the install config"},{"location":"user-guide/install-customization/#view-the-install-config","text":"curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/install-config\"","title":"View the install config"},{"location":"user-guide/install-customization/#pointer-ignition","text":"The pointer ignition is used to customize the particular host when it reboots into the installed system. These changes will be made to the host, but will not be tracked as a machine config. Generally a machine config will be a better fit than editing the pointer ignition, but if some change is specific to an individual host it would be made here. The pointer ignition override version must match the version of the ignition generated by openshift-installer. This means that the version required for the override will change depending on the cluster version being installed. OCP Version(s) Ignition version 4.6 3.1.0 4.7, 4.8, 4.9, 4.10 3.2.0","title":"Pointer Ignition"},{"location":"user-guide/install-customization/#patch-the-pointer-ignition","text":"curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '{\"config\": \"{\\\"ignition\\\": {\\\"version\\\": \\\"3.1.0\\\"}, \\\"storage\\\": {\\\"files\\\": [{\\\"path\\\": \\\"/etc/example\\\", \\\"contents\\\": {\\\"source\\\": \\\"data:text/plain;base64,SGVsbG8gZnJvbSBob3N0IDg0Njk2NzdiLThlZGEtNDQzOS1iNDQwLTc3ZGM5M2FkZmNlZgo=\\\"}}]}}\"}' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/ignition\"","title":"Patch the pointer ignition"},{"location":"user-guide/install-customization/#view-the-pointer-ignition","text":"curl --header \"Authorization: Bearer $TOKEN\" \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/ignition\"","title":"View the pointer ignition"},{"location":"user-guide/install-customization/#installer-params","text":"This endpoint sets parameters to be passed to the coreos-installer command line in addition to the ones we provide by default. The only parameters that can be set with this endpoint are: [\"--append-karg\", \"--delete-karg\", \"-n\", \"--copy-network\", \"--network-dir\", \"--save-partlabel\", \"--save-partindex\", \"--image-url\"]","title":"Installer Params"},{"location":"user-guide/install-customization/#set-the-installer-params","text":"curl \\ --header \"Content-Type: application/json\" \\ --header \"Authorization: Bearer $TOKEN\" \\ --request PATCH \\ --data '{\"args\": [\"--append-karg\", \"nameserver=8.8.8.8\", \"-n\"]}' \\ \"http://$ASSISTED_SERVICE_IP:$ASSISTED_SERVICE_PORT/api/assisted-install/v1/clusters/$CLUSTER_ID/hosts/$HOST_ID/installer-args\"","title":"Set the installer params"},{"location":"user-guide/network-configuration/","text":"Network Congfiguration This document provides an overview of network configurations supported when deploying OCP using Assisted Service. NMState NMStateConfig is a declarative way of managing configuration of networking settings. A general introduction to the NMState can be found in the upstream documentation whereas the Hive Integration readme explains how to use it together with the Assisted Service. This example shows how to create a custom NMStateConfig to be used with Assisted Service on-premises. OCP Networking There are various network types and addresses used by OCP and listed in the table below. Type DNS Description clusterNetwork IP address pools from which pod IP addresses are allocated serviceNetwork IP address pool for services machineNetwork IP address blocks for machines forming the cluster apiVIP api. The VIP to use for API communication. This setting must either be provided or pre-configured in the DNS so that the default name resolves correctly. ingressVIP test.apps. The VIP to use for ingress traffic Apart from this, depending on the desired network stack, different network controller can be selected. Currently Assisted Service can deploy OCP clusters using one of the following configurations IPv4 IPv6 (with all the required images mirrored in some local registry) Dual-stack (IPv4 + IPv6) Supported network controllers depend on the selected stack and are summarized in the table below. For a detailed CNI network provider feature comparison you may want to check the OCP Networking documentation . Stack SDN OVN IPv4 Yes (default) Yes IPv6 No Yes Dual-stack No Yes Cluster Network Cluster network is a network from which every pod deployed in the cluster gets its IP address. Given that the workload may live across multiple nodes forming the cluster, it's important for the network provider to be able to easily find an individual node based on the pod's IP address. In order to do this, clusterNetwork.cidr is further split into multiple subnets of the size defined in clusterNetwork.hostPrefix . Host prefix specifies a lenght of the subnet assigned to each individual node in the cluster. An example of how a cluster may assign addresses for the multi-node cluster clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 Creating a 3-node cluster using the snipet above may create the following network topology pods scheduled in the node #1 get IPs from 10.128.0.0/23 pods scheduled in the node #2 get IPs from 10.128.2.0/23 pods scheduled in the node #3 get IPs from 10.128.4.0/23 Explaining OVN-K8s internals is out of scope of this document, but the pattern described above gives us an easy way to route pod-to-pod traffic between different nodes without keeping a big list of mapping between pods and their corresponding nodes. Additional reading Additional explanation of the syntax used in the network subnets can be found in the Installing on bare metal OCP documentation . It may also be useful to familiarize yourself with Cluster Network Operator configuration . Bare metal IPI documentation provides additional explanation of the syntax for the VIP addresses. SNO vs Multi-Node Cluster Depending whether a Single Node OpenShift or a Multi-Node cluster is deployed, different values are mandatory. The table below explains this in more detail. Parameter SNO Multi-Node Cluster clusterNetwork Required Required serviceNetwork Required Required machineNetwork Required Forbidden in DHCP mode apiVIP Forbidden Required ingressVIP Forbidden Required IP Stack When using Assisted Service on-premises, configuration of the IP stack is done in the AgentClusterInstall custom resource . Examples below show how different configurations can be achieved. IPv4 Sample CR for deploying a SNO cluster using Assisted Service. In this scenario only one subnet per network type is supported. networking: networkType: OpenShiftSDN clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.126.0/24 serviceNetwork: - 172.30.0.0/16 Sample CR for multi-node OCP cluster spec: apiVIP: 1.2.3.8 ingressVIP: 1.2.3.9 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 serviceNetwork: - 172.30.0.0/16 IPv6 The workflow for deploying a cluster without Internet access has some prerequisites which are out of scope of this document. You may consult the Zero Touch Provisioning in the hard way git repo for some insights. Sample CR for deploying a SNO cluster using Assisted Service. In this scenario only one subnet per network type is supported. spec: networking: networkType: OVNKubernetes clusterNetwork: - cidr: fd01::/48 hostPrefix: 64 machineNetwork: - cidr: 1001:db8::/120 serviceNetwork: - fd02::/112 Sample CR for multi-node OCP cluster spec: apiVIP: \"2620:52:0:1302::3\" ingressVIP: \"2620:52:0:1302::2\" networking: clusterNetwork: - cidr: fd01::/48 hostPrefix: 64 serviceNetwork: - fd02::/112 Dual-stack This configuration allows deployment of a cluster with pods residing in both IPv4 and IPv6 subnets. Be aware of the following limitations and requirements of this configuration. apiVIP and ingressVIP support only single value. Both addresses must come from the IPv4 range (the work is being done by the Metal Platform Networking to support multiple addresses) clusterNetwork , machineNetwork and serviceNetwork support exactly 2 values. The first one must come from the IPv4 range, the second one from the IPv6 (i.e. dual-stack setups must be \"IPv4-primary\") Sample CR for deploying a SNO cluster using Assisted Service spec: networking: networkType: OVNKubernetes clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 - cidr: fd01::/48 hostPrefix: 64 machineNetwork: - cidr: 192.168.126.0/24 - cidr: 1001:db8::/120 serviceNetwork: - 172.30.0.0/16 - fd02::/112 Sample CR for multi-node OCP cluster spec: apiVIP: 1.2.3.8 ingressVIP: 1.2.3.9 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 - cidr: fd01::/48 hostPrefix: 64 serviceNetwork: - 172.30.0.0/16 - fd02::/112 OVN-Kubernetes Troubleshooting For the cluster installed using networkType: OVNKubernetes the following steps may be useful to see the network configuration after the deployment. # oc -n openshift-ovn-kubernetes get pod/ovnkube-master-9ljfk -o yaml | less metadata: annotations: networkoperator.openshift.io/ip-family-mode: dual-stack ... exec /usr/bin/ovnkube \\ --metrics-bind-address \"127.0.0.1:29102\" \\ ... --sb-address \"ssl:192.168.126.10:9642,ssl:192.168.126.12:9642\" \\ ... --nb-address \"ssl:192.168.126.10:9641,ssl:192.168.126.12:9641\" \\ ... hostIP: 192.168.126.12 phase: Running podIP: 192.168.126.12 podIPs: - ip: 192.168.126.12 - ip: 1001:db8::46 # oc describe pod/ovnkube-master-9ljfk -n openshift-ovn-kubernetes | less Annotations: networkoperator.openshift.io/ip-family-mode: dual-stack workload.openshift.io/warning: only single-node clusters support workload partitioning Status: Running IP: 192.168.126.12 IPs: IP: 192.168.126.12 IP: 1001:db8::46 Controlled By: DaemonSet/ovnkube-master # oc describe network.config.openshift.io | less Spec: Cluster Network: Cidr: fd01::/48 Host Prefix: 64 Cidr: 10.128.0.0/14 Host Prefix: 23 External IP: Policy: Network Type: OVNKubernetes Service Network: fd02::/112 172.30.0.0/16 Status: Cluster Network: Cidr: fd01::/48 Host Prefix: 64 Cidr: 10.128.0.0/14 Host Prefix: 23 Cluster Network MTU: 1400 Network Type: OVNKubernetes Service Network: fd02::/112 172.30.0.0/16 # oc get nodes test-infra-cluster-assisted-installer-master-0 -o yaml | less metadata: annotations: k8s.ovn.org/host-addresses: '[\"1001:db8:0:200::78\",\"1001:db8::5f\",\"192.168.126.10\",\"192.168.126.101\",\"192.168.141.10\"]' k8s.ovn.org/l3-gateway-config: '{\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_test-infra-cluster-assisted-installer-master-0\",\"mac-address\":\"02:00:00:af:e4:63\",\"ip-addresses\":[\"192.168.126.10/24\",\"1001:db8::5f/128\"],\"next-hops\":[\"192.168.126.1\",\"fe80::5054:ff:fe46:98d6\"],\"node-port-enable\":\"true\",\"vlan-id\":\"0\"}}' k8s.ovn.org/node-chassis-id: 1f22cd5d-b353-4be0-aa25-0f39a3e34519 k8s.ovn.org/node-mgmt-port-mac-address: 6a:46:7e:d8:aa:d5 k8s.ovn.org/node-primary-ifaddr: '{\"ipv4\":\"192.168.126.10/24\",\"ipv6\":\"1001:db8::5f/128\"}' k8s.ovn.org/node-subnets: '{\"default\":[\"10.128.0.0/23\",\"fd01:0:0:1::/64\"]}' k8s.ovn.org/topology-version: \"4\" ... status: addresses: - address: 192.168.126.10 type: InternalIP - address: 1001:db8::5f type: InternalIP - address: test-infra-cluster-assisted-installer-master-0 type: Hostname ...","title":"Network Congfiguration"},{"location":"user-guide/network-configuration/#network-congfiguration","text":"This document provides an overview of network configurations supported when deploying OCP using Assisted Service.","title":"Network Congfiguration"},{"location":"user-guide/network-configuration/#nmstate","text":"NMStateConfig is a declarative way of managing configuration of networking settings. A general introduction to the NMState can be found in the upstream documentation whereas the Hive Integration readme explains how to use it together with the Assisted Service. This example shows how to create a custom NMStateConfig to be used with Assisted Service on-premises.","title":"NMState"},{"location":"user-guide/network-configuration/#ocp-networking","text":"There are various network types and addresses used by OCP and listed in the table below. Type DNS Description clusterNetwork IP address pools from which pod IP addresses are allocated serviceNetwork IP address pool for services machineNetwork IP address blocks for machines forming the cluster apiVIP api. The VIP to use for API communication. This setting must either be provided or pre-configured in the DNS so that the default name resolves correctly. ingressVIP test.apps. The VIP to use for ingress traffic Apart from this, depending on the desired network stack, different network controller can be selected. Currently Assisted Service can deploy OCP clusters using one of the following configurations IPv4 IPv6 (with all the required images mirrored in some local registry) Dual-stack (IPv4 + IPv6) Supported network controllers depend on the selected stack and are summarized in the table below. For a detailed CNI network provider feature comparison you may want to check the OCP Networking documentation . Stack SDN OVN IPv4 Yes (default) Yes IPv6 No Yes Dual-stack No Yes","title":"OCP Networking"},{"location":"user-guide/network-configuration/#cluster-network","text":"Cluster network is a network from which every pod deployed in the cluster gets its IP address. Given that the workload may live across multiple nodes forming the cluster, it's important for the network provider to be able to easily find an individual node based on the pod's IP address. In order to do this, clusterNetwork.cidr is further split into multiple subnets of the size defined in clusterNetwork.hostPrefix . Host prefix specifies a lenght of the subnet assigned to each individual node in the cluster. An example of how a cluster may assign addresses for the multi-node cluster clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 Creating a 3-node cluster using the snipet above may create the following network topology pods scheduled in the node #1 get IPs from 10.128.0.0/23 pods scheduled in the node #2 get IPs from 10.128.2.0/23 pods scheduled in the node #3 get IPs from 10.128.4.0/23 Explaining OVN-K8s internals is out of scope of this document, but the pattern described above gives us an easy way to route pod-to-pod traffic between different nodes without keeping a big list of mapping between pods and their corresponding nodes.","title":"Cluster Network"},{"location":"user-guide/network-configuration/#additional-reading","text":"Additional explanation of the syntax used in the network subnets can be found in the Installing on bare metal OCP documentation . It may also be useful to familiarize yourself with Cluster Network Operator configuration . Bare metal IPI documentation provides additional explanation of the syntax for the VIP addresses.","title":"Additional reading"},{"location":"user-guide/network-configuration/#sno-vs-multi-node-cluster","text":"Depending whether a Single Node OpenShift or a Multi-Node cluster is deployed, different values are mandatory. The table below explains this in more detail. Parameter SNO Multi-Node Cluster clusterNetwork Required Required serviceNetwork Required Required machineNetwork Required Forbidden in DHCP mode apiVIP Forbidden Required ingressVIP Forbidden Required","title":"SNO vs Multi-Node Cluster"},{"location":"user-guide/network-configuration/#ip-stack","text":"When using Assisted Service on-premises, configuration of the IP stack is done in the AgentClusterInstall custom resource . Examples below show how different configurations can be achieved.","title":"IP Stack"},{"location":"user-guide/network-configuration/#ipv4","text":"Sample CR for deploying a SNO cluster using Assisted Service. In this scenario only one subnet per network type is supported. networking: networkType: OpenShiftSDN clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.126.0/24 serviceNetwork: - 172.30.0.0/16 Sample CR for multi-node OCP cluster spec: apiVIP: 1.2.3.8 ingressVIP: 1.2.3.9 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 serviceNetwork: - 172.30.0.0/16","title":"IPv4"},{"location":"user-guide/network-configuration/#ipv6","text":"The workflow for deploying a cluster without Internet access has some prerequisites which are out of scope of this document. You may consult the Zero Touch Provisioning in the hard way git repo for some insights. Sample CR for deploying a SNO cluster using Assisted Service. In this scenario only one subnet per network type is supported. spec: networking: networkType: OVNKubernetes clusterNetwork: - cidr: fd01::/48 hostPrefix: 64 machineNetwork: - cidr: 1001:db8::/120 serviceNetwork: - fd02::/112 Sample CR for multi-node OCP cluster spec: apiVIP: \"2620:52:0:1302::3\" ingressVIP: \"2620:52:0:1302::2\" networking: clusterNetwork: - cidr: fd01::/48 hostPrefix: 64 serviceNetwork: - fd02::/112","title":"IPv6"},{"location":"user-guide/network-configuration/#dual-stack","text":"This configuration allows deployment of a cluster with pods residing in both IPv4 and IPv6 subnets. Be aware of the following limitations and requirements of this configuration. apiVIP and ingressVIP support only single value. Both addresses must come from the IPv4 range (the work is being done by the Metal Platform Networking to support multiple addresses) clusterNetwork , machineNetwork and serviceNetwork support exactly 2 values. The first one must come from the IPv4 range, the second one from the IPv6 (i.e. dual-stack setups must be \"IPv4-primary\") Sample CR for deploying a SNO cluster using Assisted Service spec: networking: networkType: OVNKubernetes clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 - cidr: fd01::/48 hostPrefix: 64 machineNetwork: - cidr: 192.168.126.0/24 - cidr: 1001:db8::/120 serviceNetwork: - 172.30.0.0/16 - fd02::/112 Sample CR for multi-node OCP cluster spec: apiVIP: 1.2.3.8 ingressVIP: 1.2.3.9 networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 - cidr: fd01::/48 hostPrefix: 64 serviceNetwork: - 172.30.0.0/16 - fd02::/112","title":"Dual-stack"},{"location":"user-guide/network-configuration/#ovn-kubernetes-troubleshooting","text":"For the cluster installed using networkType: OVNKubernetes the following steps may be useful to see the network configuration after the deployment. # oc -n openshift-ovn-kubernetes get pod/ovnkube-master-9ljfk -o yaml | less metadata: annotations: networkoperator.openshift.io/ip-family-mode: dual-stack ... exec /usr/bin/ovnkube \\ --metrics-bind-address \"127.0.0.1:29102\" \\ ... --sb-address \"ssl:192.168.126.10:9642,ssl:192.168.126.12:9642\" \\ ... --nb-address \"ssl:192.168.126.10:9641,ssl:192.168.126.12:9641\" \\ ... hostIP: 192.168.126.12 phase: Running podIP: 192.168.126.12 podIPs: - ip: 192.168.126.12 - ip: 1001:db8::46 # oc describe pod/ovnkube-master-9ljfk -n openshift-ovn-kubernetes | less Annotations: networkoperator.openshift.io/ip-family-mode: dual-stack workload.openshift.io/warning: only single-node clusters support workload partitioning Status: Running IP: 192.168.126.12 IPs: IP: 192.168.126.12 IP: 1001:db8::46 Controlled By: DaemonSet/ovnkube-master # oc describe network.config.openshift.io | less Spec: Cluster Network: Cidr: fd01::/48 Host Prefix: 64 Cidr: 10.128.0.0/14 Host Prefix: 23 External IP: Policy: Network Type: OVNKubernetes Service Network: fd02::/112 172.30.0.0/16 Status: Cluster Network: Cidr: fd01::/48 Host Prefix: 64 Cidr: 10.128.0.0/14 Host Prefix: 23 Cluster Network MTU: 1400 Network Type: OVNKubernetes Service Network: fd02::/112 172.30.0.0/16 # oc get nodes test-infra-cluster-assisted-installer-master-0 -o yaml | less metadata: annotations: k8s.ovn.org/host-addresses: '[\"1001:db8:0:200::78\",\"1001:db8::5f\",\"192.168.126.10\",\"192.168.126.101\",\"192.168.141.10\"]' k8s.ovn.org/l3-gateway-config: '{\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_test-infra-cluster-assisted-installer-master-0\",\"mac-address\":\"02:00:00:af:e4:63\",\"ip-addresses\":[\"192.168.126.10/24\",\"1001:db8::5f/128\"],\"next-hops\":[\"192.168.126.1\",\"fe80::5054:ff:fe46:98d6\"],\"node-port-enable\":\"true\",\"vlan-id\":\"0\"}}' k8s.ovn.org/node-chassis-id: 1f22cd5d-b353-4be0-aa25-0f39a3e34519 k8s.ovn.org/node-mgmt-port-mac-address: 6a:46:7e:d8:aa:d5 k8s.ovn.org/node-primary-ifaddr: '{\"ipv4\":\"192.168.126.10/24\",\"ipv6\":\"1001:db8::5f/128\"}' k8s.ovn.org/node-subnets: '{\"default\":[\"10.128.0.0/23\",\"fd01:0:0:1::/64\"]}' k8s.ovn.org/topology-version: \"4\" ... status: addresses: - address: 192.168.126.10 type: InternalIP - address: 1001:db8::5f type: InternalIP - address: test-infra-cluster-assisted-installer-master-0 type: Hostname ...","title":"OVN-Kubernetes Troubleshooting"},{"location":"user-guide/restful-api-guide/","text":"Introduction The purpose of this guide is to assist users in using the RESTful API for interacting with the assisted-service. Setting Static Network Config User may provide static network configurations when generating the discovery ISO. The static network configurations per each host should contain: * nmstate file in YAML format, specifying the desired network configuration for the host * The file will contain interface logical names that will be replaced with host's actual interface name at discovery time * A map (mac-interface-mapping) containing the mapping of mac-address to the logical interface name In the following example server-a.yaml and server-b.yaml files contain the nmstate configuration in YAML format for two nodes. Here is an example of the content of server-a.yaml , setting network configuration for two of its network interfaces: dns-resolver: config: server: - 192.168.126.1 interfaces: - ipv4: address: - ip: 192.168.126.30 prefix-length: 24 dhcp: false enabled: true name: eth0 state: up type: ethernet - ipv4: address: - ip: 192.168.141.30 prefix-length: 24 dhcp: false enabled: true name: eth1 state: up type: ethernet routes: config: - destination: 0.0.0.0/0 next-hop-address: 192.168.126.1 next-hop-interface: eth0 table-id: 254 The mac-interface-mapping attribute should map the MAC Addresses of the host to the logical interface name as used in the network_yaml element (nmstate files): mac_interface_map: [ { mac_address: 02:00:00:2c:23:a5, logical_nic_name: eth0 }, { mac_address: 02:00:00:68:73:dc, logical_nic_name: eth1 } ] In order to use curl to send a request for setting static network configuration, there is a need to JSON-encode the content of those files. This can be achieved using the jq tool as shown below: ASSISTED_SERVICE_URL=http://${host_address}:${port} CLUSTER_ID=... NODE_SSH_KEY=\"...\" request_body=$(mktemp) jq -n --arg SSH_KEY \"$NODE_SSH_KEY\" --arg NMSTATE_YAML1 \"$(cat server-a.yaml)\" --arg NMSTATE_YAML2 \"$(cat server-b.yaml)\" \\ '{ \"ssh_public_key\": $SSH_KEY, \"image_type\": \"full-iso\", \"static_network_config\": [ { \"network_yaml\": $NMSTATE_YAML1, \"mac_interface_map\": [{\"mac_address\": \"02:00:00:2c:23:a5\", \"logical_nic_name\": \"eth0\"}, {\"mac_address\": \"02:00:00:68:73:dc\", \"logical_nic_name\": \"eth1\"}] }, { \"network_yaml\": $NMSTATE_YAML2, \"mac_interface_map\": [{\"mac_address\": \"02:00:00:9f:85:eb\", \"logical_nic_name\": \"eth1\"}, {\"mac_address\": \"02:00:00:c8:be:9b\", \"logical_nic_name\": \"eth0\"}] } ] }' >> $request_body The request will be stored in a temporary file $request_body and will be used as the request body of the curl command: curl -H \"Content-Type: application/json\" -X POST -d @$request_body ${ASSISTED_SERVICE_URL}/api/assisted-install/v1/clusters/$CLUSTER_ID/downloads/image","title":"Introduction"},{"location":"user-guide/restful-api-guide/#introduction","text":"The purpose of this guide is to assist users in using the RESTful API for interacting with the assisted-service.","title":"Introduction"},{"location":"user-guide/restful-api-guide/#setting-static-network-config","text":"User may provide static network configurations when generating the discovery ISO. The static network configurations per each host should contain: * nmstate file in YAML format, specifying the desired network configuration for the host * The file will contain interface logical names that will be replaced with host's actual interface name at discovery time * A map (mac-interface-mapping) containing the mapping of mac-address to the logical interface name In the following example server-a.yaml and server-b.yaml files contain the nmstate configuration in YAML format for two nodes. Here is an example of the content of server-a.yaml , setting network configuration for two of its network interfaces: dns-resolver: config: server: - 192.168.126.1 interfaces: - ipv4: address: - ip: 192.168.126.30 prefix-length: 24 dhcp: false enabled: true name: eth0 state: up type: ethernet - ipv4: address: - ip: 192.168.141.30 prefix-length: 24 dhcp: false enabled: true name: eth1 state: up type: ethernet routes: config: - destination: 0.0.0.0/0 next-hop-address: 192.168.126.1 next-hop-interface: eth0 table-id: 254 The mac-interface-mapping attribute should map the MAC Addresses of the host to the logical interface name as used in the network_yaml element (nmstate files): mac_interface_map: [ { mac_address: 02:00:00:2c:23:a5, logical_nic_name: eth0 }, { mac_address: 02:00:00:68:73:dc, logical_nic_name: eth1 } ] In order to use curl to send a request for setting static network configuration, there is a need to JSON-encode the content of those files. This can be achieved using the jq tool as shown below: ASSISTED_SERVICE_URL=http://${host_address}:${port} CLUSTER_ID=... NODE_SSH_KEY=\"...\" request_body=$(mktemp) jq -n --arg SSH_KEY \"$NODE_SSH_KEY\" --arg NMSTATE_YAML1 \"$(cat server-a.yaml)\" --arg NMSTATE_YAML2 \"$(cat server-b.yaml)\" \\ '{ \"ssh_public_key\": $SSH_KEY, \"image_type\": \"full-iso\", \"static_network_config\": [ { \"network_yaml\": $NMSTATE_YAML1, \"mac_interface_map\": [{\"mac_address\": \"02:00:00:2c:23:a5\", \"logical_nic_name\": \"eth0\"}, {\"mac_address\": \"02:00:00:68:73:dc\", \"logical_nic_name\": \"eth1\"}] }, { \"network_yaml\": $NMSTATE_YAML2, \"mac_interface_map\": [{\"mac_address\": \"02:00:00:9f:85:eb\", \"logical_nic_name\": \"eth1\"}, {\"mac_address\": \"02:00:00:c8:be:9b\", \"logical_nic_name\": \"eth0\"}] } ] }' >> $request_body The request will be stored in a temporary file $request_body and will be used as the request body of the curl command: curl -H \"Content-Type: application/json\" -X POST -d @$request_body ${ASSISTED_SERVICE_URL}/api/assisted-install/v1/clusters/$CLUSTER_ID/downloads/image","title":"Setting Static Network Config"}]}
